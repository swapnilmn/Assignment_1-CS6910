{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOgf+Kf6/4e6sEYSfeeltHf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swapnilmn/Assignment_1_CS6910/blob/master/Question_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Question 3 (24 Marks) Implement the backpropagation algorithm with support for the following optimisation functions\n",
        "            sgd\n",
        "            momentum based gradient descent\n",
        "            nesterov accelerated gradient descent\n",
        "            rmsprop\n",
        "            adam\n",
        "            nadam\n",
        "\n",
        "(12 marks for the backpropagation framework and 2 marks for each of the optimisation algorithms above)\n",
        "\n",
        "We will check the code for implementation and ease of use (e.g., how easy it is to add a new optimisation algorithm such as Eve). Note that the code should be flexible enough to work with different batch sizes."
      ],
      "metadata": {
        "id": "3kVFZwNkEA7D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries "
      ],
      "metadata": {
        "id": "_dP0oEKrgmQc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CZcFz8GygBp5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "outputId": "ba4c27ed-a790-4317-afbc-08557fcb80bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.13.11-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.16.0-py2.py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (2.25.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (8.1.3)\n",
            "Collecting appdirs>=1.4.3\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (3.19.6)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (5.4.8)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.9/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from wandb) (4.5.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.15.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.14)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=e1d1be07112d277b89c2c94273ea80bee03e32271ebb0061feb53fc27de66bd8\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/0a/67/ada2a22079218c75a88361c0782855cc72aebc4d18d0289d05\n",
            "Successfully built pathtools\n",
            "Installing collected packages: pathtools, appdirs, smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.31 appdirs-1.4.4 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.16.0 setproctitle-1.3.2 smmap-5.0.0 wandb-0.13.11\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from keras.datasets import fashion_mnist, mnist\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import copy\n",
        "from sklearn.model_selection import train_test_split \n",
        "import pandas as pd\n",
        "!pip install wandb\n",
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Processing"
      ],
      "metadata": {
        "id": "W-FlxB4-gnqT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OneHotEncoder_from_scratch:\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.categories = None\n",
        "    def fit(self, X):\n",
        "        self.categories =[]\n",
        "        for i in range(X.shape[1]):\n",
        "            feature_categories =list(set(X[:, i]))\n",
        "            self.categories.append(feature_categories)\n",
        "            \n",
        "    def transform(self, X):\n",
        "        one_hot_vector = []\n",
        "\n",
        "        for i in range(X.shape[0]):\n",
        "            one_hot_row = []\n",
        "            for j in range(X.shape[1]):\n",
        "\n",
        "                category_index = self.categories[j].index(X[i, j])\n",
        "                category_one_hot =[0] *len(self.categories[j])\n",
        "                category_one_hot[category_index] = 1\n",
        "\n",
        "                one_hot_row.extend(category_one_hot)\n",
        "            one_hot_vector.append(one_hot_row)\n",
        "        return np.array(one_hot_vector)"
      ],
      "metadata": {
        "id": "YTDGKrnOgGwK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = 'fashion_mnist'\n",
        "\n",
        "def dataset_type(dataset):\n",
        "  if dataset == 'fashion_mnist':\n",
        "      (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "  elif dataset == 'mnist':\n",
        "      (train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "  else:\n",
        "      raise ValueError('Invalid dataset name')\n",
        "  X_train, X_val, Y_train, Y_val = train_test_split(train_images, train_labels, test_size=0.1, random_state=42)\n",
        "  train_input = []\n",
        "  for i in range(len(X_train)):\n",
        "      train_input.append(list(np.concatenate(X_train[i]).flat))\n",
        "\n",
        "  val_input = []\n",
        "  for i in range(len(X_val)):\n",
        "      val_input.append(list(np.concatenate(X_val[i]).flat))\n",
        "\n",
        "  test_input = []\n",
        "  for i in range(len(test_images)):\n",
        "      test_input.append(list(np.concatenate(test_images[i]).flat))\n",
        "  Y_train = np.array(Y_train)\n",
        "  Y_val = np.array(Y_val)\n",
        "  Y_test = np.array(test_labels)\n",
        "\n",
        "  X_train = np.array(train_input) / 255.0\n",
        "  X_test = np.array(test_input) / 255.0\n",
        "  X_val = np.array(val_input) / 255.0\n",
        "\n",
        "  enc = OneHotEncoder_from_scratch()\n",
        "  enc.fit(Y_train.reshape(-1, 1))\n",
        "  Y_train = enc.transform(Y_train.reshape(-1, 1))\n",
        "  Y_val = enc.transform(Y_val.reshape(-1, 1))\n",
        "  Y_test = enc.transform(Y_test.reshape(-1, 1))\n",
        "\n",
        "  return X_train, X_val, X_test, Y_train, Y_val, Y_test\n",
        "\n",
        "dataset = 'fashion_mnist'\n",
        "X_train, X_val, X_test, Y_train, Y_val, Y_test = dataset_type(dataset)\n",
        "\n",
        "print(Y_train.shape, Y_val.shape, Y_test.shape)\n",
        "print(X_train.shape, X_val.shape, X_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzdukjGZgZeJ",
        "outputId": "c60912b4-9bd9-40f5-faaf-d23b58976fd4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n",
            "(54000, 10) (6000, 10) (10000, 10)\n",
            "(54000, 784) (6000, 784) (10000, 784)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Class of FeedForward Neural Network"
      ],
      "metadata": {
        "id": "DXX2kyFig0i5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(project = 'Question_4_Best_Model', entity = 'ed22s009')\n",
        "\n",
        "class FFNN:\n",
        "  def __init__(self, X, Y,\n",
        "               epochs = 100, \n",
        "               hidden_layer_count = 4,\n",
        "               hidden_layers =  [32, 64, 128, 256],\n",
        "               learning_rate = 0.001,\n",
        "               batch_size = 32,\n",
        "               activation='tanh',\n",
        "               weight_init='random',\n",
        "               loss = 'mean_squared_error',\n",
        "               weight_decay = 0):\n",
        "    \n",
        "    self.inputs =X.shape[1] # Number of inputs\n",
        "    self.outputs= Y.shape[1] # Number of outputs\n",
        "    self.epochs = epochs\n",
        "    self.hidden_layers = hidden_layer_count  # Number of hidden layers \n",
        "    self.network_size= [self.inputs] + hidden_layers +[self.outputs] # input layer + hidden layers + output layers\n",
        "    self.learning_rate = learning_rate\n",
        "    self.batch_size = batch_size\n",
        "    self.weights={} # It will create dictionary for weights and biases\n",
        "    self.weights_h = []\n",
        "    self.num_classes = Y.shape[1]\n",
        "    self.weight_init = weight_init\n",
        "    self.activation_function = activation\n",
        "    self.loss_function = loss\n",
        "    self.lambd = weight_decay\n",
        "    np.random.seed(0)  # We will set seed value so that it will generate same random numebers every time\n",
        "\n",
        "    self.grad_derivatice={}\n",
        "    self.update_weights={}\n",
        "    self.prev_update_weights={}\n",
        "    for i in range(1,self.hidden_layers+1):\n",
        "      vw_key, vb_key, mb_key, mw_key = [f'{key}{i}' for key in ['vw', 'vb', 'mb', 'mw']]\n",
        "      self.update_weights[vw_key]=0\n",
        "      self.update_weights[vb_key]=0\n",
        "      self.update_weights[mb_key]=0\n",
        "      self.update_weights[mw_key]=0\n",
        "      self.prev_update_weights[vw_key]=0\n",
        "      self.prev_update_weights[vb_key]=0\n",
        "\n",
        "    # for creating initial weights\n",
        "    if self.weight_init == 'random':\n",
        "      for i in range(1, self.hidden_layers + 2):\n",
        "          weight_shape = (self.network_size[i - 1], self.network_size[i])\n",
        "          weight_scale = 0.1\n",
        "          self.weights[f'W{i}'] = np.random.normal(scale=weight_scale, size=weight_shape)*0.1\n",
        "          # we are mulliplying ny 0.1 to get less wrights\n",
        "          bias_shape = (1, self.network_size[i])\n",
        "          self.weights[f'B{i}'] = np.zeros(bias_shape)\n",
        "\n",
        "    if self.weight_init == 'Xavier':\n",
        "      for i in range(1, self.hidden_layers + 2):\n",
        "          weight_shape = (self.network_size[i - 1], self.network_size[i])\n",
        "          weight_scale = 0.1\n",
        "          self.weights[f'W{i}'] = np.random.normal(scale=weight_scale, size=weight_shape)*np.sqrt(1/self.network_size[i-1])\n",
        "          \n",
        "          bias_shape = (1, self.network_size[i])\n",
        "          self.weights[f'B{i}'] = np.zeros(bias_shape)\n",
        "\n",
        "\n",
        "\n",
        "  def forward_activation(self, X):\n",
        "      activation_functions = {\n",
        "          'sigmoid': lambda x: 1.0 / (1.0 + np.exp(-x)),\n",
        "          'tanh': np.tanh,\n",
        "          'ReLU': lambda x: np.maximum(0, x)\n",
        "      }\n",
        "      activation_function = activation_functions.get(self.activation_function)\n",
        "      if activation_function:\n",
        "          return activation_function(X)\n",
        "      else:\n",
        "          raise ValueError(f\"Unknown activation function '{self.activation_function}'\")\n",
        "\n",
        "\n",
        "  def grad_activation(self, X):\n",
        "      activation_gradients = {\n",
        "          'sigmoid': lambda x: x * (1 - x),\n",
        "          'tanh': lambda x: 1 - np.square(x),\n",
        "          'ReLU': lambda x: 1.0 * (x > 0)\n",
        "      }\n",
        "      gradient_function = activation_gradients.get(self.activation_function)\n",
        "      if gradient_function:\n",
        "          return gradient_function(X)\n",
        "      else:\n",
        "          raise ValueError(f\"Unknown activation function '{self.activation_function}'\")\n",
        "\n",
        "  def softmax(self, X):\n",
        "    exps =np.exp(X - np.max(X, axis=1, keepdims=True))\n",
        "    return  exps /np.sum(exps, axis=1, keepdims=True)\n",
        "  \n",
        "\n",
        "  def forward_pass(self, X, weights=None):\n",
        "    #X: shape (batch_size, input_dim)\n",
        "    if weights is None:\n",
        "\n",
        "        weights =self.weights\n",
        "    #weights:a dictionary containing weight and bias parameters for each layer\n",
        "    self.z = {}\n",
        "    self.z = {}\n",
        "    self.z[0] = X\n",
        "    for i in range(self.hidden_layers):\n",
        "        self.z[i+1] = self.z[i]@weights[f'W{i+1}'] + weights[f'B{i+1}'] # weights[f'W{i+1}'](input_dim, output_dim) + # weights[f'B{i+1}'](1, output_dim)\n",
        "        #z is dictionary containing the output of each layer's activationfunction\n",
        "        self.z[i+1] = self.forward_activation(self.z[i+1]) # self.z[i+1](batch_size, output_dim)\n",
        "    self.z[self.hidden_layers+1] = self.z[self.hidden_layers] @ weights[f'W{self.hidden_layers+1}'] + weights[f'B{self.hidden_layers+1}']\n",
        "\n",
        "    #self.z[self.hidden_layers+1](batch_size, output_dim)\n",
        "    self.z[self.hidden_layers+1]=  self.softmax(self.z[self.hidden_layers+1])\n",
        "\n",
        "    return self.z[self.hidden_layers+1]\n",
        "\n",
        "\n",
        "  def backprop(self, X, Y, weights=None):\n",
        "    #X(batch_size, input_size)\n",
        "    #Y(batch_size, output_size)\n",
        "    if weights is None:\n",
        "        weights = self.weights\n",
        "\n",
        "    self.forward_pass(X, weights)\n",
        "    self.grad_derivatice = {}\n",
        "    total_layers= self.hidden_layers + 1\n",
        "\n",
        "    if self.loss_function == 'cross_entropy':\n",
        "        self.grad_derivatice[f'dA{total_layers}'] =  (self.z[total_layers] - Y)\n",
        "    elif self.loss_function == 'mean_squared_error':\n",
        "        self.grad_derivatice[f'dA{total_layers}'] = (1/X.shape[0]) * 2 * (self.z[total_layers] - Y)\n",
        "\n",
        "    for k in range(total_layers, 0, -1):\n",
        "        w_key, b_key, dw_key, db_key, da_key = [f'{key}{k}' for key in ['W', 'B', 'dW', 'dB', 'dA']]\n",
        "        self.grad_derivatice[dw_key] = np.matmul(self.z[k-1].T, self.grad_derivatice[da_key]) + self.lambd * weights[w_key]  \n",
        "        self.grad_derivatice[db_key] = np.sum(self.grad_derivatice[da_key], axis=0).reshape(1, -1) + + self.lambd * weights[b_key]\n",
        "        self.grad_derivatice[f'dH{k-1}'] = np.matmul(self.grad_derivatice[da_key], weights[w_key].T)\n",
        "        self.grad_derivatice[f'dA{k-1}'] = np.multiply(self.grad_derivatice[f'dH{k-1}'], self.grad_activation(self.z[k-1]))\n",
        "\n",
        "    return self.grad_derivatice[f'dH{k-1}']\n",
        "\n",
        "  def fit(self, X, Y, X_val, Y_val,algo= 'GD',a = 10, eps=1e-8, beta=0.9, beta1=0.9, beta2=0.9, gamma=0.9, show_loss = False):\n",
        "    if show_loss:\n",
        "      los = []\n",
        "      accuracy = []\n",
        "    for num_epoch in tqdm(range(1, self.epochs+1), unit='epoch'):\n",
        "      m = X.shape[0]\n",
        "      \n",
        "      if algo == 'sgd':\n",
        "        for i in range(m):\n",
        "            rand_idx = np.random.randint(m)\n",
        "            x_i = X[rand_idx:rand_idx+1]\n",
        "            y_i = Y[rand_idx:rand_idx+1]\n",
        "            self.backprop(x_i, y_i)\n",
        "            for j in range(1, self.hidden_layers+1):\n",
        "              w_key, b_key, dw_key, db_key = [f'{key}{j}' for key in ['W', 'B', 'dW', 'dB']]  \n",
        "              # w_key(prev_layer_output_dim,layer_output_dim)\n",
        "              # b_key1,layer_output_dim)\n",
        "              # dw_key(prev_layer_output_dim,layer_output_dim)\n",
        "              # db_key(1,layer_output_dim)\n",
        "              self.weights[w_key] -=self.learning_rate * self.grad_derivatice[dw_key]\n",
        "              self.weights[b_key] -=self.learning_rate * self.grad_derivatice[db_key]\n",
        "        self.wandlog(num_epoch, X, Y, X_val, Y_val)\n",
        "\n",
        "      elif algo == 'momentum':\n",
        "        num_examples = X.shape[0]\n",
        "        num_batches = num_examples //self.batch_size\n",
        "        for batch in range(num_batches + 1):\n",
        "            start_index = batch *self.batch_size\n",
        "            end_index = min((batch+1)*self.batch_size, num_examples)\n",
        "            X_batch, Y_batch = X[start_index:end_index], Y[start_index:end_index]\n",
        "\n",
        "            self.backprop(X_batch, Y_batch)\n",
        "\n",
        "            for i in range(1, self.hidden_layers+1):\n",
        "              w_key, b_key, vw_key, vb_key,dw_key, db_key = [f'{key}{i}' for key in ['W', 'B', 'vw', 'vb', 'dW', 'dB']]\n",
        "              self.update_weights[vw_key] = gamma *self.update_weights[vw_key] + self.learning_rate * (self.grad_derivatice[dw_key])\n",
        "              self.update_weights[vb_key] = gamma *self.update_weights[vb_key] + self.learning_rate * (self.grad_derivatice[db_key])\n",
        "              self.weights[w_key] -= self.update_weights[vw_key]\n",
        "              self.weights[b_key] -= self.update_weights[vb_key]\n",
        "        self.wandlog(num_epoch, X, Y,X_val, Y_val)\n",
        "\n",
        "      elif algo == 'rmsprop':\n",
        "        num_examples = X.shape[0]\n",
        "        num_batches = num_examples //self.batch_size\n",
        "\n",
        "        for batch in range(num_batches + 1):\n",
        "            start_index = batch *self.batch_size\n",
        "            end_index = min((batch+1)*self.batch_size, num_examples)\n",
        "            X_batch, Y_batch = X[start_index:end_index], Y[start_index:end_index]\n",
        "\n",
        "            self.backprop(X_batch, Y_batch)\n",
        "\n",
        "            for i in range(1, self.hidden_layers+1):\n",
        "                w_key, b_key, vw_key, vb_key, dw_key, db_key = [f'{key}{i}' for key in ['W', 'B', 'vw', 'vb', 'dW', 'dB']]\n",
        "                self.update_weights[vw_key] = beta * self.update_weights[vw_key] + (1 - beta) * ((self.grad_derivatice[dw_key])**2)\n",
        "                self.update_weights[vb_key] = beta * self.update_weights[vb_key] + (1 - beta) * ((self.grad_derivatice[db_key])**2)\n",
        "                self.weights[w_key] -= (self.learning_rate / (np.sqrt(self.update_weights[vw_key] + eps))) * (self.grad_derivatice[dw_key])\n",
        "                self.weights[b_key] -= (self.learning_rate / (np.sqrt(self.update_weights[vb_key] + eps))) * (self.grad_derivatice[db_key])\n",
        "\n",
        "        self.wandlog(num_epoch, X, Y, X_val, Y_val)\n",
        "      \n",
        "      elif algo == 'adam':\n",
        "\n",
        "        num_examples = X.shape[0]\n",
        "        num_batches = num_examples //self.batch_size\n",
        "\n",
        "        for batch in range(num_batches + 1):\n",
        "            start_index = batch *self.batch_size\n",
        "            end_index = min((batch+1)*self.batch_size, num_examples)\n",
        "            X_batch, Y_batch = X[start_index:end_index], Y[start_index:end_index]\n",
        "\n",
        "            self.backprop(X_batch, Y_batch)\n",
        "\n",
        "            for i in range(1, self.hidden_layers + 1):\n",
        "                w_key, b_key, vw_key, vb_key, mw_key, mb_key = [f'{key}{i}' for key in ['W', 'B', 'vw', 'vb', 'mw', 'mb']]\n",
        "                dw_key, db_key= [f'{key}{i}' for key in ['dW', 'dB']]\n",
        "\n",
        "                self.update_weights[mw_key] = beta1 * self.update_weights[mw_key] + (1 - beta1) * self.grad_derivatice[dw_key]\n",
        "                self.update_weights[vw_key] = beta2 * self.update_weights[vw_key] + (1 - beta2) * (self.grad_derivatice[dw_key] ** 2)\n",
        "                mw_hat = self.update_weights[mw_key] / (1 - np.power(beta1, batch + 1))\n",
        "                vw_hat = self.update_weights[vw_key] / (1 - np.power(beta2, batch + 1))\n",
        "                self.weights[w_key] -= (self.learning_rate / np.sqrt(vw_hat + eps)) * mw_hat\n",
        "\n",
        "                self.update_weights[mb_key] = beta1 * self.update_weights[mb_key] + (1 - beta1) * self.grad_derivatice[db_key]\n",
        "                self.update_weights[vb_key] = beta2 * self.update_weights[vb_key] + (1 - beta2) * (self.grad_derivatice[db_key] ** 2)\n",
        "                mb_hat = self.update_weights[mb_key] / (1 - np.power(beta1, batch + 1))\n",
        "                vb_hat = self.update_weights[vb_key] / (1 - np.power(beta2, batch + 1))\n",
        "                self.weights[b_key] -= (self.learning_rate / np.sqrt(vb_hat + eps)) * mb_hat\n",
        "\n",
        "        self.wandlog(num_epoch, X, Y,X_val, Y_val)\n",
        "          \n",
        "      elif algo == 'nag':\n",
        "        num_examples = X.shape[0]\n",
        "        num_batches = num_examples //self.batch_size\n",
        "\n",
        "        temp_weights = {}\n",
        "        for i in range(1, self.hidden_layers+2):\n",
        "          w_key, b_key = [f'{key}{i}' for key in ['W', 'B']]\n",
        "          temp_weights[w_key] = np.zeros_like(self.weights[w_key])\n",
        "          temp_weights[b_key] = np.zeros_like(self.weights[b_key])\n",
        "        \n",
        "        for batch in range(num_batches + 1):\n",
        "            start_index = batch *self.batch_size\n",
        "            end_index = min((batch+1)*self.batch_size, num_examples)\n",
        "            X_batch, Y_batch = X[start_index:end_index], Y[start_index:end_index]\n",
        "\n",
        "            for i in range(1,self.hidden_layers+1):\n",
        "                w_key, b_key, vw_key, vb_key,dw_key, db_key = [f'{key}{i}' for key in ['W', 'B', 'vw', 'vb', 'dW', 'dB']]\n",
        "                self.update_weights[vw_key]=gamma*self.prev_update_weights[vw_key]\n",
        "                self.update_weights[vb_key]=gamma*self.prev_update_weights[vb_key]\n",
        "                temp_weights[w_key]=self.weights[w_key]-self.update_weights[vw_key]\n",
        "                temp_weights[b_key]=self.weights[b_key]-self.update_weights[vb_key]\n",
        "            self.backprop(X_batch,Y_batch,temp_weights)\n",
        "            for i in range(1,self.hidden_layers+1):\n",
        "                w_key, b_key, vw_key, vb_key,dw_key, db_key = [f'{key}{i}' for key in ['W', 'B', 'vw', 'vb', 'dW', 'dB']]\n",
        "                self.update_weights[vw_key] = gamma *self.update_weights[vw_key] + self.learning_rate * (self.grad_derivatice[dw_key])\n",
        "                self.update_weights[vb_key] = gamma *self.update_weights[vb_key] + self.learning_rate * (self.grad_derivatice[db_key])\n",
        "                self.weights[w_key] -= self.learning_rate * (self.update_weights[vw_key]/m)\n",
        "                self.weights[b_key] -= self.learning_rate * (self.update_weights[vb_key]/m) \n",
        "\n",
        "            self.prev_update_weights=self.update_weights\n",
        "\n",
        "        self.wandlog(num_epoch, X, Y,X_val, Y_val)\n",
        "\n",
        "      elif algo == 'nadam':\n",
        "\n",
        "        num_examples = X.shape[0]\n",
        "        num_batches = num_examples //self.batch_size\n",
        "\n",
        "        num_updates = 0\n",
        "        for i in range(1, self.hidden_layers + 1):\n",
        "            w_key, b_key, vw_key, vb_key, mw_key, mb_key = [f'{key}{i}' for key in ['W', 'B', 'vw', 'vb', 'mw', 'mb']]\n",
        "            dw_key, db_key, mw_i_key, mb_i_key = [f'{key}{i}' for key in ['dW', 'dB', 'mw_inf', 'mb_inf']]\n",
        "\n",
        "            for batch in range(num_batches + 1):\n",
        "                start_index = batch *self.batch_size\n",
        "                end_index = min((batch+1)*self.batch_size, num_examples)\n",
        "                X_batch, Y_batch = X[start_index:end_index], Y[start_index:end_index]\n",
        "\n",
        "                self.backprop(X_batch, Y_batch)\n",
        "\n",
        "                num_updates += 1\n",
        "                self.update_weights.setdefault(mw_i_key, 0) # shape: (n[i-1], n[i])\n",
        "                self.update_weights[mw_key] = beta1 * self.update_weights[mw_key] + (1 - beta1) * (self.grad_derivatice[dw_key] ) # shape: (n[i-1], n[i])\n",
        "                self.update_weights[vw_key] = beta2 * self.update_weights[vw_key] + (1 - beta2) * ((self.grad_derivatice[dw_key]) ** 2) # shape: (n[i-1], n[i])\n",
        "                mw_hat = self.update_weights[mw_key] / (1 - np.power(beta1, num_updates)) # shape: (n[i-1], n[i])\n",
        "                vw_hat = self.update_weights[vw_key] / (1 - np.power(beta2, num_updates)) # shape: (n[i-1], n[i])\n",
        "                mw_inf = beta1 * self.update_weights[mw_i_key] + (1 - beta1) * np.abs(self.grad_derivatice[dw_key]) # shape: (n[i-1], n[i])\n",
        "                mw_inf_hat = mw_inf / (1 - np.power(beta1, num_updates)) # shape: (n[i-1], n[i])\n",
        "                self.weights[w_key] -= (self.learning_rate / np.sqrt(vw_hat + eps)) * ((beta1 * mw_hat) + ((1 - beta1) * self.grad_derivatice[dw_key])) / (1 - np.power(beta2, num_updates)) + self.learning_rate * eps * np.sqrt(1 - np.power(beta2, num_updates)) * mw_inf_hat\n",
        "\n",
        "                self.update_weights.setdefault(mb_i_key, 0)\n",
        "                self.update_weights[mb_key] = beta1 * self.update_weights[mb_key] + (1 - beta1) * (self.grad_derivatice[db_key])\n",
        "                self.update_weights[vb_key] = beta2 * self.update_weights[vb_key] + (1 - beta2) * ((self.grad_derivatice[db_key]) ** 2)\n",
        "                mb_hat = self.update_weights[mb_key] / (1 - np.power(beta1, num_updates))\n",
        "                vb_hat = self.update_weights[vb_key] / (1 - np.power(beta2, num_updates))\n",
        "                mb_inf = beta1 * self.update_weights[mb_i_key] + (1 - beta1) * np.abs(self.grad_derivatice[db_key])\n",
        "                mb_inf_hat = mb_inf / (1 - np.power(beta1, num_updates))\n",
        "                self.weights[b_key] -= (self.learning_rate / np.sqrt(vb_hat + eps)) * ((beta1 * mb_hat) + ((1 - beta1) * self.grad_derivatice[db_key])) / (1 - np.power(beta2, num_updates)) + self.learning_rate * eps * np.sqrt(1 - np.power(beta2, num_updates)) * mb_inf\n",
        "        self.wandlog(num_epoch, X, Y,X_val, Y_val)\n",
        "      \n",
        "      if show_loss:\n",
        "        loss, acc = self.performance(X_val, Y_val)\n",
        "        acc = acc\n",
        "        los.append(loss)\n",
        "        accuracy.append(acc)\n",
        "\n",
        "\n",
        "\n",
        "    if show_loss:\n",
        "   \n",
        "          max_acc_index = np.argmax(accuracy)\n",
        "\n",
        "\n",
        "          plt.plot(los, label='Loss')\n",
        "          plt.plot(accuracy, label='Accuracy')\n",
        "          plt.plot(max_acc_index, los[max_acc_index], marker='o', color='red')\n",
        "          plt.plot(max_acc_index, accuracy[max_acc_index], marker='o', color='green')\n",
        "\n",
        "    \n",
        "          plt.text(max_acc_index, los[max_acc_index], f'loss @ Max val acc: ( {los[max_acc_index]:.4f})',  va='top')\n",
        "          plt.text(max_acc_index, accuracy[max_acc_index], f'Max Val acc: ({accuracy[max_acc_index]:.4f})', va='bottom')\n",
        "\n",
        "          plt.ylim([min(los + accuracy)-0.1, max(los + accuracy) + 0.1])\n",
        "\n",
        "          plt.title('Val Loss and val Accuracy with {}'.format(self.loss_function))\n",
        "          plt.xlabel('Epochs')\n",
        "          plt.ylabel('Loss / Accuracy')\n",
        "          plt.legend()\n",
        "          plt.show()\n",
        "\n",
        "\n",
        "  def predict(self, X):\n",
        "    Y_pred = (self.forward_pass(X))\n",
        "    return np.array(Y_pred).squeeze()\n",
        "  \n",
        "  def accuracy_score(self, X, Y):\n",
        "    Y_true = np.argmax(Y, axis=1).reshape(-1, 1)\n",
        "    pred_labels = np.argmax(self.predict(X), axis=1).reshape(-1,1)\n",
        "    return np.sum(pred_labels == Y_true) / len(Y)\n",
        "\n",
        "  def Loss(self, X, Y):\n",
        "      Y_pred = self.predict(X)\n",
        "      if self.loss_function == 'cross_entropy':\n",
        "          loss = -np.mean(Y * np.log(Y_pred + 1e-8))\n",
        "          max_loss = -np.mean(Y * np.log(1e-8))\n",
        "      elif self.loss_function == 'mean_squared_error':\n",
        "          loss = np.mean((Y - Y_pred)**2)\n",
        "          max_loss = np.mean(Y**2)\n",
        "      else:\n",
        "          raise ValueError('Invalid loss function')\n",
        "      loss = loss / max_loss\n",
        "      return loss\n",
        "\n",
        "  def performance(self, X_test, Y_test):\n",
        "    loss = self.Loss(X_test, Y_test)\n",
        "    accuracy = self.accuracy_score(X_test, Y_test)\n",
        "    return loss, accuracy\n",
        "\n",
        "\n",
        "  def confusion_matrix(self, X, Y):\n",
        "\n",
        "      actual_labels = np.argmax(Y, axis=1)\n",
        "      predicted_labels = np.argmax(self.forward_pass(X), axis=1)\n",
        "\n",
        "\n",
        "      available_classes = np.unique(np.concatenate((actual_labels, predicted_labels)))\n",
        "\n",
        "      confo_matrix = np.zeros((len(available_classes), len(available_classes)), dtype=int)\n",
        "      for i, actual in enumerate(available_classes):\n",
        "          for j, predicted in enumerate(available_classes):\n",
        "              confo_matrix[i,j] = np.where((actual_labels == actual) & (predicted_labels == predicted))[0].shape[0]\n",
        "      wandb.log({'confusion_matrix': wandb.plot.confusion_matrix(\n",
        "          probs=None,\n",
        "          y_true=actual_labels,\n",
        "          preds=predicted_labels,\n",
        "          class_names=list(available_classes),\n",
        "          title='Confusion Matrix'\n",
        "      )})\n",
        "\n",
        "      return confo_matrix\n",
        "\n",
        "\n",
        "  def confo_matrixplot(self, confusion_matrix, title='Confusion Matrix', cmap='PuBu'):\n",
        "    confusion_matrix = np.array(confusion_matrix)\n",
        "    confusion_matrix = confusion_matrix / np.sum(confusion_matrix, axis=1, keepdims=True)\n",
        "    plt.matshow(confusion_matrix, cmap=cmap)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(confusion_matrix))\n",
        "    plt.xticks(tick_marks)\n",
        "    plt.yticks(tick_marks)\n",
        "    plt.ylabel('Actual')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "  def wandlog(self, num_epoch, X, Y,X_val, Y_val):\n",
        "    accuracy = self.accuracy_score(X, Y)\n",
        "    loss_train = self.Loss(X, Y)\n",
        "    loss_valid = self.Loss(X_val, Y_val)\n",
        "    val_accuracy = self.accuracy_score(X_val, Y_val)\n",
        "    wandb.log({'epoch': num_epoch,           \n",
        "              'loss': loss_train,\n",
        "              'accuracy': accuracy,\n",
        "              'val_loss': loss_valid,\n",
        "              'val_accuracy': val_accuracy})\n",
        "    \n",
        "    if num_epoch % 5== 0:\n",
        "      accuracy = self.accuracy_score(X, Y)\n",
        "      loss_train = self.Loss(X, Y)\n",
        "      loss_valid = self.Loss(X_val, Y_val)\n",
        "      val_accuracy = self.accuracy_score(X_val, Y_val)\n",
        "      library = {'epoch': num_epoch,           \n",
        "              'loss': loss_train,\n",
        "              'accuracy': accuracy,\n",
        "              'val_loss': loss_valid,\n",
        "              'val_accuracy': val_accuracy}\n",
        "\n",
        "      print('Epoch: {}, Train Loss: {}, Train Accuracy: {}, Val Loss: {}, Val Accuracy: {}'.format(library['epoch'], library['loss'], library['accuracy'], library['val_loss'], library['val_accuracy']))\n",
        "      if num_epoch == self.epochs:\n",
        "        print('Model trained successfully !')\n",
        "\n",
        "# model = FFNN(X_train, Y_train,\n",
        "#                   epochs = 2, \n",
        "#                   hidden_layer_count = 3,\n",
        "#                   hidden_layers =  [64, 64, 64],\n",
        "#                   learning_rate = 0.0001,\n",
        "#                   batch_size = 32,\n",
        "#                   activation='ReLU',\n",
        "#                   weight_init='random',\n",
        "#                   loss = 'cross_entropy',\n",
        "#                   weight_decay = 0.0005)\n",
        "# model.fit(X_train, Y_train, X_val, Y_val,algo= 'adam', a = 1, show_loss = True) \n",
        "# confusion_matrix = model.confusion_matrix(X_test, Y_test)\n",
        "# model.confo_matrixplot(confusion_matrix)"
      ],
      "metadata": {
        "id": "roqXcqcDgrri",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "92bf8ed8-c995-49f7-8fae-5f57017a2965"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33med22s009\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230311_033540-eoqaawki</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ed22s009/Question_4_Best_Model/runs/eoqaawki' target=\"_blank\">still-sun-226</a></strong> to <a href='https://wandb.ai/ed22s009/Question_4_Best_Model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/ed22s009/Question_4_Best_Model' target=\"_blank\">https://wandb.ai/ed22s009/Question_4_Best_Model</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/ed22s009/Question_4_Best_Model/runs/eoqaawki' target=\"_blank\">https://wandb.ai/ed22s009/Question_4_Best_Model/runs/eoqaawki</a>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rough Work"
      ],
      "metadata": {
        "id": "aMZRi3CQAFql"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# init_methods = ['random', 'Xavier']\n",
        "# activation_functions = ['sigmoid', 'tanh', 'ReLU']\n",
        "# algos = ['sgd', 'momentum', 'nag', 'rmsprop', 'adam','nadam']\n",
        "# losses = ['cross_entropy', 'mean_squared_error']\n",
        "# c = 0\n",
        "# d = 0\n",
        "# for init_method in init_methods:\n",
        "#     for activation_function in activation_functions:\n",
        "#         for algo in algos:\n",
        "#           for loss in losses:\n",
        "\n",
        "#             model = FFNN(X_train, Y_train,\n",
        "#                           epochs = 1, \n",
        "#                           hidden_layer_count = 1,\n",
        "#                           hidden_layers =  [10],\n",
        "#                           learning_rate = 0.0001,\n",
        "#                           batch_size = 32,\n",
        "#                           activation=activation_function,\n",
        "#                           weight_init=init_method,\n",
        "#                           loss = loss,\n",
        "#                           weight_decay = 0.0005)\n",
        "#             model.fit(X_train, Y_train, X_val, Y_val,algo= algo)\n",
        "#             c = c + 1\n",
        "\n",
        "#             print(c)"
      ],
      "metadata": {
        "id": "3j8_-hb8268o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "5"
      ],
      "metadata": {
        "id": "fVbMzOOFGF4L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}