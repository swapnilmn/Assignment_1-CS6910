{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOZT8vsf6VgfvDoiUEdf+rl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swapnilmn/Assignment_1_CS6910/blob/master/Question_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Question 3 (24 Marks) Implement the backpropagation algorithm with support for the following optimisation functions\n",
        "            sgd\n",
        "            momentum based gradient descent\n",
        "            nesterov accelerated gradient descent\n",
        "            rmsprop\n",
        "            adam\n",
        "            nadam\n",
        "\n",
        "(12 marks for the backpropagation framework and 2 marks for each of the optimisation algorithms above)\n",
        "\n",
        "We will check the code for implementation and ease of use (e.g., how easy it is to add a new optimisation algorithm such as Eve). Note that the code should be flexible enough to work with different batch sizes."
      ],
      "metadata": {
        "id": "3kVFZwNkEA7D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries "
      ],
      "metadata": {
        "id": "_dP0oEKrgmQc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZcFz8GygBp5"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import fashion_mnist, mnist\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import copy\n",
        "from sklearn.model_selection import train_test_split \n",
        "import pandas as pd\n",
        "!pip install wandb\n",
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Processing"
      ],
      "metadata": {
        "id": "W-FlxB4-gnqT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OneHotEncoder_from_scratch:\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.categories = None\n",
        "    def fit(self, X):\n",
        "        self.categories =[]\n",
        "        for i in range(X.shape[1]):\n",
        "            feature_categories =list(set(X[:, i]))\n",
        "            self.categories.append(feature_categories)\n",
        "            \n",
        "    def transform(self, X):\n",
        "        one_hot_vector = []\n",
        "\n",
        "        for i in range(X.shape[0]):\n",
        "            one_hot_row = []\n",
        "            for j in range(X.shape[1]):\n",
        "\n",
        "                category_index = self.categories[j].index(X[i, j])\n",
        "                category_one_hot =[0] *len(self.categories[j])\n",
        "                category_one_hot[category_index] = 1\n",
        "\n",
        "                one_hot_row.extend(category_one_hot)\n",
        "            one_hot_vector.append(one_hot_row)\n",
        "        return np.array(one_hot_vector)"
      ],
      "metadata": {
        "id": "YTDGKrnOgGwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = 'fashion_mnist'\n",
        "\n",
        "def dataset_type(dataset):\n",
        "  if dataset == 'fashion_mnist':\n",
        "      (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "  elif dataset == 'mnist':\n",
        "      (train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "  else:\n",
        "      raise ValueError('Invalid dataset name')\n",
        "  X_train, X_val, Y_train, Y_val = train_test_split(train_images, train_labels, test_size=0.1, random_state=42)\n",
        "  train_input = []\n",
        "  for i in range(len(X_train)):\n",
        "      train_input.append(list(np.concatenate(X_train[i]).flat))\n",
        "\n",
        "  val_input = []\n",
        "  for i in range(len(X_val)):\n",
        "      val_input.append(list(np.concatenate(X_val[i]).flat))\n",
        "\n",
        "  test_input = []\n",
        "  for i in range(len(test_images)):\n",
        "      test_input.append(list(np.concatenate(test_images[i]).flat))\n",
        "  Y_train = np.array(Y_train)\n",
        "  Y_val = np.array(Y_val)\n",
        "  Y_test = np.array(test_labels)\n",
        "\n",
        "  X_train = np.array(train_input) / 255.0\n",
        "  X_test = np.array(test_input) / 255.0\n",
        "  X_val = np.array(val_input) / 255.0\n",
        "\n",
        "  enc = OneHotEncoder_from_scratch()\n",
        "  enc.fit(Y_train.reshape(-1, 1))\n",
        "  Y_train = enc.transform(Y_train.reshape(-1, 1))\n",
        "  Y_val = enc.transform(Y_val.reshape(-1, 1))\n",
        "  Y_test = enc.transform(Y_test.reshape(-1, 1))\n",
        "\n",
        "  return X_train, X_val, X_test, Y_train, Y_val, Y_test\n",
        "\n",
        "dataset = 'fashion_mnist'\n",
        "X_train, X_val, X_test, Y_train, Y_val, Y_test = dataset_type(dataset)\n",
        "\n",
        "print(Y_train.shape, Y_val.shape, Y_test.shape)\n",
        "print(X_train.shape, X_val.shape, X_test.shape)"
      ],
      "metadata": {
        "id": "wzdukjGZgZeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Class of FeedForward Neural Network"
      ],
      "metadata": {
        "id": "DXX2kyFig0i5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(project = 'Question_4_Best_Model', entity = 'ed22s009')\n",
        "\n",
        "class FFNN:\n",
        "  def __init__(self, X, Y,\n",
        "               epochs = 100, \n",
        "               hidden_layer_count = 4,\n",
        "               hidden_layers =  [32, 64, 128, 256],\n",
        "               learning_rate = 0.001,\n",
        "               batch_size = 32,\n",
        "               activation='tanh',\n",
        "               weight_init='random',\n",
        "               loss = 'mean_squared_error',\n",
        "               weight_decay = 0):\n",
        "    \n",
        "    self.inputs =X.shape[1] # Number of inputs\n",
        "    self.outputs= Y.shape[1] # Number of outputs\n",
        "    self.epochs = epochs\n",
        "    self.hidden_layers = hidden_layer_count  # Number of hidden layers \n",
        "    self.network_size= [self.inputs] + hidden_layers +[self.outputs] # input layer + hidden layers + output layers\n",
        "    self.learning_rate = learning_rate\n",
        "    self.batch_size = batch_size\n",
        "    self.weights={} # It will create dictionary for weights and biases\n",
        "    self.weights_h = []\n",
        "    self.num_classes = Y.shape[1]\n",
        "    self.weight_init = weight_init\n",
        "    self.activation_function = activation\n",
        "    self.loss_function = loss\n",
        "    self.lambd = weight_decay\n",
        "    np.random.seed(0)  # We will set seed value so that it will generate same random numebers every time\n",
        "\n",
        "    self.grad_derivatice={}\n",
        "    self.update_weights={}\n",
        "    self.prev_update_weights={}\n",
        "    for i in range(1,self.hidden_layers+1):\n",
        "      vw_key, vb_key, mb_key, mw_key = [f'{key}{i}' for key in ['vw', 'vb', 'mb', 'mw']]\n",
        "      self.update_weights[vw_key]=0\n",
        "      self.update_weights[vb_key]=0\n",
        "      self.update_weights[mb_key]=0\n",
        "      self.update_weights[mw_key]=0\n",
        "      self.prev_update_weights[vw_key]=0\n",
        "      self.prev_update_weights[vb_key]=0\n",
        "\n",
        "    # for creating initial weights\n",
        "    if self.weight_init == 'random':\n",
        "      for i in range(1, self.hidden_layers + 2):\n",
        "          weight_shape = (self.network_size[i - 1], self.network_size[i])\n",
        "          weight_scale = 0.1\n",
        "          self.weights[f'W{i}'] = np.random.normal(scale=weight_scale, size=weight_shape)*0.1\n",
        "          # we are mulliplying ny 0.1 to get less wrights\n",
        "          bias_shape = (1, self.network_size[i])\n",
        "          self.weights[f'B{i}'] = np.zeros(bias_shape)\n",
        "\n",
        "    if self.weight_init == 'Xavier':\n",
        "      for i in range(1, self.hidden_layers + 2):\n",
        "          weight_shape = (self.network_size[i - 1], self.network_size[i])\n",
        "          weight_scale = 0.1\n",
        "          self.weights[f'W{i}'] = np.random.normal(scale=weight_scale, size=weight_shape)*np.sqrt(1/self.network_size[i-1])\n",
        "          \n",
        "          bias_shape = (1, self.network_size[i])\n",
        "          self.weights[f'B{i}'] = np.zeros(bias_shape)\n",
        "\n",
        "\n",
        "\n",
        "  def forward_activation(self, X):\n",
        "      activation_functions = {\n",
        "          'sigmoid': lambda x: 1.0 / (1.0 + np.exp(-x)),\n",
        "          'tanh': np.tanh,\n",
        "          'ReLU': lambda x: np.maximum(0, x)\n",
        "      }\n",
        "      activation_function = activation_functions.get(self.activation_function)\n",
        "      if activation_function:\n",
        "          return activation_function(X)\n",
        "      else:\n",
        "          raise ValueError(f\"Unknown activation function '{self.activation_function}'\")\n",
        "\n",
        "\n",
        "  def grad_activation(self, X):\n",
        "      activation_gradients = {\n",
        "          'sigmoid': lambda x: x * (1 - x),\n",
        "          'tanh': lambda x: 1 - np.square(x),\n",
        "          'ReLU': lambda x: 1.0 * (x > 0)\n",
        "      }\n",
        "      gradient_function = activation_gradients.get(self.activation_function)\n",
        "      if gradient_function:\n",
        "          return gradient_function(X)\n",
        "      else:\n",
        "          raise ValueError(f\"Unknown activation function '{self.activation_function}'\")\n",
        "\n",
        "  def softmax(self, X):\n",
        "    exps =np.exp(X - np.max(X, axis=1, keepdims=True))\n",
        "    return  exps /np.sum(exps, axis=1, keepdims=True)\n",
        "  \n",
        "\n",
        "  def forward_pass(self, X, weights=None):\n",
        "    #X: shape (batch_size, input_dim)\n",
        "    if weights is None:\n",
        "\n",
        "        weights =self.weights\n",
        "    #weights:a dictionary containing weight and bias parameters for each layer\n",
        "    self.z = {}\n",
        "    self.h = {}\n",
        "    self.h[0] = X\n",
        "    for i in range(self.hidden_layers):\n",
        "        self.z[i+1] = self.h[i]@weights[f'W{i+1}'] + weights[f'B{i+1}'] # weights[f'W{i+1}'](input_dim, output_dim) + # weights[f'B{i+1}'](1, output_dim)\n",
        "        #z is dictionary containing the output of each layer's activationfunction\n",
        "        self.h[i+1] = self.forward_activation(self.z[i+1]) # self.z[i+1](batch_size, output_dim)\n",
        "    self.z[self.hidden_layers+1] = self.h[self.hidden_layers] @ weights[f'W{self.hidden_layers+1}'] + weights[f'B{self.hidden_layers+1}']\n",
        "\n",
        "    #self.z[self.hidden_layers+1](batch_size, output_dim)\n",
        "    self.h[self.hidden_layers+1]=  self.softmax(self.z[self.hidden_layers+1])\n",
        "\n",
        "    return self.h[self.hidden_layers+1]\n",
        "\n",
        "\n",
        "\n",
        "  def backprop(self, X, Y, weights=None):\n",
        "    #X(batch_size, input_size)\n",
        "    #Y(batch_size, output_size)\n",
        "    if weights is None:\n",
        "        weights = self.weights\n",
        "\n",
        "    self.forward_pass(X, weights)\n",
        "    self.grad_derivatice = {}\n",
        "    total_layers= self.hidden_layers + 1\n",
        "\n",
        "    if self.loss_function == 'cross_entropy':\n",
        "        self.grad_derivatice[f'dA{total_layers}'] =  (self.h[total_layers] - Y)\n",
        "    elif self.loss_function == 'mean_squared_error':\n",
        "        self.grad_derivatice[f'dA{total_layers}'] = (1/X.shape[0]) * 2 * (self.h[total_layers] - Y)\n",
        "\n",
        "    for k in range(total_layers, 0, -1):\n",
        "        w_key, b_key, dw_key, db_key, da_key = [f'{key}{k}' for key in ['W', 'B', 'dW', 'dB', 'dA']]\n",
        "        self.grad_derivatice[dw_key] = np.matmul(self.h[k-1].T, self.grad_derivatice[da_key]) + self.lambd * weights[w_key]  \n",
        "        self.grad_derivatice[db_key] = np.sum(self.grad_derivatice[da_key], axis=0).reshape(1, -1) + + self.lambd * weights[b_key]\n",
        "        self.grad_derivatice[f'dH{k-1}'] = np.matmul(self.grad_derivatice[da_key], weights[w_key].T)\n",
        "        self.grad_derivatice[f'dA{k-1}'] = np.multiply(self.grad_derivatice[f'dH{k-1}'], self.grad_activation(self.h[k-1]))\n",
        "\n",
        "    return self.grad_derivatice[f'dH{k-1}']\n",
        "\n",
        "  \n",
        " \n",
        "\n",
        "  def fit(self, X, Y, X_val, Y_val,algo= 'GD',a = 10, eps=1e-8, beta=0.9, beta1=0.9, beta2=0.9, gamma=0.9, show_loss = False):\n",
        "    if show_loss:\n",
        "      los = []\n",
        "      accuracy = []\n",
        "    for num_epoch in tqdm(range(1, self.epochs+1), unit='epoch'):\n",
        "      m = X.shape[0]\n",
        "      \n",
        "      if algo == 'sgd':\n",
        "        for i in range(m):\n",
        "            rand_idx = np.random.randint(m)\n",
        "            x_i = X[rand_idx:rand_idx+1]\n",
        "            y_i = Y[rand_idx:rand_idx+1]\n",
        "            self.backprop(x_i, y_i)\n",
        "            for j in range(1, self.hidden_layers+1):\n",
        "              w_key, b_key, dw_key, db_key = [f'{key}{j}' for key in ['W', 'B', 'dW', 'dB']]  \n",
        "              # w_key(prev_layer_output_dim,layer_output_dim)\n",
        "              # b_key1,layer_output_dim)\n",
        "              # dw_key(prev_layer_output_dim,layer_output_dim)\n",
        "              # db_key(1,layer_output_dim)\n",
        "              self.weights[w_key] -=self.learning_rate * self.grad_derivatice[dw_key]\n",
        "              self.weights[b_key] -=self.learning_rate * self.grad_derivatice[db_key]\n",
        "        self.wandlog(num_epoch, X, Y, X_val, Y_val)\n",
        "\n",
        "      elif algo == 'momentum':\n",
        "        num_examples = X.shape[0]\n",
        "        num_batches = num_examples //self.batch_size\n",
        "        for batch in range(num_batches + 1):\n",
        "            start_index = batch *self.batch_size\n",
        "            end_index = min((batch+1)*self.batch_size, num_examples)\n",
        "            X_batch, Y_batch = X[start_index:end_index], Y[start_index:end_index]\n",
        "\n",
        "            self.backprop(X_batch, Y_batch)\n",
        "\n",
        "            for i in range(1, self.hidden_layers+1):\n",
        "              w_key, b_key, vw_key, vb_key,dw_key, db_key = [f'{key}{i}' for key in ['W', 'B', 'vw', 'vb', 'dW', 'dB']]\n",
        "              self.update_weights[vw_key] = gamma *self.update_weights[vw_key] + self.learning_rate * (self.grad_derivatice[dw_key])\n",
        "              self.update_weights[vb_key] = gamma *self.update_weights[vb_key] + self.learning_rate * (self.grad_derivatice[db_key])\n",
        "              self.weights[w_key] -= self.update_weights[vw_key]\n",
        "              self.weights[b_key] -= self.update_weights[vb_key]\n",
        "        self.wandlog(num_epoch, X, Y,X_val, Y_val)\n",
        "\n",
        "      elif algo == 'rmsprop':\n",
        "        num_examples = X.shape[0]\n",
        "        num_batches = num_examples //self.batch_size\n",
        "\n",
        "        for batch in range(num_batches + 1):\n",
        "            start_index = batch *self.batch_size\n",
        "            end_index = min((batch+1)*self.batch_size, num_examples)\n",
        "            X_batch, Y_batch = X[start_index:end_index], Y[start_index:end_index]\n",
        "\n",
        "            self.backprop(X_batch, Y_batch)\n",
        "\n",
        "            for i in range(1, self.hidden_layers+1):\n",
        "                w_key, b_key, vw_key, vb_key, dw_key, db_key = [f'{key}{i}' for key in ['W', 'B', 'vw', 'vb', 'dW', 'dB']]\n",
        "                self.update_weights[vw_key] = beta * self.update_weights[vw_key] + (1 - beta) * ((self.grad_derivatice[dw_key])**2)\n",
        "                self.update_weights[vb_key] = beta * self.update_weights[vb_key] + (1 - beta) * ((self.grad_derivatice[db_key])**2)\n",
        "                self.weights[w_key] -= (self.learning_rate / (np.sqrt(self.update_weights[vw_key] + eps))) * (self.grad_derivatice[dw_key])\n",
        "                self.weights[b_key] -= (self.learning_rate / (np.sqrt(self.update_weights[vb_key] + eps))) * (self.grad_derivatice[db_key])\n",
        "\n",
        "        self.wandlog(num_epoch, X, Y, X_val, Y_val)\n",
        "      \n",
        "      elif algo == 'adam':\n",
        "\n",
        "        num_examples = X.shape[0]\n",
        "        num_batches = num_examples //self.batch_size\n",
        "\n",
        "        for batch in range(num_batches + 1):\n",
        "            start_index = batch *self.batch_size\n",
        "            end_index = min((batch+1)*self.batch_size, num_examples)\n",
        "            X_batch, Y_batch = X[start_index:end_index], Y[start_index:end_index]\n",
        "\n",
        "            self.backprop(X_batch, Y_batch)\n",
        "\n",
        "            for i in range(1, self.hidden_layers + 1):\n",
        "                w_key, b_key, vw_key, vb_key, mw_key, mb_key = [f'{key}{i}' for key in ['W', 'B', 'vw', 'vb', 'mw', 'mb']]\n",
        "                dw_key, db_key= [f'{key}{i}' for key in ['dW', 'dB']]\n",
        "\n",
        "                self.update_weights[mw_key] = beta1 * self.update_weights[mw_key] + (1 - beta1) * self.grad_derivatice[dw_key]\n",
        "                self.update_weights[vw_key] = beta2 * self.update_weights[vw_key] + (1 - beta2) * (self.grad_derivatice[dw_key] ** 2)\n",
        "                mw_hat = self.update_weights[mw_key] / (1 - np.power(beta1, batch + 1))\n",
        "                vw_hat = self.update_weights[vw_key] / (1 - np.power(beta2, batch + 1))\n",
        "                self.weights[w_key] -= (self.learning_rate / np.sqrt(vw_hat + eps)) * mw_hat\n",
        "\n",
        "                self.update_weights[mb_key] = beta1 * self.update_weights[mb_key] + (1 - beta1) * self.grad_derivatice[db_key]\n",
        "                self.update_weights[vb_key] = beta2 * self.update_weights[vb_key] + (1 - beta2) * (self.grad_derivatice[db_key] ** 2)\n",
        "                mb_hat = self.update_weights[mb_key] / (1 - np.power(beta1, batch + 1))\n",
        "                vb_hat = self.update_weights[vb_key] / (1 - np.power(beta2, batch + 1))\n",
        "                self.weights[b_key] -= (self.learning_rate / np.sqrt(vb_hat + eps)) * mb_hat\n",
        "\n",
        "        self.wandlog(num_epoch, X, Y,X_val, Y_val)\n",
        "          \n",
        "      elif algo == 'nag':\n",
        "        num_examples = X.shape[0]\n",
        "        num_batches = num_examples //self.batch_size\n",
        "\n",
        "        temp_weights = {}\n",
        "        for i in range(1, self.hidden_layers+2):\n",
        "          w_key, b_key = [f'{key}{i}' for key in ['W', 'B']]\n",
        "          temp_weights[w_key] = np.zeros_like(self.weights[w_key])\n",
        "          temp_weights[b_key] = np.zeros_like(self.weights[b_key])\n",
        "        \n",
        "        for batch in range(num_batches + 1):\n",
        "            start_index = batch *self.batch_size\n",
        "            end_index = min((batch+1)*self.batch_size, num_examples)\n",
        "            X_batch, Y_batch = X[start_index:end_index], Y[start_index:end_index]\n",
        "\n",
        "            for i in range(1,self.hidden_layers+1):\n",
        "                w_key, b_key, vw_key, vb_key,dw_key, db_key = [f'{key}{i}' for key in ['W', 'B', 'vw', 'vb', 'dW', 'dB']]\n",
        "                self.update_weights[vw_key]=gamma*self.prev_update_weights[vw_key]\n",
        "                self.update_weights[vb_key]=gamma*self.prev_update_weights[vb_key]\n",
        "                temp_weights[w_key]=self.weights[w_key]-self.update_weights[vw_key]\n",
        "                temp_weights[b_key]=self.weights[b_key]-self.update_weights[vb_key]\n",
        "            self.backprop(X_batch,Y_batch,temp_weights)\n",
        "            for i in range(1,self.hidden_layers+1):\n",
        "                w_key, b_key, vw_key, vb_key,dw_key, db_key = [f'{key}{i}' for key in ['W', 'B', 'vw', 'vb', 'dW', 'dB']]\n",
        "                self.update_weights[vw_key] = gamma *self.update_weights[vw_key] + self.learning_rate * (self.grad_derivatice[dw_key])\n",
        "                self.update_weights[vb_key] = gamma *self.update_weights[vb_key] + self.learning_rate * (self.grad_derivatice[db_key])\n",
        "                self.weights[w_key] -= self.learning_rate * (self.update_weights[vw_key]/m)\n",
        "                self.weights[b_key] -= self.learning_rate * (self.update_weights[vb_key]/m) \n",
        "\n",
        "            self.prev_update_weights=self.update_weights\n",
        "\n",
        "        self.wandlog(num_epoch, X, Y,X_val, Y_val)\n",
        "\n",
        "      elif algo == 'nadam':\n",
        "\n",
        "        num_examples = X.shape[0]\n",
        "        num_batches = num_examples //self.batch_size\n",
        "\n",
        "        num_updates = 0\n",
        "        for i in range(1, self.hidden_layers + 1):\n",
        "            w_key, b_key, vw_key, vb_key, mw_key, mb_key = [f'{key}{i}' for key in ['W', 'B', 'vw', 'vb', 'mw', 'mb']]\n",
        "            dw_key, db_key, mw_i_key, mb_i_key = [f'{key}{i}' for key in ['dW', 'dB', 'mw_inf', 'mb_inf']]\n",
        "\n",
        "            for batch in range(num_batches + 1):\n",
        "                start_index = batch *self.batch_size\n",
        "                end_index = min((batch+1)*self.batch_size, num_examples)\n",
        "                X_batch, Y_batch = X[start_index:end_index], Y[start_index:end_index]\n",
        "\n",
        "                self.backprop(X_batch, Y_batch)\n",
        "\n",
        "                num_updates += 1\n",
        "                self.update_weights.setdefault(mw_i_key, 0) # shape: (n[i-1], n[i])\n",
        "                self.update_weights[mw_key] = beta1 * self.update_weights[mw_key] + (1 - beta1) * (self.grad_derivatice[dw_key] ) # shape: (n[i-1], n[i])\n",
        "                self.update_weights[vw_key] = beta2 * self.update_weights[vw_key] + (1 - beta2) * ((self.grad_derivatice[dw_key]) ** 2) # shape: (n[i-1], n[i])\n",
        "                mw_hat = self.update_weights[mw_key] / (1 - np.power(beta1, num_updates)) # shape: (n[i-1], n[i])\n",
        "                vw_hat = self.update_weights[vw_key] / (1 - np.power(beta2, num_updates)) # shape: (n[i-1], n[i])\n",
        "                mw_inf = beta1 * self.update_weights[mw_i_key] + (1 - beta1) * np.abs(self.grad_derivatice[dw_key]) # shape: (n[i-1], n[i])\n",
        "                mw_inf_hat = mw_inf / (1 - np.power(beta1, num_updates)) # shape: (n[i-1], n[i])\n",
        "                self.weights[w_key] -= (self.learning_rate / np.sqrt(vw_hat + eps)) * ((beta1 * mw_hat) + ((1 - beta1) * self.grad_derivatice[dw_key])) / (1 - np.power(beta2, num_updates)) + self.learning_rate * eps * np.sqrt(1 - np.power(beta2, num_updates)) * mw_inf_hat\n",
        "\n",
        "                self.update_weights.setdefault(mb_i_key, 0)\n",
        "                self.update_weights[mb_key] = beta1 * self.update_weights[mb_key] + (1 - beta1) * (self.grad_derivatice[db_key])\n",
        "                self.update_weights[vb_key] = beta2 * self.update_weights[vb_key] + (1 - beta2) * ((self.grad_derivatice[db_key]) ** 2)\n",
        "                mb_hat = self.update_weights[mb_key] / (1 - np.power(beta1, num_updates))\n",
        "                vb_hat = self.update_weights[vb_key] / (1 - np.power(beta2, num_updates))\n",
        "                mb_inf = beta1 * self.update_weights[mb_i_key] + (1 - beta1) * np.abs(self.grad_derivatice[db_key])\n",
        "                mb_inf_hat = mb_inf / (1 - np.power(beta1, num_updates))\n",
        "                self.weights[b_key] -= (self.learning_rate / np.sqrt(vb_hat + eps)) * ((beta1 * mb_hat) + ((1 - beta1) * self.grad_derivatice[db_key])) / (1 - np.power(beta2, num_updates)) + self.learning_rate * eps * np.sqrt(1 - np.power(beta2, num_updates)) * mb_inf\n",
        "        self.wandlog(num_epoch, X, Y,X_val, Y_val)\n",
        "      \n",
        "      if show_loss:\n",
        "        loss, acc = self.performance(X_val, Y_val)\n",
        "        acc = acc\n",
        "        los.append(loss)\n",
        "        accuracy.append(acc)\n",
        "\n",
        "\n",
        "\n",
        "    if show_loss:\n",
        "   \n",
        "          max_acc_index = np.argmax(accuracy)\n",
        "\n",
        "\n",
        "          plt.plot(los, label='Loss')\n",
        "          plt.plot(accuracy, label='Accuracy')\n",
        "          plt.plot(max_acc_index, los[max_acc_index], marker='o', color='red')\n",
        "          plt.plot(max_acc_index, accuracy[max_acc_index], marker='o', color='green')\n",
        "\n",
        "    \n",
        "          plt.text(max_acc_index, los[max_acc_index], f'loss @ Max val acc: ( {los[max_acc_index]:.4f})',  va='top')\n",
        "          plt.text(max_acc_index, accuracy[max_acc_index], f'Max Val acc: ({accuracy[max_acc_index]:.4f})', va='bottom')\n",
        "\n",
        "          plt.ylim([min(los + accuracy)-0.1, max(los + accuracy) + 0.1])\n",
        "\n",
        "          plt.title('Val Loss and val Accuracy with {}'.format(self.loss_function))\n",
        "          plt.xlabel('Epochs')\n",
        "          plt.ylabel('Loss / Accuracy')\n",
        "          plt.legend()\n",
        "          plt.show()\n",
        "\n",
        "\n",
        "  def predict(self, X):\n",
        "    Y_pred = (self.forward_pass(X))\n",
        "    return np.array(Y_pred).squeeze()\n",
        "  \n",
        "  def accuracy_score(self, X, Y):\n",
        "    Y_true = np.argmax(Y, axis=1).reshape(-1, 1)\n",
        "    pred_labels = np.argmax(self.predict(X), axis=1).reshape(-1,1)\n",
        "    return np.sum(pred_labels == Y_true) / len(Y)\n",
        "\n",
        "  def Loss(self, X, Y):\n",
        "      Y_pred = self.predict(X)\n",
        "      if self.loss_function == 'cross_entropy':\n",
        "          loss = -np.mean(Y * np.log(Y_pred + 1e-8))\n",
        "          max_loss = -np.mean(Y * np.log(1e-8))\n",
        "      elif self.loss_function == 'mean_squared_error':\n",
        "          loss = np.mean((Y - Y_pred)**2)\n",
        "          max_loss = np.mean(Y**2)\n",
        "      else:\n",
        "          raise ValueError('Invalid loss function')\n",
        "      loss = loss / max_loss\n",
        "      return loss\n",
        "\n",
        "  def performance(self, X_test, Y_test):\n",
        "    loss = self.Loss(X_test, Y_test)\n",
        "    accuracy = self.accuracy_score(X_test, Y_test)\n",
        "    return loss, accuracy\n",
        "\n",
        "\n",
        "  def confusion_matrix(self, X, Y):\n",
        "\n",
        "      actual_labels = np.argmax(Y, axis=1)\n",
        "      predicted_labels = np.argmax(self.forward_pass(X), axis=1)\n",
        "\n",
        "\n",
        "      available_classes = np.unique(np.concatenate((actual_labels, predicted_labels)))\n",
        "\n",
        "      confo_matrix = np.zeros((len(available_classes), len(available_classes)), dtype=int)\n",
        "      for i, actual in enumerate(available_classes):\n",
        "          for j, predicted in enumerate(available_classes):\n",
        "              confo_matrix[i,j] = np.where((actual_labels == actual) & (predicted_labels == predicted))[0].shape[0]\n",
        "      wandb.log({'confusion_matrix': wandb.plot.confusion_matrix(\n",
        "          probs=None,\n",
        "          y_true=actual_labels,\n",
        "          preds=predicted_labels,\n",
        "          class_names=list(available_classes),\n",
        "          title='Confusion Matrix'\n",
        "      )})\n",
        "\n",
        "      return confo_matrix\n",
        "\n",
        "\n",
        "  def confo_matrixplot(self, confusion_matrix, title='Confusion Matrix', cmap='PuBu'):\n",
        "    confusion_matrix = np.array(confusion_matrix)\n",
        "    confusion_matrix = confusion_matrix / np.sum(confusion_matrix, axis=1, keepdims=True)\n",
        "    plt.matshow(confusion_matrix, cmap=cmap)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(confusion_matrix))\n",
        "    plt.xticks(tick_marks)\n",
        "    plt.yticks(tick_marks)\n",
        "    plt.ylabel('Actual')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "  def wandlog(self, num_epoch, X, Y,X_val, Y_val):\n",
        "    accuracy = self.accuracy_score(X, Y)\n",
        "    loss_train = self.Loss(X, Y)\n",
        "    loss_valid = self.Loss(X_val, Y_val)\n",
        "    val_accuracy = self.accuracy_score(X_val, Y_val)\n",
        "    wandb.log({'epoch': num_epoch,           \n",
        "              'loss': loss_train,\n",
        "              'accuracy': accuracy,\n",
        "              'val_loss': loss_valid,\n",
        "              'val_accuracy': val_accuracy})\n",
        "    \n",
        "    if num_epoch % 5== 0:\n",
        "      accuracy = self.accuracy_score(X, Y)\n",
        "      loss_train = self.Loss(X, Y)\n",
        "      loss_valid = self.Loss(X_val, Y_val)\n",
        "      val_accuracy = self.accuracy_score(X_val, Y_val)\n",
        "      library = {'epoch': num_epoch,           \n",
        "              'loss': loss_train,\n",
        "              'accuracy': accuracy,\n",
        "              'val_loss': loss_valid,\n",
        "              'val_accuracy': val_accuracy}\n",
        "\n",
        "      print('Epoch: {}, Train Loss: {}, Train Accuracy: {}, Val Loss: {}, Val Accuracy: {}'.format(library['epoch'], library['loss'], library['accuracy'], library['val_loss'], library['val_accuracy']))\n",
        "      if num_epoch == self.epochs:\n",
        "        print('Model trained successfully !')\n",
        "\n",
        "\n",
        "# model = FFNN(X_train, Y_train,\n",
        "#                   epochs = 20, \n",
        "#                   hidden_layer_count = 3,\n",
        "#                   hidden_layers =  [512, 512, 512],\n",
        "#                   learning_rate = 0.001,\n",
        "#                   batch_size = 64,\n",
        "#                   activation='tanh',\n",
        "#                   weight_init='random',\n",
        "#                   loss = 'cross_entropy',\n",
        "#                   weight_decay = 0.05)\n",
        "# model.fit(X_train, Y_train, X_val, Y_val,algo= 'adam', a = 1, show_loss = True) \n",
        "# confusion_matrix = model.confusion_matrix(X_test, Y_test)\n",
        "# model.confo_matrixplot(confusion_matrix)\n",
        "\n"
      ],
      "metadata": {
        "id": "roqXcqcDgrri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rough Work"
      ],
      "metadata": {
        "id": "aMZRi3CQAFql"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# init_methods = ['random', 'Xavier']\n",
        "# activation_functions = ['sigmoid', 'tanh', 'ReLU']\n",
        "# algos = ['sgd', 'momentum', 'nag', 'rmsprop', 'adam','nadam']\n",
        "# losses = ['cross_entropy', 'mean_squared_error']\n",
        "# c = 0\n",
        "# d = 0\n",
        "# for init_method in init_methods:\n",
        "#     for activation_function in activation_functions:\n",
        "#         for algo in algos:\n",
        "#           for loss in losses:\n",
        "#             print(init_method,activation_function,algo,loss)\n",
        "\n",
        "#             model = FFNN(X_train, Y_train,\n",
        "#                           epochs = 1, \n",
        "#                           hidden_layer_count = 1,\n",
        "#                           hidden_layers =  [10],\n",
        "#                           learning_rate = 0.0001,\n",
        "#                           batch_size = 32,\n",
        "#                           activation=activation_function,\n",
        "#                           weight_init=init_method,\n",
        "#                           loss = loss,\n",
        "#                           weight_decay = 0.0005)\n",
        "#             model.fit(X_train, Y_train, X_val, Y_val,algo= algo)\n",
        "#             c = c + 1\n",
        "\n",
        "#             print(c)"
      ],
      "metadata": {
        "id": "3j8_-hb8268o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}