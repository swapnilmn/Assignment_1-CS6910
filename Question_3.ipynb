{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN1W/ImPiRm3Z2y86e+aw7G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swapnilmn/Assignment_1_CS6910/blob/master/Question_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Question 3 (24 Marks) Implement the backpropagation algorithm with support for the following optimisation functions\n",
        "            sgd\n",
        "            momentum based gradient descent\n",
        "            nesterov accelerated gradient descent\n",
        "            rmsprop\n",
        "            adam\n",
        "            nadam\n",
        "\n",
        "(12 marks for the backpropagation framework and 2 marks for each of the optimisation algorithms above)\n",
        "\n",
        "We will check the code for implementation and ease of use (e.g., how easy it is to add a new optimisation algorithm such as Eve). Note that the code should be flexible enough to work with different batch sizes."
      ],
      "metadata": {
        "id": "3kVFZwNkEA7D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries "
      ],
      "metadata": {
        "id": "_dP0oEKrgmQc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CZcFz8GygBp5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "ce69954b-2bc0-4eeb-9952-3a2e0ae07008"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ],
      "source": [
        "from keras.datasets import fashion_mnist, mnist\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import copy\n",
        "from sklearn.model_selection import train_test_split \n",
        "import pandas as pd\n",
        "import subprocess\n",
        "subprocess.call(['pip', 'install', 'wandb'])\n",
        "import wandb\n",
        "wandb.login()\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Processing"
      ],
      "metadata": {
        "id": "W-FlxB4-gnqT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OneHotEncoder_from_scratch:\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.categories = None\n",
        "    def fit(self, X):\n",
        "        self.categories =[]\n",
        "        for i in range(X.shape[1]):\n",
        "            feature_categories =list(set(X[:, i]))\n",
        "            self.categories.append(feature_categories)\n",
        "            \n",
        "    def transform(self, X):\n",
        "        one_hot_vector = []\n",
        "\n",
        "        for i in range(X.shape[0]):\n",
        "            one_hot_row = []\n",
        "            for j in range(X.shape[1]):\n",
        "\n",
        "                category_index = self.categories[j].index(X[i, j])\n",
        "                category_one_hot =[0] *len(self.categories[j])\n",
        "                category_one_hot[category_index] = 1\n",
        "\n",
        "                one_hot_row.extend(category_one_hot)\n",
        "            one_hot_vector.append(one_hot_row)\n",
        "        return np.array(one_hot_vector)"
      ],
      "metadata": {
        "id": "YTDGKrnOgGwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dataset_type(dataset = 'fashion_mnist'):\n",
        "  if dataset == 'fashion_mnist':\n",
        "      (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "  elif dataset == 'mnist':\n",
        "      (train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "  else:\n",
        "      raise ValueError('Invalid dataset name')\n",
        "  X_train, X_val, Y_train, Y_val = train_test_split(train_images, train_labels, test_size=0.1, random_state=42)\n",
        "  # Flatten input images\n",
        "  train_input = X_train.reshape((X_train.shape[0], -1))\n",
        "  test_input = test_images.reshape((test_images.shape[0], -1))\n",
        "  val_input = X_val.reshape((X_val.shape[0], -1))\n",
        "\n",
        "  Y_train = np.array(Y_train)\n",
        "  Y_val = np.array(Y_val)\n",
        "  Y_test = np.array(test_labels)\n",
        "\n",
        "  X_train = np.array(train_input) / 255.0\n",
        "  X_test = np.array(test_input) / 255.0\n",
        "  X_val = np.array(val_input) / 255.0\n",
        "\n",
        "  enc = OneHotEncoder_from_scratch()\n",
        "  enc.fit(Y_train.reshape(-1, 1))\n",
        "  Y_train = enc.transform(Y_train.reshape(-1, 1))\n",
        "  Y_val = enc.transform(Y_val.reshape(-1, 1))\n",
        "  Y_test = enc.transform(Y_test.reshape(-1, 1))\n",
        "\n",
        "  return X_train, X_val, X_test, Y_train, Y_val, Y_test\n",
        "\n",
        "X_train, X_val, X_test, Y_train, Y_val, Y_test = dataset_type(dataset = 'fashion_mnist')\n",
        "# print(Y_train.shape, Y_val.shape, Y_test.shape)\n",
        "# print(X_train.shape, X_val.shape, X_test.shape)"
      ],
      "metadata": {
        "id": "wzdukjGZgZeJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00820d14-ab85-4dc0-9422-0712e9cec3f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Class of FeedForward Neural Network"
      ],
      "metadata": {
        "id": "DXX2kyFig0i5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(project = 'Question_4_Best_Model', entity = 'ed22s009')\n",
        "class FFNN:\n",
        "  def __init__(self, X, Y,\n",
        "               epochs=100, \n",
        "               hidden_layer_count=4,\n",
        "               hidden_layers=[32, 64, 128, 256],\n",
        "               learning_rate=0.001,\n",
        "               batch_size=32,\n",
        "               activation='tanh',\n",
        "               weight_init='random',\n",
        "               loss='mean_squared_error',\n",
        "               weight_decay=0):\n",
        "    \n",
        "    self.params = {\n",
        "      'inputs': X.shape[1],\n",
        "      'outputs': Y.shape[1],\n",
        "      'epochs': epochs,\n",
        "      'hidden_layers': hidden_layer_count,\n",
        "      'network_size': [X.shape[1]] + hidden_layers + [Y.shape[1]],\n",
        "      'learning_rate': learning_rate,\n",
        "      'batch_size': batch_size,\n",
        "      'weights': {},\n",
        "      'weight_init': weight_init,\n",
        "      'activation_function': activation,\n",
        "      'loss_function': loss,\n",
        "      'lambd': weight_decay\n",
        "    }\n",
        "\n",
        "    self.update(self.params)\n",
        "    np.random.seed(0)\n",
        "\n",
        "  def update(self, params):\n",
        "    for key, value in params.items():\n",
        "      setattr(self, key, value)\n",
        "\n",
        "    self.grad_derivatice={}\n",
        "    self.u_w = {f'{key}{i}': 0 for i in range(1, self.hidden_layers+1) for key in ['vw', 'vb', 'mb', 'mw']}\n",
        "    self.p_u_w = {f'{key}{i}': 0 for i in range(1, self.hidden_layers+1) for key in ['vw', 'vb', 'mb', 'mw']}\n",
        "\n",
        "    # for creating initial weights\n",
        "    if self.weight_init == 'random':\n",
        "      for i in range(1, self.hidden_layers + 2):\n",
        "          weight_shape = (self.network_size[i - 1], self.network_size[i])\n",
        "          weight_scale = 0.1\n",
        "          self.weights[f'W{i}'] = np.random.normal(scale=weight_scale, size=weight_shape)*0.1\n",
        "          # we are mulliplying ny 0.1 to get less wrights\n",
        "          bias_shape = (1, self.network_size[i])\n",
        "          self.weights[f'B{i}'] = np.zeros(bias_shape)\n",
        "\n",
        "    if self.weight_init == 'Xavier':\n",
        "      for i in range(1, self.hidden_layers + 2):\n",
        "          weight_shape = (self.network_size[i - 1], self.network_size[i])\n",
        "          weight_scale = 0.1\n",
        "          self.weights[f'W{i}'] = np.random.normal(scale=weight_scale, size=weight_shape)*np.sqrt(1/self.network_size[i-1])\n",
        "          # we are mulliplying ny 0.1 to get less wrights\n",
        "          bias_shape = (1, self.network_size[i])\n",
        "          self.weights[f'B{i}'] = np.zeros(bias_shape)\n",
        "\n",
        "\n",
        "\n",
        "  def forward_activation(self, X):\n",
        "      # Define a dictionary of activation functions and their corresponding lambda functions\n",
        "    activation_functions = {\n",
        "        'sigmoid': lambda x: 1.0 / (1.0 + np.exp(-x)), # sigmoid activation function\n",
        "        'tanh': np.tanh, # hyperbolic tangent activation function\n",
        "        'ReLU': lambda x: np.maximum(0, x) # rectified linear unit (ReLU) activation function\n",
        "    }\n",
        "\n",
        "    # Get the activation function based on the value of `self.activation_function`\n",
        "    activation_function = activation_functions.get(self.activation_function)\n",
        "\n",
        "    # If the activation function is found, apply it to the input matrix `X`\n",
        "    if activation_function:\n",
        "        return activation_function(X)\n",
        "\n",
        "    # If the activation function is not found, raise a ValueError indicating that it is unknown\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown activation function '{self.activation_function}'\")\n",
        "\n",
        "\n",
        "\n",
        "  def grad_activation(self, X):\n",
        "    # Define a dictionary of activation function derivatives and their corresponding lambda functions\n",
        "    activation_gradients = {\n",
        "        'sigmoid': lambda x: x * (1 - x), # derivative of the sigmoid activation function\n",
        "        'tanh': lambda x: 1 - np.square(x), # derivative of the hyperbolic tangent activation function\n",
        "        'ReLU': lambda x: 1.0 * (x > 0) # derivative of the rectified linear unit (ReLU) activation function\n",
        "    }\n",
        "\n",
        "    # Get the derivative of the activation function based on the value of `self.activation_function`\n",
        "    gradient_function = activation_gradients.get(self.activation_function)\n",
        "\n",
        "    # If the derivative function is found, apply it to the input matrix `X`\n",
        "    if gradient_function:\n",
        "        return gradient_function(X)\n",
        "\n",
        "    # If the derivative function is not found, raise a ValueError indicating that it is unknown\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown activation function '{self.activation_function}'\")\n",
        "\n",
        "\n",
        "  def softmax(self, X):\n",
        "    exps =np.exp(X - np.max(X, axis=1, keepdims=True))\n",
        "    return  exps /np.sum(exps, axis=1, keepdims=True)\n",
        "\n",
        "\n",
        "  def forward_pass(self, X, weights=None):\n",
        "    # X: shape (batch_size, input_dim)\n",
        "    if weights is None:\n",
        "        weights = self.weights\n",
        "\n",
        "    # Initialize dictionaries to store intermediate outputs\n",
        "    self.z = {}\n",
        "    self.h = {}\n",
        "    self.h[0] = X\n",
        "\n",
        "    # Perform forward pass through hidden layers\n",
        "    for i in range(self.hidden_layers):\n",
        "        # Compute weighted sum of inputs and biases\n",
        "        z_i = self.h[i] @ weights[f'W{i+1}']\n",
        "        z_i = z_i + weights[f'B{i+1}']\n",
        "        self.z[i+1] = z_i\n",
        "\n",
        "        # Apply activation function\n",
        "        h_i = self.forward_activation(z_i)\n",
        "        self.h[i+1] = h_i\n",
        "\n",
        "    # Compute final output\n",
        "    z_final = self.h[self.hidden_layers] @ weights[f'W{self.hidden_layers+1}']\n",
        "    z_final = z_final + weights[f'B{self.hidden_layers+1}']\n",
        "    self.z[self.hidden_layers+1] = z_final\n",
        "\n",
        "    # Apply softmax activation function to final output\n",
        "    h_final = self.softmax(z_final)\n",
        "    self.h[self.hidden_layers+1] = h_final\n",
        "\n",
        "    # Return final output\n",
        "    return h_final\n",
        "\n",
        "  def backprop(self, X, Y, weights=None):\n",
        "    #X(batch_size, input_size)\n",
        "    #Y(batch_size, output_size)\n",
        "    if weights is None:\n",
        "        weights = self.weights\n",
        "\n",
        "    # Perform forward pass\n",
        "    self.forward_pass(X, weights)\n",
        "\n",
        "    # Initialize dictionary to store gradients\n",
        "    self.grad_derivatice = {}\n",
        "\n",
        "    # Calculate total number of layers (including input and output)\n",
        "    total_layers = self.hidden_layers + 1\n",
        "\n",
        "    # Calculate gradients for output layer based on the selected loss function\n",
        "    if self.loss_function == 'cross_entropy':\n",
        "        self.grad_derivatice[f'dA{total_layers}'] =  (self.h[total_layers] - Y)\n",
        "    elif self.loss_function == 'mean_squared_error':\n",
        "        self.grad_derivatice[f'dA{total_layers}'] = (1/X.shape[0]) * 2 * (self.h[total_layers] - Y)\n",
        "\n",
        "    # Loop through hidden layers in reverse order and calculate gradients\n",
        "    for k in range(total_layers, 0, -1):\n",
        "        # Define keys for weights, biases, and gradients\n",
        "        w_key, b_key, dw_key, db_key, da_key = [f'{key}{k}' for key in ['W', 'B', 'dW', 'dB', 'dA']]\n",
        "\n",
        "        # Calculate gradients for weights\n",
        "        dw = np.matmul(self.h[k-1].T, self.grad_derivatice[da_key])\n",
        "        dw_reg = dw + self.lambd * weights[w_key]\n",
        "        self.grad_derivatice[dw_key] = dw_reg\n",
        "\n",
        "        # Calculate gradients for biases\n",
        "        db = np.sum(self.grad_derivatice[da_key], axis=0).reshape(1, -1)\n",
        "        db_reg = db + self.lambd * weights[b_key]\n",
        "        self.grad_derivatice[db_key] = db_reg\n",
        "\n",
        "        # Calculate gradient for previous layer's output\n",
        "        dH = np.matmul(self.grad_derivatice[da_key], weights[w_key].T)\n",
        "        self.grad_derivatice[f'dH{k-1}'] = dH\n",
        "\n",
        "        # Calculate gradient for previous layer's activation\n",
        "        dA = np.multiply(dH, self.grad_activation(self.h[k-1]))\n",
        "        self.grad_derivatice[f'dA{k-1}'] = dA\n",
        "\n",
        "        # Calculate gradient for regularization term\n",
        "        reg_grad = self.lambd * weights[w_key]\n",
        "        self.grad_derivatice[f'dW_reg{k}'] = reg_grad\n",
        "\n",
        "    # Add regularization gradient to gradients for weights\n",
        "    for k in range(1, total_layers+1):\n",
        "        dw_key, dw_reg_key = [f'{key}{k}' for key in ['dW', 'dW_reg']]\n",
        "        self.grad_derivatice[dw_key] += self.grad_derivatice[dw_reg_key]\n",
        "\n",
        "    # Return gradient for previous layer's output\n",
        "    return self.grad_derivatice[f'dH{k-1}']\n",
        "\n",
        "\n",
        "  def fit(self, X, Y, X_val, Y_val,algo= 'GD',a = 10, eps=1e-8, beta=0.9, beta1=0.9, beta2=0.9, gamma=0.9, show_loss = False):\n",
        "    #X(batch_size, input_size)\n",
        "    #Y(batch_size, output_size)\n",
        "    if show_loss:\n",
        "      los = []\n",
        "      accuracy = []\n",
        "    for num_epoch in tqdm(range(1, self.epochs+1), unit='epoch'):\n",
        "      m = X.shape[0]\n",
        "      \n",
        "      if algo == 'sgd':\n",
        "          # Loop over the number of samples\n",
        "          for i in range(m):\n",
        "              # Select a random sample\n",
        "              rand_idx = np.random.randint(m)\n",
        "              x_i = X[rand_idx:rand_idx+1]\n",
        "              y_i = Y[rand_idx:rand_idx+1]\n",
        "\n",
        "              # Perform backpropagation on the sample\n",
        "              self.backprop(x_i, y_i)\n",
        "\n",
        "              # Loop over the hidden layers\n",
        "              for j in range(1, self.hidden_layers+1):\n",
        "                  # Get the keys for the weights, biases, and gradients\n",
        "                  w_key, b_key, dw_key, db_key = [f'{key}{j}' for key in ['W', 'B', 'dW', 'dB']]\n",
        "\n",
        "                  # Update the weights and biases using the gradients\n",
        "                  self.weights[w_key] -= self.learning_rate * self.grad_derivatice[dw_key]\n",
        "                  self.weights[b_key] -= self.learning_rate * self.grad_derivatice[db_key]\n",
        "\n",
        "          # Log the results for this epoch\n",
        "          self.wandlog(num_epoch, X, Y, X_val, Y_val)\n",
        "\n",
        "      \n",
        "      elif algo == 'nag':\n",
        "          num_examples = X.shape[0]\n",
        "          num_batches = num_examples // self.batch_size\n",
        "          gamma = 0.9  # Nesterov momentum parameter\n",
        "          for batch in range(num_batches + 1):\n",
        "              start_index = batch * self.batch_size\n",
        "              end_index = min((batch + 1) * self.batch_size, num_examples)\n",
        "              X_batch, Y_batch = X[start_index:end_index], Y[start_index:end_index]\n",
        "\n",
        "              self.backprop(X_batch, Y_batch)\n",
        "\n",
        "              for i in range(1, self.hidden_layers + 1):\n",
        "                  w_key, b_key, vw_key, vb_key, dw_key, db_key = [f'{key}{i}' for key in ['W', 'B', 'vw', 'vb', 'dW', 'dB']]\n",
        "                  # w_key(prev_layer_output_dim,layer_output_dim)\n",
        "                  # b_key1,layer_output_dim)\n",
        "                  # dw_key(prev_layer_output_dim,layer_output_dim)\n",
        "                  # db_key(1,layer_output_dim)\n",
        "\n",
        "                  # Compute gradients with Nesterov acceleration\n",
        "                  self.u_w[vw_key] = gamma * self.u_w[vw_key] + self.learning_rate * self.grad_derivatice[dw_key]\n",
        "                  self.u_w[vb_key] = gamma * self.u_w[vb_key] + self.learning_rate * self.grad_derivatice[db_key]\n",
        "                  grad_dw = self.grad_derivatice[dw_key] + gamma * self.u_w[vw_key]\n",
        "                  grad_db = self.grad_derivatice[db_key] + gamma * self.u_w[vb_key]\n",
        "\n",
        "                  # Update weights and biases\n",
        "                  self.weights[w_key] -= self.learning_rate * grad_dw\n",
        "                  self.weights[b_key] -= self.learning_rate * grad_db\n",
        "\n",
        "              # Log the loss and accuracy after every N steps\n",
        "          self.wandlog(num_epoch, X, Y, X_val, Y_val)\n",
        "\n",
        "\n",
        "      \n",
        "      elif algo == 'rmsprop':\n",
        "          num_examples = X.shape[0]\n",
        "          num_batches = num_examples // self.batch_size\n",
        "\n",
        "          for batch in range(num_batches + 1):\n",
        "              start_index = batch * self.batch_size\n",
        "              end_index = min((batch+1)*self.batch_size, num_examples)\n",
        "              X_batch, Y_batch = X[start_index:end_index], Y[start_index:end_index]\n",
        "\n",
        "              self.backprop(X_batch, Y_batch)\n",
        "\n",
        "              for i in range(1, self.hidden_layers+1):\n",
        "                  w_key, b_key, vw_key, vb_key, dw_key, db_key = [f'{key}{i}' for key in ['W', 'B', 'vw', 'vb', 'dW', 'dB']]\n",
        "                  prev_layer_output_dim = self.weights[f'{w_key}'].shape[0]\n",
        "                  layer_output_dim = self.weights[f'{w_key}'].shape[1]\n",
        "\n",
        "                  # Compute gradients for this layer\n",
        "                  dW, dB = self.grad_derivatice[dw_key], self.grad_derivatice[db_key]\n",
        "                  dw_squared = np.square(dW)\n",
        "                  db_squared = np.square(dB)\n",
        "\n",
        "                  # Update the exponentially weighted averages of the squared gradients\n",
        "                  self.u_w[vw_key] = beta * self.u_w[vw_key] + (1 - beta) * dw_squared\n",
        "                  self.u_w[vb_key] = beta * self.u_w[vb_key] + (1 - beta) * db_squared\n",
        "\n",
        "                  # Compute the RMSProp update for the weights and biases\n",
        "                  dw_rms = np.sqrt(self.u_w[vw_key] + 1e-8)\n",
        "                  db_rms = np.sqrt(self.u_w[vb_key] + 1e-8)\n",
        "\n",
        "                  dw_update = (self.learning_rate / dw_rms) * dW\n",
        "                  db_update = (self.learning_rate / db_rms) * dB\n",
        "\n",
        "                  # Update the weights and biases\n",
        "                  self.weights[w_key] -= dw_update\n",
        "                  self.weights[b_key] -= db_update\n",
        "\n",
        "          self.wandlog(num_epoch, X, Y, X_val, Y_val)\n",
        "      \n",
        "      elif algo == 'adam':\n",
        "          num_examples = X.shape[0]\n",
        "          num_batches = num_examples //self.batch_size\n",
        "\n",
        "          for batch in range(num_batches + 1):\n",
        "              start_index = batch *self.batch_size\n",
        "              end_index = min((batch+1)*self.batch_size, num_examples)\n",
        "              X_batch, Y_batch = X[start_index:end_index], Y[start_index:end_index]\n",
        "\n",
        "              self.backprop(X_batch, Y_batch)\n",
        "\n",
        "              for i in range(1, self.hidden_layers + 1):\n",
        "                  w_key, b_key, vw_key, vb_key, mw_key, mb_key = [f'{key}{i}' for key in ['W', 'B', 'vw', 'vb', 'mw', 'mb']]\n",
        "                  dw_key, db_key= [f'{key}{i}' for key in ['dW', 'dB']]\n",
        "                  # Compute gradients for the current layer\n",
        "                  dW = self.grad_derivatice[dw_key]\n",
        "                  dB = self.grad_derivatice[db_key]\n",
        "\n",
        "                  # Update moving averages for the current layer's weights\n",
        "                  self.u_w[mw_key] = beta1 * self.u_w[mw_key] + (1 - beta1) * dW\n",
        "                  self.u_w[vw_key] = beta2 * self.u_w[vw_key] + (1 - beta2) * (dW ** 2)\n",
        "\n",
        "                  # Compute bias-corrected moving averages for the current layer's weights\n",
        "                  mw_hat = self.u_w[mw_key] / (1 - np.power(beta1, batch + 1))\n",
        "                  vw_hat = self.u_w[vw_key] / (1 - np.power(beta2, batch + 1))\n",
        "\n",
        "                  # Update the current layer's weights using the bias-corrected moving averages\n",
        "                  self.weights[w_key] -= (self.learning_rate / np.sqrt(vw_hat + eps)) * mw_hat\n",
        "\n",
        "                  # Update moving averages for the current layer's biases\n",
        "                  self.u_w[mb_key] = beta1 * self.u_w[mb_key] + (1 - beta1) * dB\n",
        "                  self.u_w[vb_key] = beta2 * self.u_w[vb_key] + (1 - beta2) * (dB ** 2)\n",
        "\n",
        "                  # Compute bias-corrected moving averages for the current layer's biases\n",
        "                  mb_hat = self.u_w[mb_key] / (1 - np.power(beta1, batch + 1))\n",
        "                  vb_hat = self.u_w[vb_key] / (1 - np.power(beta2, batch + 1))\n",
        "\n",
        "                  # Update the current layer's biases using the bias-corrected moving averages\n",
        "                  self.weights[b_key] -= (self.learning_rate / np.sqrt(vb_hat + eps)) * mb_hat\n",
        "\n",
        "          # Log epoch information\n",
        "          self.wandlog(num_epoch, X, Y, X_val, Y_val)\n",
        "\n",
        "          \n",
        "      elif algo == 'nag':\n",
        "        # Calculate number of examples and batches\n",
        "        num_examples = X.shape[0]\n",
        "        num_batches = num_examples //self.batch_size\n",
        "\n",
        "        # Initialize temporary weight variables for each layer\n",
        "        temp_weights = {}\n",
        "        for i in range(1, self.hidden_layers+2):\n",
        "          w_key, b_key = [f'{key}{i}' for key in ['W', 'B']]\n",
        "          temp_weights[w_key] = np.zeros_like(self.weights[w_key])\n",
        "          temp_weights[b_key] = np.zeros_like(self.weights[b_key])\n",
        "\n",
        "        # Iterate over each batch of data\n",
        "        for batch in range(num_batches + 1):\n",
        "            # Set start and end indices for the current batch\n",
        "            start_index = batch *self.batch_size\n",
        "            end_index = min((batch+1)*self.batch_size, num_examples)\n",
        "            # Select batch of data\n",
        "            X_batch, Y_batch = X[start_index:end_index], Y[start_index:end_index]\n",
        "\n",
        "            # Update weights using Nesterov accelerated gradient descent\n",
        "            for i in range(1,self.hidden_layers+1):\n",
        "                # Set variable names for weights, biases, velocity for current layer\n",
        "                w_key, b_key, vw_key, vb_key,dw_key, db_key = [f'{key}{i}' for key in ['W', 'B', 'vw', 'vb', 'dW', 'dB']]\n",
        "                # Update velocity using Nesterov accelerated gradient descent\n",
        "                self.u_w[vw_key]=gamma*self.p_u_w[vw_key]\n",
        "                self.u_w[vb_key]=gamma*self.p_u_w[vb_key]\n",
        "                temp_weights[w_key]=self.weights[w_key]-self.u_w[vw_key]\n",
        "                temp_weights[b_key]=self.weights[b_key]-self.u_w[vb_key]\n",
        "            \n",
        "            # Perform backpropagation to calculate gradients\n",
        "            self.backprop(X_batch,Y_batch,temp_weights)\n",
        "\n",
        "            # Update weights for each layer using Nesterov accelerated gradient descent\n",
        "            for i in range(1,self.hidden_layers+1):\n",
        "                # Set variable names for weights, biases, velocity for current layer\n",
        "                w_key, b_key, vw_key, vb_key,dw_key, db_key = [f'{key}{i}' for key in ['W', 'B', 'vw', 'vb', 'dW', 'dB']]\n",
        "                # Update velocity using Nesterov accelerated gradient descent\n",
        "                self.u_w[vw_key] = gamma *self.u_w[vw_key] + self.learning_rate * (self.grad_derivatice[dw_key])\n",
        "                self.u_w[vb_key] = gamma *self.u_w[vb_key] + self.learning_rate * (self.grad_derivatice[db_key])\n",
        "                # Update weights using velocity and learning rate\n",
        "                self.weights[w_key] -= self.learning_rate * (self.u_w[vw_key]/m)\n",
        "                self.weights[b_key] -= self.learning_rate * (self.u_w[vb_key]/m) \n",
        "\n",
        "            # Set the previous velocity to the current velocity\n",
        "            self.p_u_w=self.u_w\n",
        "\n",
        "        # Call wandlog method to log training and validation loss and accuracy\n",
        "        self.wandlog(num_epoch, X, Y,X_val, Y_val)\n",
        "\n",
        "\n",
        "\n",
        "      elif algo == 'nadam':\n",
        "\n",
        "        num_examples = X.shape[0]\n",
        "        num_batches = num_examples // self.batch_size\n",
        "\n",
        "        num_updates = 0\n",
        "\n",
        "        # Loop through each layer in the network\n",
        "        for i in range(1, self.hidden_layers + 1):\n",
        "            # Get the keys for the weights, biases, velocity, momentum, and infinite norm buffers for this layer\n",
        "            w_key, b_key, vw_key, vb_key, mw_key, mb_key = [f'{key}{i}' for key in ['W', 'B', 'vw', 'vb', 'mw', 'mb']]\n",
        "            # The shape of the weight matrix for this layer (prev_layer_output_dim, layer_output_dim)\n",
        "            # The shape of the bias matrix for this layer (1, layer_output_dim)\n",
        "            # The shape of the gradient of the weight matrix for this layer (prev_layer_output_dim, layer_output_dim)\n",
        "            # The shape of the gradient of the bias matrix for this layer (1, layer_output_dim)\n",
        "            dw_key, db_key, mw_i_key, mb_i_key = [f'{key}{i}' for key in ['dW', 'dB', 'mw_inf', 'mb_inf']]\n",
        "\n",
        "            # Loop through each batch of examples\n",
        "            for batch in range(num_batches + 1):\n",
        "                start_index = batch * self.batch_size\n",
        "                end_index = min((batch + 1) * self.batch_size, num_examples)\n",
        "                X_batch, Y_batch = X[start_index:end_index], Y[start_index:end_index]\n",
        "\n",
        "                # Perform backpropagation on the current batch\n",
        "                self.backprop(X_batch, Y_batch)\n",
        "\n",
        "                num_updates += 1\n",
        "\n",
        "                # Initialize the velocity, momentum, and infinite norm buffers for this layer if they haven't been already\n",
        "                self.u_w.setdefault(mw_i_key, 0)  # shape: (prev_layer_output_dim, layer_output_dim)\n",
        "                # Update the velocity buffer for the weights\n",
        "                self.u_w[mw_key] = beta1 * self.u_w[mw_key] + (1 - beta1) * (self.grad_derivatice[dw_key])\n",
        "                # Update the velocity buffer for the squared gradients of the weights\n",
        "                self.u_w[vw_key] = beta2 * self.u_w[vw_key] + (1 - beta2) * ((self.grad_derivatice[dw_key]) ** 2)\n",
        "                # Compute the bias-corrected velocity and squared gradient velocity for the weights\n",
        "                mw_hat = self.u_w[mw_key] / (1 - np.power(beta1, num_updates))\n",
        "                vw_hat = self.u_w[vw_key] / (1 - np.power(beta2, num_updates))\n",
        "                # Compute the infinite norm buffer for the weights\n",
        "                mw_inf = beta1 * self.u_w[mw_i_key] + (1 - beta1) * np.abs(self.grad_derivatice[dw_key])\n",
        "                # Compute the bias-corrected infinite norm buffer for the weights\n",
        "                mw_inf_hat = mw_inf / (1 - np.power(beta1, num_updates))\n",
        "                # Update the weights using the NAdam update rule\n",
        "                self.weights[w_key] -= (self.learning_rate / np.sqrt(vw_hat + eps)) * ((beta1 * mw_hat) + ((1 - beta1) * self.grad_derivatice[dw_key])) / (1 - np.power(beta2, num_updates)) + self.learning_rate * eps * np.sqrt(1 - np.power(beta2, num_updates)) * mw_inf_hat\n",
        "\n",
        "\n",
        "                # Set default value of mb_i_key to 0 in u_w dictionary\n",
        "                self.u_w.setdefault(mb_i_key, 0)\n",
        "\n",
        "                # Update u_w dictionary for mb_key and vb_key using Adam optimizer\n",
        "                self.u_w[mb_key] = beta1 * self.u_w[mb_key] + (1 - beta1) * (self.grad_derivatice[db_key])\n",
        "                self.u_w[vb_key] = beta2 * self.u_w[vb_key] + (1 - beta2) * ((self.grad_derivatice[db_key]) ** 2)\n",
        "\n",
        "                # Compute bias-corrected estimates for mb_key and vb_key\n",
        "                mb_hat = self.u_w[mb_key] / (1 - np.power(beta1, num_updates))\n",
        "                vb_hat = self.u_w[vb_key] / (1 - np.power(beta2, num_updates))\n",
        "\n",
        "                # Compute exponentially weighted infinity norm of gradient for mb_i_key\n",
        "                mb_inf = beta1 * self.u_w[mb_i_key] + (1 - beta1) * np.abs(self.grad_derivatice[db_key])\n",
        "\n",
        "                # Compute bias-corrected estimate for mb_inf\n",
        "                mb_inf_hat = mb_inf / (1 - np.power(beta1, num_updates))\n",
        "\n",
        "                # Update weights using Adam optimizer and the computed estimates\n",
        "                self.weights[b_key] -= (self.learning_rate / np.sqrt(vb_hat + eps)) * ((beta1 * mb_hat) + ((1 - beta1) * self.grad_derivatice[db_key])) / (1 - np.power(beta2, num_updates)) + self.learning_rate * eps * np.sqrt(1 - np.power(beta2, num_updates)) * mb_inf\n",
        "\n",
        "        # Log and track the training and validation loss\n",
        "        self.wandlog(num_epoch, X, Y, X_val, Y_val)\n",
        "\n",
        "            \n",
        "      if show_loss:\n",
        "          # Calculate the performance on the validation set and append to the lists\n",
        "        loss, acc = self.performance(X_val, Y_val)\n",
        "        los.append(loss)\n",
        "        accuracy.append(acc)\n",
        "\n",
        "\n",
        "\n",
        "    if show_loss:\n",
        "        # Find the index of maximum validation accuracy achieved during training\n",
        "        max_acc_index = np.argmax(accuracy)\n",
        "        \n",
        "        # Plot the loss and accuracy curves, with markers indicating the position of the maximum validation accuracy\n",
        "        plt.plot(los, label='Loss')\n",
        "        plt.plot(accuracy, label='Accuracy')\n",
        "        plt.plot(max_acc_index, los[max_acc_index], marker='o', color='red')\n",
        "        plt.plot(max_acc_index, accuracy[max_acc_index], marker='o', color='green')\n",
        "\n",
        "        # Annotate the plot with the loss value and accuracy value achieved at the maximum validation accuracy point\n",
        "        plt.text(max_acc_index, los[max_acc_index], f'loss @ Max val acc: ( {los[max_acc_index]:.4f})',  va='top')\n",
        "        plt.text(max_acc_index, accuracy[max_acc_index], f'Max Val acc: ({accuracy[max_acc_index]:.4f})', va='bottom')\n",
        "\n",
        "        # Set the y-axis limits for the plot\n",
        "        plt.ylim([min(los + accuracy)-0.1, max(los + accuracy) + 0.1])\n",
        "\n",
        "        # Add a title and axis labels to the plot, and display it\n",
        "        plt.title('Val Loss and val Accuracy with {}'.format(self.loss_function))\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Loss / Accuracy')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "\n",
        "  def predict(self, X):\n",
        "      '''\n",
        "      Given input data X, predict the output using the trained neural network model.\n",
        "\n",
        "      Args:\n",
        "      - X: Input data of shape (num_samples, input_size)\n",
        "\n",
        "      Returns:\n",
        "      - Y_pred: Predicted output of shape (num_samples, output_size)\n",
        "      '''\n",
        "      Y_pred = (self.forward_pass(X))\n",
        "      return np.array(Y_pred).squeeze()\n",
        "\n",
        "\n",
        "  def accuracy_score(self, X, Y):\n",
        "      '''\n",
        "      Calculate the accuracy score of the trained neural network model on the given input data and output labels.\n",
        "\n",
        "      Args:\n",
        "      - X: Input data of shape (num_samples, input_size)\n",
        "      - Y: Output labels of shape (num_samples, output_size)\n",
        "\n",
        "      Returns:\n",
        "      - accuracy: Accuracy score of the model\n",
        "      '''\n",
        "      Y_true = np.argmax(Y, axis=1).reshape(-1, 1)\n",
        "      pred_labels = np.argmax(self.predict(X), axis=1).reshape(-1,1)\n",
        "      return np.sum(pred_labels == Y_true) / len(Y)\n",
        "\n",
        "\n",
        "  def Loss(self, X, Y):\n",
        "      '''\n",
        "      Calculate the loss of the trained neural network model on the given input data and output labels.\n",
        "\n",
        "      Args:\n",
        "      - X: Input data of shape (num_samples, input_size)\n",
        "      - Y: Output labels of shape (num_samples, output_size)\n",
        "\n",
        "      Returns:\n",
        "      - loss: Loss of the model\n",
        "      '''\n",
        "      Y_pred = self.predict(X)\n",
        "      if self.loss_function == 'cross_entropy':\n",
        "          loss = -np.mean(Y * np.log(Y_pred + 1e-8))\n",
        "      elif self.loss_function == 'mean_squared_error':\n",
        "          loss = np.mean((Y - Y_pred)**2)/2\n",
        "      else:\n",
        "          raise ValueError('Invalid loss function')\n",
        "      return loss\n",
        "\n",
        "\n",
        "  def performance(self, X_test, Y_test):\n",
        "      '''\n",
        "      Calculate the performance of the trained neural network model on the given test data and output labels.\n",
        "\n",
        "      Args:\n",
        "      - X_test: Test data of shape (num_samples, input_size)\n",
        "      - Y_test: Output labels of shape (num_samples, output_size)\n",
        "\n",
        "      Returns:\n",
        "      - loss: Loss of the model on the test data\n",
        "      - accuracy: Accuracy score of the model on the test data\n",
        "      '''\n",
        "      loss = self.Loss(X_test, Y_test)\n",
        "      accuracy = self.accuracy_score(X_test, Y_test)\n",
        "      return loss, accuracy\n",
        "\n",
        "\n",
        "\n",
        "  def confusion_matrix(self, X, Y):\n",
        "      \n",
        "    # Get the actual labels by taking the index of the maximum value in Y\n",
        "    a = np.argmax(Y, axis=1)\n",
        "    \n",
        "    # Get the predicted labels by taking the index of the maximum value in the output of forward_pass method\n",
        "    p = np.argmax(self.forward_pass(X), axis=1)\n",
        "    \n",
        "    # Get all the available classes by combining actual and predicted labels and then finding unique classes\n",
        "    ap = np.concatenate((a, p))\n",
        "    c = np.unique(ap)\n",
        "    \n",
        "    # Create an empty confusion matrix with dimensions equal to number of available classes\n",
        "    cm = np.zeros((len(c), len(c)), dtype=int)\n",
        "    \n",
        "    # Fill the confusion matrix with the counts of actual and predicted labels for each class\n",
        "    for i, actual in enumerate(c):\n",
        "        for j, predicted in enumerate(c):\n",
        "            count = np.where((a == actual) & (p == predicted))[0].shape[0]\n",
        "            cm[i,j] = count\n",
        "    \n",
        "    # Log the confusion matrix as an interactive plot to Weights and Biases\n",
        "    wandb.log({'confusion_matrix': wandb.plot.confusion_matrix(\n",
        "        probs=None,\n",
        "        y_true=a,\n",
        "        preds=p,\n",
        "        class_names=list(c),\n",
        "        title='Confusion Matrix'\n",
        "    )})\n",
        "    \n",
        "    # Return the confusion matrix\n",
        "    return cm\n",
        "\n",
        "  def confo_matrixplot(self, confusion_matrix, title='Confusion Matrix', cmap='PuBu'):\n",
        "    # Convert confusion matrix to a numpy array\n",
        "    confusion_matrix = np.array(confusion_matrix)\n",
        "    # Normalize the confusion matrix by dividing each row by its sum\n",
        "    confusion_matrix = confusion_matrix / np.sum(confusion_matrix, axis=1, keepdims=True)\n",
        "    # Plot the normalized confusion matrix as a heatmap using the specified color map\n",
        "    plt.matshow(confusion_matrix, cmap=cmap)\n",
        "    # Add a color bar indicating the scale of the values in the heatmap\n",
        "    plt.colorbar()\n",
        "    # Create tick marks for each label in the confusion matrix\n",
        "    tick_marks = np.arange(len(confusion_matrix))\n",
        "    # Set the x and y tick labels to the corresponding labels in the confusion matrix\n",
        "    plt.xticks(tick_marks)\n",
        "    plt.yticks(tick_marks)\n",
        "    # Add labels for the x and y axes\n",
        "    plt.ylabel('Actual')\n",
        "    plt.xlabel('Predicted')\n",
        "    # Add a title for the plot\n",
        "    plt.title(title)\n",
        "    # Display the plot\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "  def wandlog(self, num_epoch, X, Y, X_val, Y_val):\n",
        "      # Calculate accuracy and loss on training data\n",
        "      accuracy = self.accuracy_score(X, Y)\n",
        "      loss_train = self.Loss(X, Y)\n",
        "      \n",
        "      # Calculate accuracy and loss on validation data\n",
        "      loss_valid = self.Loss(X_val, Y_val)\n",
        "      val_accuracy = self.accuracy_score(X_val, Y_val)\n",
        "      \n",
        "      # Log the metrics using wandb\n",
        "      wandb.log({\n",
        "          'epoch': num_epoch,           \n",
        "          'loss': loss_train,\n",
        "          'accuracy': accuracy,\n",
        "          'val_loss': loss_valid,\n",
        "          'val_accuracy': val_accuracy\n",
        "      })\n",
        "      \n",
        "      # Print the metrics every 1 epoch\n",
        "      if num_epoch % 1 == 0:\n",
        "          # Calculate accuracy and loss on training data\n",
        "          accuracy = self.accuracy_score(X, Y)\n",
        "          loss_train = self.Loss(X, Y)\n",
        "          \n",
        "          # Calculate accuracy and loss on validation data\n",
        "          loss_valid = self.Loss(X_val, Y_val)\n",
        "          val_accuracy = self.accuracy_score(X_val, Y_val)\n",
        "          \n",
        "          # Store the metrics in a dictionary\n",
        "          library = {\n",
        "              'epoch': num_epoch,           \n",
        "              'loss': loss_train,\n",
        "              'accuracy': accuracy,\n",
        "              'val_loss': loss_valid,\n",
        "              'val_accuracy': val_accuracy\n",
        "          }\n",
        "\n",
        "          # Print the metrics\n",
        "          print('Epoch: {}, Train Loss: {}, Train Accuracy: {}, Val Loss: {}, Val Accuracy: {}'.format(\n",
        "              library['epoch'], library['loss'], library['accuracy'], library['val_loss'], library['val_accuracy']))\n",
        "          \n",
        "          # Check if the last epoch has been reached and print a success message\n",
        "          if num_epoch == self.epochs:\n",
        "              print('Model trained successfully !')\n",
        "# model = FFNN(X_train, Y_train,\n",
        "#                   epochs = 18, \n",
        "#                   hidden_layer_count = 3,\n",
        "#                   hidden_layers =  [256, 256, 256],\n",
        "#                   learning_rate = 0.001,\n",
        "#                   batch_size = 128,\n",
        "#                   activation='tanh',\n",
        "#                   weight_init='Xavier',\n",
        "#                   loss = 'cross_entropy',\n",
        "#                   weight_decay = 0)\n",
        "# model.fit(X_train, Y_train, X_val, Y_val,algo= 'nadam', a = 1, show_loss = True) \n",
        "# confusion_matrix = model.confusion_matrix(X_test, Y_test)\n",
        "# model.confo_matrixplot(confusion_matrix)"
      ],
      "metadata": {
        "id": "roqXcqcDgrri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rough Work"
      ],
      "metadata": {
        "id": "aMZRi3CQAFql"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# init_methods = ['random', 'Xavier']\n",
        "# activation_functions = ['sigmoid', 'tanh', 'ReLU']\n",
        "# algos = ['sgd', 'momentum', 'nag', 'rmsprop', 'adam','nadam']\n",
        "# losses = ['cross_entropy', 'mean_squared_error']\n",
        "# c = 0\n",
        "# d = 0\n",
        "# for init_method in init_methods:\n",
        "#     for activation_function in activation_functions:\n",
        "#         for algo in algos:\n",
        "#           for loss in losses:\n",
        "#             print(init_method,activation_function,algo,loss)\n",
        "\n",
        "#             model = FFNN(X_train, Y_train,\n",
        "#                           epochs = 1, \n",
        "#                           hidden_layer_count = 1,\n",
        "#                           hidden_layers =  [10],\n",
        "#                           learning_rate = 0.0001,\n",
        "#                           batch_size = 32,\n",
        "#                           activation=activation_function,\n",
        "#                           weight_init=init_method,\n",
        "#                           loss = loss,\n",
        "#                           weight_decay = 0.0005)\n",
        "#             model.fit(X_train, Y_train, X_val, Y_val,algo= algo)\n",
        "#             c = c + 1\n",
        "\n",
        "#             print(c)"
      ],
      "metadata": {
        "id": "3j8_-hb8268o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}