{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPFeIH9OBa45Ff8z1B7q4H9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swapnilmn/Assignment_1-CS6910/blob/master/Question_2ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Implement a feedforward neural network which takes images from the fashion-mnist data as input and outputs a probability distribution over the 10 classes.\n",
        "\n",
        "Your code should be flexible such that it is easy to change the number of hidden layers and the number of neurons in each hidden layer."
      ],
      "metadata": {
        "id": "v0Q5opYMgYb-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries "
      ],
      "metadata": {
        "id": "_dP0oEKrgmQc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CZcFz8GygBp5"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import fashion_mnist, mnist\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split \n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class OneHotEncoder_from_scratch:\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.categories = None\n",
        "    def fit(self, X):\n",
        "        self.categories =[]\n",
        "        for i in range(X.shape[1]):\n",
        "            feature_categories =list(set(X[:, i]))\n",
        "            self.categories.append(feature_categories)\n",
        "            \n",
        "    def transform(self, X):\n",
        "        one_hot_vector = []\n",
        "\n",
        "        for i in range(X.shape[0]):\n",
        "            one_hot_row = []\n",
        "            for j in range(X.shape[1]):\n",
        "\n",
        "                category_index = self.categories[j].index(X[i, j])\n",
        "                category_one_hot =[0] *len(self.categories[j])\n",
        "                category_one_hot[category_index] = 1\n",
        "\n",
        "                one_hot_row.extend(category_one_hot)\n",
        "            one_hot_vector.append(one_hot_row)\n",
        "        return np.array(one_hot_vector)\n",
        "\n",
        "X = np.array([[1],[2]]) \n",
        "enc = OneHotEncoder_from_scratch()\n",
        "enc.fit(X)\n",
        "enc.transform(X)\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "enc.fit(train_labels.reshape(-1, 1))\n",
        "enc.transform(train_labels.reshape(-1, 1))\n",
        "#train_labels.reshape(-1, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjPsVG8wafVH",
        "outputId": "69787bd2-f9a2-4e45-e63f-a4d8c9b1d70b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 2s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, ..., 0, 0, 1],\n",
              "       [1, 0, 0, ..., 0, 0, 0],\n",
              "       [1, 0, 0, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [1, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = 'fashion_mnist'\n",
        "if dataset == 'fashion_mnist':\n",
        "    (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "elif dataset == 'mnist':\n",
        "    (train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "else:\n",
        "    raise ValueError('Invalid dataset name')\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(train_images, train_labels, test_size=0.1, random_state=42)\n",
        "train_input = []\n",
        "for i in range(len(X_train)):\n",
        "    train_input.append(list(np.concatenate(X_train[i]).flat))\n",
        "\n",
        "val_input = []\n",
        "for i in range(len(X_val)):\n",
        "    val_input.append(list(np.concatenate(X_val[i]).flat))\n",
        "\n",
        "test_input = []\n",
        "for i in range(len(test_images)):\n",
        "    test_input.append(list(np.concatenate(test_images[i]).flat))\n",
        "Y_train = np.array(Y_train)\n",
        "Y_val = np.array(Y_val)\n",
        "Y_test = np.array(test_labels)\n",
        "\n",
        "X_train = np.array(train_input) / 255.0\n",
        "X_test = np.array(test_input) / 255.0\n",
        "X_val = np.array(val_input) / 255.0\n",
        "\n",
        "enc = OneHotEncoder_from_scratch()\n",
        "enc.fit(Y_train.reshape(-1, 1))\n",
        "Y_train = enc.transform(Y_train.reshape(-1, 1))\n",
        "Y_val = enc.transform(Y_val.reshape(-1, 1))\n",
        "Y_test = enc.transform(Y_test.reshape(-1, 1))\n",
        "\n",
        "print(Y_train.shape, Y_val.shape, Y_test.shape)\n",
        "print(X_train.shape, X_val.shape, X_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrLcGXP5QTUp",
        "outputId": "1b7ce933-68e0-480b-88c6-f91c6a5fe015"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(54000, 10) (6000, 10) (10000, 10)\n",
            "(54000, 784) (6000, 784) (10000, 784)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we will set a configuartion dictionary for future use for wandb seerp\n",
        "# we will take input from that configuration dict\n",
        "\n",
        "config = {'hidden_layers_size' : [32, 64, 128], # We can modify numbers of neurons each layers, parallaly it will modify number of layers\n",
        "          'pre_activation_function': 'sigmoid',\n",
        "          'weight_initialization_method': 'xavier',\n",
        "          }\n",
        "# Now we will dynamically add no of hidden layers key \n",
        "config['hidden_layers_no'] = len(config['hidden_layers_size'])\n"
      ],
      "metadata": {
        "id": "58U2Q5O1FS1p"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FFNetwork:\n",
        "  def __init__(self, X, Y,\n",
        "               epochs = 100, \n",
        "               hidden_layer_count = 4,\n",
        "               hidden_layers =  [32, 64, 128, 256],\n",
        "               learning_rate = 0.001,\n",
        "               batch_size = 32,\n",
        "               activation='tanh',\n",
        "               weight_init='random',\n",
        "               loss = 'MSE',\n",
        "               weight_decay = 0):\n",
        "    \n",
        "    self.inputs =X.shape[1] # Number of inputs\n",
        "    self.outputs= Y.shape[1] # Number of outputs\n",
        "    self.epochs = epochs\n",
        "    self.hidden_layers = hidden_layer_count  # Number of hidden layers \n",
        "    self.network_size= [self.inputs] + hidden_layers +[self.outputs] # input layer + hidden layers + output layers\n",
        "    self.learning_rate = learning_rate\n",
        "    self.batch_size = batch_size\n",
        "    self.weights={} # It will create dictionary for weights and biases\n",
        "    self.weights_h = []\n",
        "    self.num_classes = Y.shape[1]\n",
        "    self.weight_init = weight_init\n",
        "    self.activation_function = activation\n",
        "    self.loss_function = loss\n",
        "    self.lambd = 0\n",
        "    np.random.seed(0)  # We will set seed value so that it will generate same random numebers every time\n",
        "\n",
        "    self.grad_derivatice={}\n",
        "    self.update_weights={}\n",
        "    self.prev_update_weights={}\n",
        "    for i in range(1,self.hidden_layers+1):\n",
        "      vw_key, vb_key, mb_key, mw_key = [f\"{key}{i}\" for key in ['vw', 'vb', 'mb', 'mw']]\n",
        "      self.update_weights[vw_key]=0\n",
        "      self.update_weights[vb_key]=0\n",
        "      self.update_weights[mb_key]=0\n",
        "      self.update_weights[mw_key]=0\n",
        "      self.prev_update_weights[vw_key]=0\n",
        "      self.prev_update_weights[vb_key]=0\n",
        "\n",
        "    \n",
        "    if self.weight_init == 'random':\n",
        "      for i in range(1, self.hidden_layers + 2):\n",
        "          weight_shape = (self.network_size[i - 1], self.network_size[i])\n",
        "          weight_scale = 0.1\n",
        "          self.weights[f'W{i}'] = np.random.normal(scale=weight_scale, size=weight_shape)*0.1\n",
        "          \n",
        "          bias_shape = (1, self.network_size[i])\n",
        "          self.weights[f'B{i}'] = np.zeros(bias_shape)\n",
        "\n",
        "    if self.weight_init == 'Xavier':\n",
        "      for i in range(1, self.hidden_layers + 2):\n",
        "          weight_shape = (self.network_size[i - 1], self.network_size[i])\n",
        "          weight_scale = 0.1\n",
        "          self.weights[f'W{i}'] = np.random.normal(scale=weight_scale, size=weight_shape)*np.sqrt(1/self.network_size[i-1])\n",
        "          \n",
        "          bias_shape = (1, self.network_size[i])\n",
        "          self.weights[f'B{i}'] = np.zeros(bias_shape)\n",
        "\n",
        "  def forward_activation(self, X):\n",
        "      activation_functions = {\n",
        "          'sigmoid': lambda x: 1.0 / (1.0 + np.exp(-x)),\n",
        "          'tanh': np.tanh,\n",
        "          'Relu': lambda x: np.maximum(0, x)\n",
        "      }\n",
        "      activation_function = activation_functions.get(self.activation_function)\n",
        "      if activation_function:\n",
        "          return activation_function(X)\n",
        "      else:\n",
        "          raise ValueError(f\"Unknown activation function '{self.activation_function}'\")\n",
        "\n",
        "\n",
        "  def grad_activation(self, X):\n",
        "      activation_gradients = {\n",
        "          'sigmoid': lambda x: x * (1 - x),\n",
        "          'tanh': lambda x: 1 - np.square(x),\n",
        "          'Relu': lambda x: 1.0 * (x > 0)\n",
        "      }\n",
        "      gradient_function = activation_gradients.get(self.activation_function)\n",
        "      if gradient_function:\n",
        "          return gradient_function(X)\n",
        "      else:\n",
        "          raise ValueError(f\"Unknown activation function '{self.activation_function}'\")\n",
        "\n",
        "\n",
        "  def softmax(self, X):\n",
        "    exps =np.exp(X - np.max(X, axis=1, keepdims=True))\n",
        "    return  exps /np.sum(exps, axis=1, keepdims=True)\n",
        "  \n",
        "\n",
        "  def forward_pass(self, X, weights=None):\n",
        "      if weights is None:\n",
        "          weights = self.weights\n",
        "      self.A = {}\n",
        "      self.H = {}\n",
        "      self.H[0] = X\n",
        "      for i in range(self.hidden_layers):\n",
        "          self.A[i+1] = self.H[i] @ weights[f'W{i+1}'] + weights[f'B{i+1}']\n",
        "          self.H[i+1] = self.forward_activation(self.A[i+1])\n",
        "      self.A[self.hidden_layers+1] = self.H[self.hidden_layers] @ weights[f'W{self.hidden_layers+1}'] + weights[f'B{self.hidden_layers+1}']\n",
        "      self.H[self.hidden_layers+1] = self.softmax(self.A[self.hidden_layers+1])\n",
        "      return self.H[self.hidden_layers+1] \n",
        "  \n",
        "model = FFNetwork(X_train, Y_train,\n",
        "                  epochs = 1, \n",
        "                  hidden_layer_count = 4,\n",
        "                  hidden_layers =  [32, 64, 128, 256],\n",
        "                  learning_rate = 0.001,\n",
        "                  batch_size = 32,\n",
        "                  activation='Relu',\n",
        "                  weight_init='random',\n",
        "                  loss = 'CE')\n",
        "model.forward_pass(X_train)[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjDM3VLYdkxY",
        "outputId": "c18235a5-0b41-46e2-9e32-d1312149805b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.10000004, 0.10000008, 0.09999996, 0.09999994, 0.09999997,\n",
              "       0.10000002, 0.09999998, 0.10000001, 0.10000006, 0.09999994])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    }
  ]
}