{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN3uMT+z/08xFUp7PWDbBgI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swapnilmn/Assignment_1_CS6910/blob/master/Question_2ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Implement a feedforward neural network which takes images from the fashion-mnist data as input and outputs a probability distribution over the 10 classes.\n",
        "\n",
        "Your code should be flexible such that it is easy to change the number of hidden layers and the number of neurons in each hidden layer."
      ],
      "metadata": {
        "id": "v0Q5opYMgYb-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries "
      ],
      "metadata": {
        "id": "_dP0oEKrgmQc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CZcFz8GygBp5"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import fashion_mnist, mnist\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split \n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class OneHotEncoder_from_scratch:\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.categories = None\n",
        "    def fit(self, X):\n",
        "        self.categories =[]\n",
        "        for i in range(X.shape[1]):\n",
        "            feature_categories =list(set(X[:, i]))\n",
        "            self.categories.append(feature_categories)\n",
        "            \n",
        "    def transform(self, X):\n",
        "        one_hot_vector = []\n",
        "\n",
        "        for i in range(X.shape[0]):\n",
        "            one_hot_row = []\n",
        "            for j in range(X.shape[1]):\n",
        "\n",
        "                category_index = self.categories[j].index(X[i, j])\n",
        "                category_one_hot =[0] *len(self.categories[j])\n",
        "                category_one_hot[category_index] = 1\n",
        "\n",
        "                one_hot_row.extend(category_one_hot)\n",
        "            one_hot_vector.append(one_hot_row)\n",
        "        return np.array(one_hot_vector)\n",
        "\n",
        "X = np.array([[1],[2]]) \n",
        "enc = OneHotEncoder_from_scratch()\n",
        "enc.fit(X)\n",
        "enc.transform(X)\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "enc.fit(train_labels.reshape(-1, 1))\n",
        "enc.transform(train_labels.reshape(-1, 1))\n",
        "#train_labels.reshape(-1, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjPsVG8wafVH",
        "outputId": "92465313-27e7-4901-d751-64e7db143190"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, ..., 0, 0, 1],\n",
              "       [1, 0, 0, ..., 0, 0, 0],\n",
              "       [1, 0, 0, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [1, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = 'fashion_mnist'\n",
        "\n",
        "def dataset_type(dataset):\n",
        "  if dataset == 'fashion_mnist':\n",
        "      (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "  elif dataset == 'mnist':\n",
        "      (train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "  else:\n",
        "      raise ValueError('Invalid dataset name')\n",
        "  X_train, X_val, Y_train, Y_val = train_test_split(train_images, train_labels, test_size=0.1, random_state=42)\n",
        "  train_input = []\n",
        "  for i in range(len(X_train)):\n",
        "      train_input.append(list(np.concatenate(X_train[i]).flat))\n",
        "\n",
        "  val_input = []\n",
        "  for i in range(len(X_val)):\n",
        "      val_input.append(list(np.concatenate(X_val[i]).flat))\n",
        "\n",
        "  test_input = []\n",
        "  for i in range(len(test_images)):\n",
        "      test_input.append(list(np.concatenate(test_images[i]).flat))\n",
        "  Y_train = np.array(Y_train)\n",
        "  Y_val = np.array(Y_val)\n",
        "  Y_test = np.array(test_labels)\n",
        "\n",
        "  X_train = np.array(train_input) / 255.0\n",
        "  X_test = np.array(test_input) / 255.0\n",
        "  X_val = np.array(val_input) / 255.0\n",
        "\n",
        "  enc = OneHotEncoder_from_scratch()\n",
        "  enc.fit(Y_train.reshape(-1, 1))\n",
        "  Y_train = enc.transform(Y_train.reshape(-1, 1))\n",
        "  Y_val = enc.transform(Y_val.reshape(-1, 1))\n",
        "  Y_test = enc.transform(Y_test.reshape(-1, 1))\n",
        "\n",
        "  return X_train, X_val, X_test, Y_train, Y_val, Y_test\n",
        "\n",
        "dataset = 'fashion_mnist'\n",
        "X_train, X_val, X_test, Y_train, Y_val, Y_test = dataset_type(dataset)\n",
        "\n",
        "print(Y_train.shape, Y_val.shape, Y_test.shape)\n",
        "print(X_train.shape, X_val.shape, X_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrLcGXP5QTUp",
        "outputId": "ce13dc3e-c87c-436f-e952-dcf8ee9999b7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(54000, 10) (6000, 10) (10000, 10)\n",
            "(54000, 784) (6000, 784) (10000, 784)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we will set a configuartion dictionary for future use for wandb seerp\n",
        "# we will take input from that configuration dict\n",
        "\n",
        "config = {'hidden_layers_size' : [32, 64, 128], # We can modify numbers of neurons each layers, parallaly it will modify number of layers\n",
        "          'pre_activation_function': 'sigmoid',\n",
        "          'weight_initialization_method': 'xavier',\n",
        "          }\n",
        "# Now we will dynamically add no of hidden layers key \n",
        "config['hidden_layers_no'] = len(config['hidden_layers_size'])\n"
      ],
      "metadata": {
        "id": "58U2Q5O1FS1p"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FFNN:\n",
        "  def __init__(self, X, Y,\n",
        "               epochs = 100, \n",
        "               hidden_layer_count = 4,\n",
        "               hidden_layers =  [32, 64, 128, 256],\n",
        "               learning_rate = 0.001,\n",
        "               batch_size = 32,\n",
        "               activation='tanh',\n",
        "               weight_init='random',\n",
        "               loss = 'cross_entropy',\n",
        "               weight_decay = 0):\n",
        "    \n",
        "    self.inputs =X.shape[1] # Number of inputs\n",
        "    self.outputs= Y.shape[1] # Number of outputs\n",
        "    self.epochs = epochs\n",
        "    self.hidden_layers = hidden_layer_count  # Number of hidden layers \n",
        "    self.network_size= [self.inputs] + hidden_layers +[self.outputs] # input layer + hidden layers + output layers\n",
        "    self.learning_rate = learning_rate\n",
        "    self.batch_size = batch_size\n",
        "    self.weights={} # It will create dictionary for weights and biases\n",
        "    self.weights_h = []\n",
        "    self.num_classes = Y.shape[1]\n",
        "    self.weight_init = weight_init\n",
        "    self.activation_function = activation\n",
        "    self.loss_function = loss\n",
        "    self.lambd = weight_decay\n",
        "    np.random.seed(0)  # We will set seed value so that it will generate same random numebers every time\n",
        "\n",
        "    self.grad_derivatice={}\n",
        "    self.update_weights={}\n",
        "    self.prev_update_weights={}\n",
        "    for i in range(1,self.hidden_layers+1):\n",
        "      vw_key, vb_key, mb_key, mw_key = [f'{key}{i}' for key in ['vw', 'vb', 'mb', 'mw']]\n",
        "      self.update_weights[vw_key]=0\n",
        "      self.update_weights[vb_key]=0\n",
        "      self.update_weights[mb_key]=0\n",
        "      self.update_weights[mw_key]=0\n",
        "      self.prev_update_weights[vw_key]=0\n",
        "      self.prev_update_weights[vb_key]=0\n",
        "\n",
        "    # for creating initial weights\n",
        "    if self.weight_init == 'random':\n",
        "      for i in range(1, self.hidden_layers + 2):\n",
        "          weight_shape = (self.network_size[i - 1], self.network_size[i])\n",
        "          weight_scale = 0.1\n",
        "          self.weights[f'W{i}'] = np.random.normal(scale=weight_scale, size=weight_shape)*0.1\n",
        "          # we are mulliplying ny 0.1 to get less wrights\n",
        "          bias_shape = (1, self.network_size[i])\n",
        "          self.weights[f'B{i}'] = np.zeros(bias_shape)\n",
        "\n",
        "    if self.weight_init == 'Xavier':\n",
        "      for i in range(1, self.hidden_layers + 2):\n",
        "          weight_shape = (self.network_size[i - 1], self.network_size[i])\n",
        "          weight_scale = 0.1\n",
        "          self.weights[f'W{i}'] = np.random.normal(scale=weight_scale, size=weight_shape)*np.sqrt(1/self.network_size[i-1])\n",
        "          \n",
        "          bias_shape = (1, self.network_size[i])\n",
        "          self.weights[f'B{i}'] = np.zeros(bias_shape)\n",
        "\n",
        "\n",
        "\n",
        "  def forward_activation(self, X):\n",
        "      activation_functions = {\n",
        "          'sigmoid': lambda x: 1.0 / (1.0 + np.exp(-x)),\n",
        "          'tanh': np.tanh,\n",
        "          'ReLU': lambda x: np.maximum(0, x)\n",
        "      }\n",
        "      activation_function = activation_functions.get(self.activation_function)\n",
        "      if activation_function:\n",
        "          return activation_function(X)\n",
        "      else:\n",
        "          raise ValueError(f\"Unknown activation function '{self.activation_function}'\")\n",
        "\n",
        "\n",
        "  def grad_activation(self, X):\n",
        "      activation_gradients = {\n",
        "          'sigmoid': lambda x: x * (1 - x),\n",
        "          'tanh': lambda x: 1 - np.square(x),\n",
        "          'ReLU': lambda x: 1.0 * (x > 0)\n",
        "      }\n",
        "      gradient_function = activation_gradients.get(self.activation_function)\n",
        "      if gradient_function:\n",
        "          return gradient_function(X)\n",
        "      else:\n",
        "          raise ValueError(f\"Unknown activation function '{self.activation_function}'\")\n",
        "\n",
        "  def softmax(self, X):\n",
        "    exps =np.exp(X - np.max(X, axis=1, keepdims=True))\n",
        "    return  exps /np.sum(exps, axis=1, keepdims=True)\n",
        "  \n",
        "\n",
        "  def forward_pass(self, X, weights=None):\n",
        "    #X: shape (batch_size, input_dim)\n",
        "    if weights is None:\n",
        "\n",
        "        weights =self.weights\n",
        "    #weights:a dictionary containing weight and bias parameters for each layer\n",
        "    self.z = {}\n",
        "    self.z = {}\n",
        "    self.z[0] = X\n",
        "    for i in range(self.hidden_layers):\n",
        "        self.z[i+1] = self.z[i]@weights[f'W{i+1}'] + weights[f'B{i+1}'] # weights[f'W{i+1}'](input_dim, output_dim) + # weights[f'B{i+1}'](1, output_dim)\n",
        "        #z is dictionary containing the output of each layer's activationfunction\n",
        "        self.z[i+1] = self.forward_activation(self.z[i+1]) # self.z[i+1](batch_size, output_dim)\n",
        "    self.z[self.hidden_layers+1] = self.z[self.hidden_layers] @ weights[f'W{self.hidden_layers+1}'] + weights[f'B{self.hidden_layers+1}']\n",
        "\n",
        "    #self.z[self.hidden_layers+1](batch_size, output_dim)\n",
        "    self.z[self.hidden_layers+1]=  self.softmax(self.z[self.hidden_layers+1])\n",
        "\n",
        "    return self.z[self.hidden_layers+1]\n",
        "\n",
        "  \n",
        "model = FFNN(X_train, Y_train,\n",
        "                  epochs = 1, \n",
        "                  hidden_layer_count = 4,\n",
        "                  hidden_layers =  [32, 64, 128, 256],\n",
        "                  learning_rate = 0.001,\n",
        "                  batch_size = 32,\n",
        "                  activation='ReLU',\n",
        "                  weight_init='random',\n",
        "                  loss = 'cross_entropy')\n",
        "model.forward_pass(X_train)[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjDM3VLYdkxY",
        "outputId": "d8f72b30-1cfd-4f06-9b47-d25e14c2a66f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.10000004, 0.10000008, 0.09999996, 0.09999994, 0.09999997,\n",
              "       0.10000002, 0.09999998, 0.10000001, 0.10000006, 0.09999994])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ]
}