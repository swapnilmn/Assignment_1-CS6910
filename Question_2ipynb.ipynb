{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOr0xY2a9gnNKk+oScthgzv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swapnilmn/Assignment_1_CS6910/blob/master/Question_2ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Implement a feedforward neural network which takes images from the fashion-mnist data as input and outputs a probability distribution over the 10 classes.\n",
        "\n",
        "Your code should be flexible such that it is easy to change the number of hidden layers and the number of neurons in each hidden layer."
      ],
      "metadata": {
        "id": "v0Q5opYMgYb-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries "
      ],
      "metadata": {
        "id": "_dP0oEKrgmQc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CZcFz8GygBp5"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import fashion_mnist, mnist\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split \n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class OneHotEncoder_from_scratch:\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.categories = None\n",
        "    def fit(self, X):\n",
        "        self.categories =[]\n",
        "        for i in range(X.shape[1]):\n",
        "            feature_categories =list(set(X[:, i]))\n",
        "            self.categories.append(feature_categories)\n",
        "            \n",
        "    def transform(self, X):\n",
        "        one_hot_vector = []\n",
        "\n",
        "        for i in range(X.shape[0]):\n",
        "            one_hot_row = []\n",
        "            for j in range(X.shape[1]):\n",
        "\n",
        "                category_index = self.categories[j].index(X[i, j])\n",
        "                category_one_hot =[0] *len(self.categories[j])\n",
        "                category_one_hot[category_index] = 1\n",
        "\n",
        "                one_hot_row.extend(category_one_hot)\n",
        "            one_hot_vector.append(one_hot_row)\n",
        "        return np.array(one_hot_vector)\n",
        "\n",
        "X = np.array([[1],[2]]) \n",
        "enc = OneHotEncoder_from_scratch()\n",
        "enc.fit(X)\n",
        "enc.transform(X)\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "enc.fit(train_labels.reshape(-1, 1))\n",
        "enc.transform(train_labels.reshape(-1, 1))\n",
        "#train_labels.reshape(-1, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjPsVG8wafVH",
        "outputId": "722e47fc-88ab-4c48-a45c-d26f948aa330"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, ..., 0, 0, 1],\n",
              "       [1, 0, 0, ..., 0, 0, 0],\n",
              "       [1, 0, 0, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [1, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def dataset_type(dataset = 'fashion_mnist'):\n",
        "  if dataset == 'fashion_mnist':\n",
        "      (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "  elif dataset == 'mnist':\n",
        "      (train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "  else:\n",
        "      raise ValueError('Invalid dataset name')\n",
        "  X_train, X_val, Y_train, Y_val = train_test_split(train_images, train_labels, test_size=0.1, random_state=42)\n",
        "  # Flatten input images\n",
        "  train_input = X_train.reshape((X_train.shape[0], -1))\n",
        "  test_input = test_images.reshape((test_images.shape[0], -1))\n",
        "  val_input = X_val.reshape((X_val.shape[0], -1))\n",
        "\n",
        "  Y_train = np.array(Y_train)\n",
        "  Y_val = np.array(Y_val)\n",
        "  Y_test = np.array(test_labels)\n",
        "\n",
        "  X_train = np.array(train_input) / 255.0\n",
        "  X_test = np.array(test_input) / 255.0\n",
        "  X_val = np.array(val_input) / 255.0\n",
        "\n",
        "  enc = OneHotEncoder_from_scratch()\n",
        "  enc.fit(Y_train.reshape(-1, 1))\n",
        "  Y_train = enc.transform(Y_train.reshape(-1, 1))\n",
        "  Y_val = enc.transform(Y_val.reshape(-1, 1))\n",
        "  Y_test = enc.transform(Y_test.reshape(-1, 1))\n",
        "\n",
        "  return X_train, X_val, X_test, Y_train, Y_val, Y_test\n",
        "\n",
        "X_train, X_val, X_test, Y_train, Y_val, Y_test = dataset_type(dataset = 'fashion_mnist')\n",
        "# print(Y_train.shape, Y_val.shape, Y_test.shape)\n",
        "# print(X_train.shape, X_val.shape, X_test.shape)\n",
        "\n",
        "print(Y_train.shape, Y_val.shape, Y_test.shape)\n",
        "print(X_train.shape, X_val.shape, X_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrLcGXP5QTUp",
        "outputId": "cb2a9c71-1335-4133-989d-6714ff7fa3cc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(54000, 10) (6000, 10) (10000, 10)\n",
            "(54000, 784) (6000, 784) (10000, 784)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we will set a configuartion dictionary for future use for wandb seerp\n",
        "# we will take input from that configuration dict\n",
        "\n",
        "config = {'hidden_layers_size' : [32, 64, 128], # We can modify numbers of neurons each layers, parallaly it will modify number of layers\n",
        "          'pre_activation_function': 'sigmoid',\n",
        "          'weight_initialization_method': 'xavier',\n",
        "          }\n",
        "# Now we will dynamically add no of hidden layers key \n",
        "config['hidden_layers_no'] = len(config['hidden_layers_size'])\n"
      ],
      "metadata": {
        "id": "58U2Q5O1FS1p"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FFNN:\n",
        "  def __init__(self, X, Y,\n",
        "               epochs=100, \n",
        "               hidden_layer_count=4,\n",
        "               hidden_layers=[32, 64, 128, 256],\n",
        "               learning_rate=0.001,\n",
        "               batch_size=32,\n",
        "               activation='tanh',\n",
        "               weight_init='random',\n",
        "               loss='mean_squared_error',\n",
        "               weight_decay=0):\n",
        "    \n",
        "    self.params = {\n",
        "      'inputs': X.shape[1],\n",
        "      'outputs': Y.shape[1],\n",
        "      'epochs': epochs,\n",
        "      'hidden_layers': hidden_layer_count,\n",
        "      'network_size': [X.shape[1]] + hidden_layers +[Y.shape[1]],\n",
        "      'learning_rate': learning_rate,\n",
        "      'batch_size': batch_size,\n",
        "      'weights': {},\n",
        "      'weight_init': weight_init,\n",
        "      'activation_function':activation,\n",
        "      'loss_function': loss,\n",
        "      'lambd': weight_decay\n",
        "    }\n",
        "\n",
        "    self.update(self.params)\n",
        "    np.random.seed(0)\n",
        "\n",
        "  def update(self, params):\n",
        "    for key, value in params.items():\n",
        "      setattr(self, key, value)\n",
        "\n",
        "    self.grad_derivatice={}\n",
        "    self.u_w = {f'{key}{i}': 0 for i in range(1, self.hidden_layers+1) for key in ['vw','vb', 'mb', 'mw']}\n",
        "    self.p_u_w = {f'{key}{i}': 0 for i in range(1, self.hidden_layers+1) for key in ['vw', 'vb', 'mb', 'mw']}\n",
        "\n",
        "    # for creating initial weights\n",
        "    if self.weight_init == 'random':\n",
        "      for i in range(1, self.hidden_layers + 2):\n",
        "          weight_shape = (self.network_size[i - 1], self.network_size[i])\n",
        "          weight_scale = 0.1\n",
        "          np.random.seed(0) # including random seet fpr reproducibilty\n",
        "          self.weights[f'W{i}'] = np.random.normal(scale=weight_scale, size=weight_shape)*0.1\n",
        "          # we are mulliplying ny 0.1 to get less wrights\n",
        "          bias_shape = (1, self.network_size[i])\n",
        "          self.weights[f'B{i}'] = np.zeros(bias_shape)\n",
        "\n",
        "    if self.weight_init == 'Xavier':\n",
        "      for i in range(1, self.hidden_layers + 2):\n",
        "          weight_shape = (self.network_size[i - 1],self.network_size[i])\n",
        "          weight_scale = 0.1\n",
        "          np.random.seed(0)\n",
        "          self.weights[f'W{i}'] = np.random.normal(scale=weight_scale,size=weight_shape)*np.sqrt(1/self.network_size[i-1])\n",
        "          # we are mulliplying ny 0.1 to get less wrights\n",
        "          bias_shape = (1, self.network_size[i])\n",
        "          self.weights[f'B{i}'] = np.zeros(bias_shape)\n",
        "\n",
        "\n",
        "\n",
        "  def forward_activation(self, X):\n",
        "      # Define a dictionary of activation functions and their corresponding lambda functions\n",
        "    activation_functions = {\n",
        "        'sigmoid': lambda x: 1.0 / (1.0 + np.exp(-x)), # sigmoid activation function\n",
        "        'tanh': np.tanh, # hyperbolic tangent activation function\n",
        "        'ReLU': lambda x: np.maximum(0, x) # rectified linear unit (ReLU) activation function\n",
        "    }\n",
        "    # Get the activation function based on the value of `self.activation_function`\n",
        "    activation_function = activation_functions.get(self.activation_function)\n",
        "    # If the activation function is found, apply it to the input matrix `X`\n",
        "    if activation_function:\n",
        "        return activation_function(X)\n",
        "    # If the activation function is not found, raise a ValueError indicating that it is unknown\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown activation function '{self.activation_function}'\")\n",
        "\n",
        "\n",
        "\n",
        "  def grad_activation(self, X):\n",
        "    # Define a dictionary of activation function derivatives and their corresponding lambda functions\n",
        "    activation_gradients = {\n",
        "        'sigmoid': lambda x: x * (1 - x), # derivative of the sigmoid activation function\n",
        "        'tanh': lambda x: 1 - np.square(x), # derivative of the hyperbolic tangent activation function\n",
        "        'ReLU': lambda x: 1.0 * (x > 0) # derivative of the rectified linear unit (ReLU) activation function\n",
        "    }\n",
        "    # Get the derivative of the activation function based on the value of `self.activation_function`\n",
        "    gradient_function = activation_gradients.get(self.activation_function)\n",
        "    # If the derivative function is found, apply it to the input matrix `X`\n",
        "    if gradient_function:\n",
        "        return gradient_function(X)\n",
        "    # If theerivative function is not found, raise a alueError indicating that it is unknown\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown activation function '{self.activation_function}'\")\n",
        "\n",
        "\n",
        "  def softmax(self, X):\n",
        "    exps =np.exp(X - np.max(X, axis=1, keepdims=True)) # to reduce long numbers\n",
        "    return  exps /np.sum(exps, axis=1, keepdims=True)\n",
        "\n",
        "\n",
        "  def forward_pass(self, X, weights=None):\n",
        "    # X: shape (batch_size, input_dim)\n",
        "    if weights is None:\n",
        "        weights = self.weights\n",
        "\n",
        "    # Initialize dictionaries to store intermediate outputs\n",
        "    self.z = {}\n",
        "    self.h = {}\n",
        "    self.h[0] = X\n",
        "\n",
        "    # Perform forward pass through hidden layers\n",
        "    for i in range(self.hidden_layers):\n",
        "        # Compute weighted sum of inputs and biases\n",
        "        z_i = self.h[i] @ weights[f'W{i+1}']\n",
        "        z_i = z_i + weights[f'B{i+1}']\n",
        "        self.z[i+1] = z_i\n",
        "        # Apply activation function\n",
        "        h_i = self.forward_activation(z_i)\n",
        "        self.h[i+1] = h_i\n",
        "\n",
        "    # Compute final output\n",
        "    z_final = self.h[self.hidden_layers] @ weights[f'W{self.hidden_layers+1}']\n",
        "    z_final = z_final + weights[f'B{self.hidden_layers+1}']\n",
        "    self.z[self.hidden_layers+1] = z_final\n",
        "\n",
        "    # Apply softmax activation functionto final output\n",
        "    h_final = self.softmax(z_final)\n",
        "    self.h[self.hidden_layers+1] = h_final\n",
        "    # Return final output\n",
        "    return h_final\n",
        "\n",
        "  \n",
        "model = FFNN(X_train, Y_train,\n",
        "                  epochs = 1, \n",
        "                  hidden_layer_count = 4,\n",
        "                  hidden_layers =  [32, 64, 128, 256],\n",
        "                  learning_rate = 0.001,\n",
        "                  batch_size = 32,\n",
        "                  activation='ReLU',\n",
        "                  weight_init='random',\n",
        "                  loss = 'cross_entropy')\n",
        "model.forward_pass(X_train)[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjDM3VLYdkxY",
        "outputId": "03df6024-0faa-4de1-ba60-d94b270e1219"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.09999944, 0.09999937, 0.10000029, 0.10000046, 0.09999999,\n",
              "       0.1000001 , 0.09999989, 0.10000012, 0.09999968, 0.10000066])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    }
  ]
}