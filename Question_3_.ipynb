{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMgxO2zrQac68e4BkAhjDYb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2da7c78f66cf4ecf8f49333aed87254c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c1435e9f24d541839480dfd1ba10c6d7",
              "IPY_MODEL_8bc894367c9a44a3a6bcbb501706be8b",
              "IPY_MODEL_7c9c74642b0b4882acfac03a5e7f8cd5"
            ],
            "layout": "IPY_MODEL_ca60f75b4077434e9d6310c1347a3c5c"
          }
        },
        "c1435e9f24d541839480dfd1ba10c6d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec78f1fa449a4049bf28730b485cda67",
            "placeholder": "​",
            "style": "IPY_MODEL_006f6610a3b94b818cfa91ff39d241e0",
            "value": "100%"
          }
        },
        "8bc894367c9a44a3a6bcbb501706be8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f86253aeed446e59d0e19f61b4a771b",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a98c76a2f18943568cd3032b2d10bbea",
            "value": 5
          }
        },
        "7c9c74642b0b4882acfac03a5e7f8cd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_72357e8d6b1c4d6eac202d35b7ae213d",
            "placeholder": "​",
            "style": "IPY_MODEL_cc794ba6779947e09f823f398f5ec781",
            "value": " 5/5 [00:47&lt;00:00,  8.15s/epoch]"
          }
        },
        "ca60f75b4077434e9d6310c1347a3c5c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec78f1fa449a4049bf28730b485cda67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "006f6610a3b94b818cfa91ff39d241e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5f86253aeed446e59d0e19f61b4a771b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a98c76a2f18943568cd3032b2d10bbea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "72357e8d6b1c4d6eac202d35b7ae213d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc794ba6779947e09f823f398f5ec781": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swapnilmn/Assignment_1-CS6910/blob/master/Question_3_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Question 3 (24 Marks) Implement the backpropagation algorithm with support for the following optimisation functions\n",
        "            sgd\n",
        "            momentum based gradient descent\n",
        "            nesterov accelerated gradient descent\n",
        "            rmsprop\n",
        "            adam\n",
        "            nadam\n",
        "\n",
        "(12 marks for the backpropagation framework and 2 marks for each of the optimisation algorithms above)\n",
        "\n",
        "We will check the code for implementation and ease of use (e.g., how easy it is to add a new optimisation algorithm such as Eve). Note that the code should be flexible enough to work with different batch sizes."
      ],
      "metadata": {
        "id": "3kVFZwNkEA7D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries "
      ],
      "metadata": {
        "id": "_dP0oEKrgmQc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZcFz8GygBp5"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import fashion_mnist, mnist\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import copy\n",
        "from sklearn.model_selection import train_test_split \n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb\n",
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 798
        },
        "id": "BvSVAtZsdAoe",
        "outputId": "7faeba50-643d-4ccd-b733-439cb56bb1a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.13.11-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.12.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (3.19.6)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from wandb) (57.4.0)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from wandb) (4.5.0)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.8/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (8.1.3)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31 kB)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (2.25.1)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.16.0-py2.py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.15.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.14)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (4.0.0)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=9e06225ff24893a7d84aaceb4f405164459c7e1704d45609caa486b952f829de\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/8e/7e/72fbc243e1aeecae64a96875432e70d4e92f3d2d18123be004\n",
            "Successfully built pathtools\n",
            "Installing collected packages: pathtools, smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.31 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.16.0 setproctitle-1.3.2 smmap-5.0.0 wandb-0.13.11\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Processing"
      ],
      "metadata": {
        "id": "W-FlxB4-gnqT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OneHotEncoder_from_scratch:\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.categories = None\n",
        "    def fit(self, X):\n",
        "        self.categories =[]\n",
        "        for i in range(X.shape[1]):\n",
        "            feature_categories =list(set(X[:, i]))\n",
        "            self.categories.append(feature_categories)\n",
        "            \n",
        "    def transform(self, X):\n",
        "        one_hot_vector = []\n",
        "\n",
        "        for i in range(X.shape[0]):\n",
        "            one_hot_row = []\n",
        "            for j in range(X.shape[1]):\n",
        "\n",
        "                category_index = self.categories[j].index(X[i, j])\n",
        "                category_one_hot =[0] *len(self.categories[j])\n",
        "                category_one_hot[category_index] = 1\n",
        "\n",
        "                one_hot_row.extend(category_one_hot)\n",
        "            one_hot_vector.append(one_hot_row)\n",
        "        return np.array(one_hot_vector)"
      ],
      "metadata": {
        "id": "YTDGKrnOgGwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = 'fashion_mnist'\n",
        "if dataset == 'fashion_mnist':\n",
        "    (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "elif dataset == 'mnist':\n",
        "    (train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "else:\n",
        "    raise ValueError('Invalid dataset name')\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(train_images, train_labels, test_size=0.1, random_state=42)\n",
        "train_input = []\n",
        "for i in range(len(X_train)):\n",
        "    train_input.append(list(np.concatenate(X_train[i]).flat))\n",
        "\n",
        "val_input = []\n",
        "for i in range(len(X_val)):\n",
        "    val_input.append(list(np.concatenate(X_val[i]).flat))\n",
        "\n",
        "test_input = []\n",
        "for i in range(len(test_images)):\n",
        "    test_input.append(list(np.concatenate(test_images[i]).flat))\n",
        "Y_train = np.array(Y_train)\n",
        "Y_val = np.array(Y_val)\n",
        "Y_test = np.array(test_labels)\n",
        "\n",
        "X_train = np.array(train_input) / 255.0\n",
        "X_test = np.array(test_input) / 255.0\n",
        "X_val = np.array(val_input) / 255.0\n",
        "\n",
        "enc = OneHotEncoder_from_scratch()\n",
        "enc.fit(Y_train.reshape(-1, 1))\n",
        "Y_train = enc.transform(Y_train.reshape(-1, 1))\n",
        "Y_val = enc.transform(Y_val.reshape(-1, 1))\n",
        "Y_test = enc.transform(Y_test.reshape(-1, 1))\n",
        "\n",
        "print(Y_train.shape, Y_val.shape, Y_test.shape)\n",
        "print(X_train.shape, X_val.shape, X_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzdukjGZgZeJ",
        "outputId": "201c6528-734a-4b82-dabd-e4c1c609a8bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n",
            "(54000, 10) (6000, 10) (10000, 10)\n",
            "(54000, 784) (6000, 784) (10000, 784)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Class of FeedForward Neural Network"
      ],
      "metadata": {
        "id": "DXX2kyFig0i5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(project = 'Question_4_Best_Model')\n",
        "\n",
        "class FFNN:\n",
        "  def __init__(self, X, Y,\n",
        "               epochs = 100, \n",
        "               hidden_layer_count = 4,\n",
        "               hidden_layers =  [32, 64, 128, 256],\n",
        "               learning_rate = 0.001,\n",
        "               batch_size = 32,\n",
        "               activation='tanh',\n",
        "               weight_init='random',\n",
        "               loss = 'MSE',\n",
        "               weight_decay = 0):\n",
        "    \n",
        "    self.inputs =X.shape[1] # Number of inputs\n",
        "    self.outputs= Y.shape[1] # Number of outputs\n",
        "    self.epochs = epochs\n",
        "    self.hidden_layers = hidden_layer_count  # Number of hidden layers \n",
        "    self.network_size= [self.inputs] + hidden_layers +[self.outputs] # input layer + hidden layers + output layers\n",
        "    self.learning_rate = learning_rate\n",
        "    self.batch_size = batch_size\n",
        "    self.weights={} # It will create dictionary for weights and biases\n",
        "    self.weights_h = []\n",
        "    self.num_classes = Y.shape[1]\n",
        "    self.weight_init = weight_init\n",
        "    self.activation_function = activation\n",
        "    self.loss_function = loss\n",
        "    self.lambd = 0\n",
        "    np.random.seed(0)  # We will set seed value so that it will generate same random numebers every time\n",
        "\n",
        "    self.grad_derivatice={}\n",
        "    self.update_weights={}\n",
        "    self.prev_update_weights={}\n",
        "    for i in range(1,self.hidden_layers+1):\n",
        "      vw_key, vb_key, mb_key, mw_key = [f\"{key}{i}\" for key in ['vw', 'vb', 'mb', 'mw']]\n",
        "      self.update_weights[vw_key]=0\n",
        "      self.update_weights[vb_key]=0\n",
        "      self.update_weights[mb_key]=0\n",
        "      self.update_weights[mw_key]=0\n",
        "      self.prev_update_weights[vw_key]=0\n",
        "      self.prev_update_weights[vb_key]=0\n",
        "\n",
        "    \n",
        "    if self.weight_init == 'random':\n",
        "      for i in range(1, self.hidden_layers + 2):\n",
        "          weight_shape = (self.network_size[i - 1], self.network_size[i])\n",
        "          weight_scale = 0.1\n",
        "          self.weights[f'W{i}'] = np.random.normal(scale=weight_scale, size=weight_shape)*0.1\n",
        "          \n",
        "          bias_shape = (1, self.network_size[i])\n",
        "          self.weights[f'B{i}'] = np.zeros(bias_shape)\n",
        "\n",
        "    if self.weight_init == 'Xavier':\n",
        "      for i in range(1, self.hidden_layers + 2):\n",
        "          weight_shape = (self.network_size[i - 1], self.network_size[i])\n",
        "          weight_scale = 0.1\n",
        "          self.weights[f'W{i}'] = np.random.normal(scale=weight_scale, size=weight_shape)*np.sqrt(1/self.network_size[i-1])\n",
        "          \n",
        "          bias_shape = (1, self.network_size[i])\n",
        "          self.weights[f'B{i}'] = np.zeros(bias_shape)\n",
        "\n",
        "  def forward_activation(self, X):\n",
        "      activation_functions = {\n",
        "          'sigmoid': lambda x: 1.0 / (1.0 + np.exp(-x)),\n",
        "          'tanh': np.tanh,\n",
        "          'Relu': lambda x: np.maximum(0, x)\n",
        "      }\n",
        "      activation_function = activation_functions.get(self.activation_function)\n",
        "      if activation_function:\n",
        "          return activation_function(X)\n",
        "      else:\n",
        "          raise ValueError(f\"Unknown activation function '{self.activation_function}'\")\n",
        "\n",
        "\n",
        "  def grad_activation(self, X):\n",
        "      activation_gradients = {\n",
        "          'sigmoid': lambda x: x * (1 - x),\n",
        "          'tanh': lambda x: 1 - np.square(x),\n",
        "          'Relu': lambda x: 1.0 * (x > 0)\n",
        "      }\n",
        "      gradient_function = activation_gradients.get(self.activation_function)\n",
        "      if gradient_function:\n",
        "          return gradient_function(X)\n",
        "      else:\n",
        "          raise ValueError(f\"Unknown activation function '{self.activation_function}'\")\n",
        "\n",
        "\n",
        "  def softmax(self, X):\n",
        "    exps =np.exp(X - np.max(X, axis=1, keepdims=True))\n",
        "    return  exps /np.sum(exps, axis=1, keepdims=True)\n",
        "  \n",
        "\n",
        "  def forward_pass(self, X, weights=None):\n",
        "      if weights is None:\n",
        "          weights = self.weights\n",
        "      self.A = {}\n",
        "      self.H = {}\n",
        "      self.H[0] = X\n",
        "      for i in range(self.hidden_layers):\n",
        "          self.A[i+1] = self.H[i] @ weights[f'W{i+1}'] + weights[f'B{i+1}']\n",
        "          self.H[i+1] = self.forward_activation(self.A[i+1])\n",
        "      self.A[self.hidden_layers+1] = self.H[self.hidden_layers] @ weights[f'W{self.hidden_layers+1}'] + weights[f'B{self.hidden_layers+1}']\n",
        "      self.H[self.hidden_layers+1] = self.softmax(self.A[self.hidden_layers+1])\n",
        "      return self.H[self.hidden_layers+1]\n",
        "\n",
        "  def backprop(self, X, Y, weights=None):\n",
        "    if weights is None:\n",
        "        weights = self.weights\n",
        "\n",
        "    self.forward_pass(X, weights)\n",
        "    self.grad_derivatice = {}\n",
        "    L = self.hidden_layers + 1\n",
        "\n",
        "    if self.loss_function == 'CE':\n",
        "        self.grad_derivatice[f'dA{L}'] = (self.H[L] - Y) * (1/X.shape[0])\n",
        "    elif self.loss_function == 'MSE':\n",
        "        self.grad_derivatice[f'dA{L}'] = (1/X.shape[0]) * 2 * (self.H[L] - Y)\n",
        "\n",
        "    for k in range(L, 0, -1):\n",
        "        w_key, b_key, dw_key, db_key, da_key = [f\"{key}{k}\" for key in ['W', 'B', 'dW', 'dB', 'dA']]\n",
        "        self.grad_derivatice[dw_key] = np.matmul(self.H[k-1].T, self.grad_derivatice[da_key]) + self.lambd * weights[w_key]\n",
        "        self.grad_derivatice[db_key] = np.sum(self.grad_derivatice[da_key], axis=0).reshape(1, -1)\n",
        "        self.grad_derivatice[f'dH{k-1}'] = np.matmul(self.grad_derivatice[da_key], weights[w_key].T)\n",
        "        self.grad_derivatice[f'dA{k-1}'] = np.multiply(self.grad_derivatice[f'dH{k-1}'], self.grad_activation(self.H[k-1]))\n",
        "\n",
        "    return self.grad_derivatice[f'dH{k-1}']\n",
        "\n",
        "  def fit(self, X, Y, X_val, Y_val,algo= 'GD',a = 10, eps=1e-8, beta=0.9, beta1=0.9, beta2=0.9, gamma=0.9 , show_loss = False):\n",
        "\n",
        "    if show_loss:\n",
        "      loss = []\n",
        "    for num_epoch in tqdm(range(1, self.epochs+1), unit='epoch'):\n",
        "      m = X.shape[0]\n",
        "      \n",
        "      if algo == 'SGD':\n",
        "        for i in range(m):\n",
        "            rand_idx = np.random.randint(m)\n",
        "            x_i = X[rand_idx:rand_idx+1]\n",
        "            y_i = Y[rand_idx:rand_idx+1]\n",
        "            self.backprop(x_i, y_i)\n",
        "            for j in range(1, self.hidden_layers+1):\n",
        "              w_key, b_key, dw_key, db_key = [f\"{key}{j}\" for key in ['W', 'B', 'dW', 'dB']]\n",
        "              self.weights[w_key] -=self.learning_rate * self.grad_derivatice[dw_key]\n",
        "              self.weights[b_key] -=self.learning_rate * self.grad_derivatice[db_key]\n",
        "        self.wandlog(num_epoch, X, Y, X_val, Y_val)\n",
        "\n",
        "      elif algo == 'Momentum':\n",
        "        num_examples = X.shape[0]\n",
        "        num_batches = num_examples //self.batch_size\n",
        "        for batch in range(num_batches + 1):\n",
        "            start_index = batch *self.batch_size\n",
        "            end_index = min((batch+1)*self.batch_size, num_examples)\n",
        "            X_batch, Y_batch = X[start_index:end_index], Y[start_index:end_index]\n",
        "\n",
        "            self.backprop(X_batch, Y_batch)\n",
        "\n",
        "            for i in range(1, self.hidden_layers+1):\n",
        "              w_key, b_key, vw_key, vb_key,dw_key, db_key = [f\"{key}{i}\" for key in ['W', 'B', 'vw', 'vb', 'dW', 'dB']]\n",
        "              self.update_weights[vw_key] = gamma *self.update_weights[vw_key] + self.learning_rate * (self.grad_derivatice[dw_key])\n",
        "              self.update_weights[vb_key] = gamma *self.update_weights[vb_key] + self.learning_rate * (self.grad_derivatice[db_key])\n",
        "              self.weights[w_key] -= self.update_weights[vw_key]\n",
        "              self.weights[b_key] -= self.update_weights[vb_key]\n",
        "        self.wandlog(num_epoch, X, Y,X_val, Y_val)\n",
        "\n",
        "      elif algo == 'RMSProp':\n",
        "        num_examples = X.shape[0]\n",
        "        num_batches = num_examples //self.batch_size\n",
        "\n",
        "        for batch in range(num_batches + 1):\n",
        "            start_index = batch *self.batch_size\n",
        "            end_index = min((batch+1)*self.batch_size, num_examples)\n",
        "            X_batch, Y_batch = X[start_index:end_index], Y[start_index:end_index]\n",
        "\n",
        "            self.backprop(X_batch, Y_batch)\n",
        "\n",
        "            for i in range(1, self.hidden_layers+1):\n",
        "                w_key, b_key, vw_key, vb_key, dw_key, db_key = [f\"{key}{i}\" for key in ['W', 'B', 'vw', 'vb', 'dW', 'dB']]\n",
        "                self.update_weights[vw_key] = beta * self.update_weights[vw_key] + (1 - beta) * ((self.grad_derivatice[dw_key])**2)\n",
        "                self.update_weights[vb_key] = beta * self.update_weights[vb_key] + (1 - beta) * ((self.grad_derivatice[db_key])**2)\n",
        "                self.weights[w_key] -= (self.learning_rate / (np.sqrt(self.update_weights[vw_key] + eps))) * (self.grad_derivatice[dw_key])\n",
        "                self.weights[b_key] -= (self.learning_rate / (np.sqrt(self.update_weights[vb_key] + eps))) * (self.grad_derivatice[db_key])\n",
        "\n",
        "        self.wandlog(num_epoch, X, Y, X_val, Y_val)\n",
        "      \n",
        "      elif algo == 'Adam':\n",
        "\n",
        "        num_examples = X.shape[0]\n",
        "        num_batches = num_examples //self.batch_size\n",
        "\n",
        "        for batch in range(num_batches + 1):\n",
        "            start_index = batch *self.batch_size\n",
        "            end_index = min((batch+1)*self.batch_size, num_examples)\n",
        "            X_batch, Y_batch = X[start_index:end_index], Y[start_index:end_index]\n",
        "\n",
        "            self.backprop(X_batch, Y_batch)\n",
        "\n",
        "            for i in range(1, self.hidden_layers + 1):\n",
        "                w_key, b_key, vw_key, vb_key, mw_key, mb_key = [f\"{key}{i}\" for key in ['W', 'B', 'vw', 'vb', 'mw', 'mb']]\n",
        "                dw_key, db_key= [f\"{key}{i}\" for key in ['dW', 'dB']]\n",
        "\n",
        "                self.update_weights[mw_key] = beta1 * self.update_weights[mw_key] + (1 - beta1) * self.grad_derivatice[dw_key]\n",
        "                self.update_weights[vw_key] = beta2 * self.update_weights[vw_key] + (1 - beta2) * (self.grad_derivatice[dw_key] ** 2)\n",
        "                mw_hat = self.update_weights[mw_key] / (1 - np.power(beta1, batch + 1))\n",
        "                vw_hat = self.update_weights[vw_key] / (1 - np.power(beta2, batch + 1))\n",
        "                self.weights[w_key] -= (self.learning_rate / np.sqrt(vw_hat + eps)) * mw_hat\n",
        "\n",
        "                self.update_weights[mb_key] = beta1 * self.update_weights[mb_key] + (1 - beta1) * self.grad_derivatice[db_key]\n",
        "                self.update_weights[vb_key] = beta2 * self.update_weights[vb_key] + (1 - beta2) * (self.grad_derivatice[db_key] ** 2)\n",
        "                mb_hat = self.update_weights[mb_key] / (1 - np.power(beta1, batch + 1))\n",
        "                vb_hat = self.update_weights[vb_key] / (1 - np.power(beta2, batch + 1))\n",
        "                self.weights[b_key] -= (self.learning_rate / np.sqrt(vb_hat + eps)) * mb_hat\n",
        "\n",
        "        self.wandlog(num_epoch, X, Y,X_val, Y_val)\n",
        "          \n",
        "      elif algo == 'NAG':\n",
        "        num_examples = X.shape[0]\n",
        "        num_batches = num_examples //self.batch_size\n",
        "\n",
        "        temp_weights = {}\n",
        "        for i in range(1, self.hidden_layers+2):\n",
        "          w_key, b_key = [f\"{key}{i}\" for key in ['W', 'B']]\n",
        "          temp_weights[w_key] = np.zeros_like(self.weights[w_key])\n",
        "          temp_weights[b_key] = np.zeros_like(self.weights[b_key])\n",
        "        \n",
        "        for batch in range(num_batches + 1):\n",
        "            start_index = batch *self.batch_size\n",
        "            end_index = min((batch+1)*self.batch_size, num_examples)\n",
        "            X_batch, Y_batch = X[start_index:end_index], Y[start_index:end_index]\n",
        "\n",
        "            for i in range(1,self.hidden_layers+1):\n",
        "                w_key, b_key, vw_key, vb_key,dw_key, db_key = [f\"{key}{i}\" for key in ['W', 'B', 'vw', 'vb', 'dW', 'dB']]\n",
        "                self.update_weights[vw_key]=gamma*self.prev_update_weights[vw_key]\n",
        "                self.update_weights[vb_key]=gamma*self.prev_update_weights[vb_key]\n",
        "                temp_weights[w_key]=self.weights[w_key]-self.update_weights[vw_key]\n",
        "                temp_weights[b_key]=self.weights[b_key]-self.update_weights[vb_key]\n",
        "            self.backprop(X_batch,Y_batch,temp_weights)\n",
        "            for i in range(1,self.hidden_layers+1):\n",
        "                w_key, b_key, vw_key, vb_key,dw_key, db_key = [f\"{key}{i}\" for key in ['W', 'B', 'vw', 'vb', 'dW', 'dB']]\n",
        "                self.update_weights[vw_key] = gamma *self.update_weights[vw_key] + self.learning_rate * (self.grad_derivatice[dw_key])\n",
        "                self.update_weights[vb_key] = gamma *self.update_weights[vb_key] + self.learning_rate * (self.grad_derivatice[db_key])\n",
        "                self.weights[w_key] -= self.learning_rate * (self.update_weights[vw_key])\n",
        "                self.weights[b_key] -= self.learning_rate * (self.update_weights[vb_key]) \n",
        "\n",
        "            self.prev_update_weights=self.update_weights\n",
        "\n",
        "        self.wandlog(num_epoch, X, Y,X_val, Y_val)\n",
        "\n",
        "      elif algo == 'Nadam':\n",
        "\n",
        "        num_examples = X.shape[0]\n",
        "        num_batches = num_examples //self.batch_size\n",
        "\n",
        "        num_updates = 0\n",
        "        for i in range(1, self.hidden_layers + 1):\n",
        "            w_key, b_key, vw_key, vb_key, mw_key, mb_key = [f\"{key}{i}\" for key in ['W', 'B', 'vw', 'vb', 'mw', 'mb']]\n",
        "            dw_key, db_key, mw_i_key, mb_i_key = [f\"{key}{i}\" for key in ['dW', 'dB', 'mw_inf', 'mb_inf']]\n",
        "\n",
        "            for batch in range(num_batches + 1):\n",
        "                start_index = batch *self.batch_size\n",
        "                end_index = min((batch+1)*self.batch_size, num_examples)\n",
        "                X_batch, Y_batch = X[start_index:end_index], Y[start_index:end_index]\n",
        "\n",
        "                self.backprop(X_batch, Y_batch)\n",
        "\n",
        "                num_updates += 1\n",
        "                self.update_weights.setdefault(mw_i_key, 0)\n",
        "                self.update_weights[mw_key] = beta1 * self.update_weights[mw_key] + (1 - beta1) * (self.grad_derivatice[dw_key] )\n",
        "                self.update_weights[vw_key] = beta2 * self.update_weights[vw_key] + (1 - beta2) * ((self.grad_derivatice[dw_key]) ** 2)\n",
        "                mw_hat = self.update_weights[mw_key] / (1 - np.power(beta1, num_updates))\n",
        "                vw_hat = self.update_weights[vw_key] / (1 - np.power(beta2, num_updates))\n",
        "                mw_inf = beta1 * self.update_weights[mw_i_key] + (1 - beta1) * np.abs(self.grad_derivatice[dw_key])\n",
        "                mw_inf_hat = mw_inf / (1 - np.power(beta1, num_updates))\n",
        "                self.weights[w_key] -= (self.learning_rate / np.sqrt(vw_hat + eps)) * ((beta1 * mw_hat) + ((1 - beta1) * self.grad_derivatice[dw_key])) / (1 - np.power(beta2, num_updates)) + self.learning_rate * eps * np.sqrt(1 - np.power(beta2, num_updates)) * mw_inf_hat\n",
        "\n",
        "                self.update_weights.setdefault(mb_i_key, 0)\n",
        "                self.update_weights[mb_key] = beta1 * self.update_weights[mb_key] + (1 - beta1) * (self.grad_derivatice[db_key])\n",
        "                self.update_weights[vb_key] = beta2 * self.update_weights[vb_key] + (1 - beta2) * ((self.grad_derivatice[db_key]) ** 2)\n",
        "                mb_hat = self.update_weights[mb_key] / (1 - np.power(beta1, num_updates))\n",
        "                vb_hat = self.update_weights[vb_key] / (1 - np.power(beta2, num_updates))\n",
        "                mb_inf = beta1 * self.update_weights[mb_i_key] + (1 - beta1) * np.abs(self.grad_derivatice[db_key])\n",
        "                mb_inf_hat = mb_inf / (1 - np.power(beta1, num_updates))\n",
        "                self.weights[b_key] -= (self.learning_rate / np.sqrt(vb_hat + eps)) * ((beta1 * mb_hat) + ((1 - beta1) * self.grad_derivatice[db_key])) / (1 - np.power(beta2, num_updates)) + self.learning_rate * eps * np.sqrt(1 - np.power(beta2, num_updates)) * mb_inf\n",
        "        self.wandlog(num_epoch, X, Y,X_val, Y_val)\n",
        "        \n",
        "      if show_loss:\n",
        "        loss.append(self.Loss(X, Y))\n",
        "    \n",
        "    if show_loss:\n",
        "      plt.plot(loss)\n",
        "      plt.xlabel('Epochs')\n",
        "      plt.ylabel('Loss')\n",
        "      plt.show()\n",
        "  \n",
        "  def predict(self, X):\n",
        "    Y_pred = (self.forward_pass(X))\n",
        "    return np.array(Y_pred).squeeze()\n",
        "  \n",
        "  def accuracy_score(self, X, Y):\n",
        "    Y_true = np.argmax(Y, axis=1).reshape(-1, 1)\n",
        "    pred_labels = np.argmax(self.predict(X), axis=1).reshape(-1,1)\n",
        "    return np.sum(pred_labels == Y_true) / len(Y)\n",
        "\n",
        "  def Loss(self, X, Y):\n",
        "    Y_pred = self.predict(X)\n",
        "    if self.loss_function== 'CE':\n",
        "        loss = -np.mean(Y * np.log(Y_pred + 1e-8))\n",
        "    elif self.loss_function == 'MSE':\n",
        "        loss = np.mean((Y - Y_pred)**2)\n",
        "    return loss\n",
        "\n",
        "  def performance(self, X_test, Y_test):\n",
        "    loss = self.Loss(X_test, Y_test)\n",
        "    accuracy = self.accuracy_score(X_test, Y_test)\n",
        "\n",
        "\n",
        "  def confusion_matrix(self, X, Y):\n",
        "\n",
        "      actual_labels = np.argmax(Y, axis=1)\n",
        "      predicted_labels = np.argmax(self.forward_pass(X), axis=1)\n",
        "\n",
        "\n",
        "      available_classes = np.unique(np.concatenate((actual_labels, predicted_labels)))\n",
        "\n",
        "      confusion_matrix_ = np.zeros((len(available_classes), len(available_classes)), dtype=int)\n",
        "      for i, actual in enumerate(available_classes):\n",
        "          for j, predicted in enumerate(available_classes):\n",
        "              confusion_matrix_[i,j] = np.where((actual_labels == actual) & (predicted_labels == predicted))[0].shape[0]\n",
        "      wandb.log({\"confusion_matrix\": wandb.plot.confusion_matrix(\n",
        "          probs=None,\n",
        "          y_true=actual_labels,\n",
        "          preds=predicted_labels,\n",
        "          class_names=list(available_classes),\n",
        "          title=\"Confusion Matrix\"\n",
        "      )})\n",
        "\n",
        "      return confusion_matrix_\n",
        "\n",
        "\n",
        "\n",
        "  def confusion_matrix_plot(self, confusion_matrix, title='Confusion matrix', cmap=plt.cm.gray_r):\n",
        "    confusion_matrix = confusion_matrix/10\n",
        "    plt.matshow(confusion_matrix, cmap=cmap)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(confusion_matrix))\n",
        "    plt.xticks(tick_marks)\n",
        "    plt.yticks(tick_marks)\n",
        "    plt.ylabel('actual')\n",
        "    plt.xlabel('predicted')\n",
        "\n",
        "\n",
        "  def wandlog(self, num_epoch, X, Y,X_val, Y_val):\n",
        "    accuracy = self.accuracy_score(X, Y)\n",
        "    loss_train = self.Loss(X, Y)\n",
        "    loss_valid = self.Loss(X_val, Y_val)\n",
        "    val_accuracy = self.accuracy_score(X_val, Y_val)\n",
        "    wandb.log({'epoch': num_epoch,           \n",
        "              'loss': loss_train,\n",
        "              'accuracy': accuracy,\n",
        "              'val_loss': loss_valid,\n",
        "              'val_accuracy': val_accuracy})\n",
        "    \n",
        "    if num_epoch % 5== 0:\n",
        "      accuracy = self.accuracy_score(X, Y)\n",
        "      loss_train = self.Loss(X, Y)\n",
        "      loss_valid = self.Loss(X_val, Y_val)\n",
        "      val_accuracy = self.accuracy_score(X_val, Y_val)\n",
        "      library = {'epoch': num_epoch,           \n",
        "              'loss': loss_train,\n",
        "              'accuracy': accuracy,\n",
        "              'val_loss': loss_valid,\n",
        "              'val_accuracy': val_accuracy}\n",
        "\n",
        "      print('Epoch: {}, Train Loss: {}, Train Accuracy: {}, Val Loss: {}, Val Accuracy: {}'.format(library['epoch'], library['loss'], library['accuracy'], library['val_loss'], library['val_accuracy']))\n",
        "      if num_epoch == self.epochs:\n",
        "        print('Model trained successfully !')\n",
        "  \n"
      ],
      "metadata": {
        "id": "roqXcqcDgrri",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "344cf169-2c81-4d72-ce3a-600b02125b7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33med22s009\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230308_152115-c8bu83tb</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ed22s009/Question_4_Best_Model/runs/c8bu83tb' target=\"_blank\">cerulean-water-42</a></strong> to <a href='https://wandb.ai/ed22s009/Question_4_Best_Model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/ed22s009/Question_4_Best_Model' target=\"_blank\">https://wandb.ai/ed22s009/Question_4_Best_Model</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/ed22s009/Question_4_Best_Model/runs/c8bu83tb' target=\"_blank\">https://wandb.ai/ed22s009/Question_4_Best_Model/runs/c8bu83tb</a>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(project = 'Question_4_Best_Model')\n",
        "\n",
        "class FFNN:\n",
        "  def __init__(self, X, Y,\n",
        "               epochs = 100, \n",
        "               hidden_layer_count = 4,\n",
        "               hidden_layers =  [32, 64, 128, 256],\n",
        "               learning_rate = 0.001,\n",
        "               batch_size = 32,\n",
        "               activation='tanh',\n",
        "               weight_init='random',\n",
        "               loss = 'MSE',\n",
        "               weight_decay = 0):\n",
        "    \n",
        "    self.inputs =X.shape[1] # Number of inputs\n",
        "    self.outputs= Y.shape[1] # Number of outputs\n",
        "    self.epochs = epochs\n",
        "    self.hidden_layers = hidden_layer_count  # Number of hidden layers \n",
        "    self.network_size= [self.inputs] + hidden_layers +[self.outputs] # input layer + hidden layers + output layers\n",
        "    self.learning_rate = learning_rate\n",
        "    self.batch_size = batch_size\n",
        "    self.weights={} # It will create dictionary for weights and biases\n",
        "    self.weights_h = []\n",
        "    self.num_classes = Y.shape[1]\n",
        "    self.weight_init = weight_init\n",
        "    self.activation_function = activation\n",
        "    self.loss_function = loss\n",
        "    self.lambd = weight_decay\n",
        "    np.random.seed(0)  # We will set seed value so that it will generate same random numebers every time\n",
        "\n",
        "    self.grad_derivatice={}\n",
        "    self.update_weights={}\n",
        "    self.prev_update_weights={}\n",
        "    for i in range(1,self.hidden_layers+1):\n",
        "      vw_key, vb_key, mb_key, mw_key = [f\"{key}{i}\" for key in ['vw', 'vb', 'mb', 'mw']]\n",
        "      self.update_weights[vw_key]=0\n",
        "      self.update_weights[vb_key]=0\n",
        "      self.update_weights[mb_key]=0\n",
        "      self.update_weights[mw_key]=0\n",
        "      self.prev_update_weights[vw_key]=0\n",
        "      self.prev_update_weights[vb_key]=0\n",
        "\n",
        "    \n",
        "    if self.weight_init == 'random':\n",
        "      for i in range(1, self.hidden_layers + 2):\n",
        "          weight_shape = (self.network_size[i - 1], self.network_size[i])\n",
        "          weight_scale = 0.1\n",
        "          self.weights[f'W{i}'] = np.random.normal(scale=weight_scale, size=weight_shape)*0.1\n",
        "          \n",
        "          bias_shape = (1, self.network_size[i])\n",
        "          self.weights[f'B{i}'] = np.zeros(bias_shape)\n",
        "\n",
        "    if self.weight_init == 'Xavier':\n",
        "      for i in range(1, self.hidden_layers + 2):\n",
        "          weight_shape = (self.network_size[i - 1], self.network_size[i])\n",
        "          weight_scale = 0.1\n",
        "          self.weights[f'W{i}'] = np.random.normal(scale=weight_scale, size=weight_shape)*np.sqrt(1/self.network_size[i-1])\n",
        "          \n",
        "          bias_shape = (1, self.network_size[i])\n",
        "          self.weights[f'B{i}'] = np.zeros(bias_shape)\n",
        "\n",
        "  def forward_activation(self, X):\n",
        "      activation_functions = {\n",
        "          'sigmoid': lambda x: 1.0 / (1.0 + np.exp(-x)),\n",
        "          'tanh': np.tanh,\n",
        "          'Relu': lambda x: np.maximum(0, x)\n",
        "      }\n",
        "      activation_function = activation_functions.get(self.activation_function)\n",
        "      if activation_function:\n",
        "          return activation_function(X)\n",
        "      else:\n",
        "          raise ValueError(f\"Unknown activation function '{self.activation_function}'\")\n",
        "\n",
        "\n",
        "  def grad_activation(self, X):\n",
        "      activation_gradients = {\n",
        "          'sigmoid': lambda x: x * (1 - x),\n",
        "          'tanh': lambda x: 1 - np.square(x),\n",
        "          'Relu': lambda x: 1.0 * (x > 0)\n",
        "      }\n",
        "      gradient_function = activation_gradients.get(self.activation_function)\n",
        "      if gradient_function:\n",
        "          return gradient_function(X)\n",
        "      else:\n",
        "          raise ValueError(f\"Unknown activation function '{self.activation_function}'\")\n",
        "\n",
        "\n",
        "  def softmax(self, X):\n",
        "    exps =np.exp(X - np.max(X, axis=1, keepdims=True))\n",
        "    return  exps /np.sum(exps, axis=1, keepdims=True)\n",
        "  \n",
        "\n",
        "  def forward_pass(self, X, weights=None):\n",
        "      if weights is None:\n",
        "          weights = self.weights\n",
        "      self.A = {}\n",
        "      self.H = {}\n",
        "      self.H[0] = X\n",
        "      for i in range(self.hidden_layers):\n",
        "          self.A[i+1] = self.H[i] @ weights[f'W{i+1}'] + weights[f'B{i+1}']\n",
        "          self.H[i+1] = self.forward_activation(self.A[i+1])\n",
        "      self.A[self.hidden_layers+1] = self.H[self.hidden_layers] @ weights[f'W{self.hidden_layers+1}'] + weights[f'B{self.hidden_layers+1}']\n",
        "      self.H[self.hidden_layers+1] = self.softmax(self.A[self.hidden_layers+1])\n",
        "      return self.H[self.hidden_layers+1]\n",
        "\n",
        "  def backprop(self, X, Y, weights=None):\n",
        "    if weights is None:\n",
        "        weights = self.weights\n",
        "\n",
        "    self.forward_pass(X, weights)\n",
        "    self.grad_derivatice = {}\n",
        "    L = self.hidden_layers + 1\n",
        "\n",
        "    if self.loss_function == 'CE':\n",
        "        self.grad_derivatice[f'dA{L}'] =  (self.H[L] - Y)\n",
        "    elif self.loss_function == 'MSE':\n",
        "        self.grad_derivatice[f'dA{L}'] = (1/X.shape[0]) * 2 * (self.H[L] - Y)\n",
        "\n",
        "    for k in range(L, 0, -1):\n",
        "        w_key, b_key, dw_key, db_key, da_key = [f\"{key}{k}\" for key in ['W', 'B', 'dW', 'dB', 'dA']]\n",
        "        self.grad_derivatice[dw_key] = np.matmul(self.H[k-1].T, self.grad_derivatice[da_key]) + self.lambd * weights[w_key]\n",
        "        self.grad_derivatice[db_key] = np.sum(self.grad_derivatice[da_key], axis=0).reshape(1, -1) + + self.lambd * weights[b_key]\n",
        "        self.grad_derivatice[f'dH{k-1}'] = np.matmul(self.grad_derivatice[da_key], weights[w_key].T)\n",
        "        self.grad_derivatice[f'dA{k-1}'] = np.multiply(self.grad_derivatice[f'dH{k-1}'], self.grad_activation(self.H[k-1]))\n",
        "\n",
        "    return self.grad_derivatice[f'dH{k-1}']\n",
        "\n",
        "  def fit(self, X, Y, X_val, Y_val,algo= 'GD',a = 10, eps=1e-8, beta=0.9, beta1=0.9, beta2=0.9, gamma=0.9, show_loss = False):\n",
        "    if show_loss:\n",
        "      los = []\n",
        "      accuracy = []\n",
        "    for num_epoch in tqdm(range(1, self.epochs+1), unit='epoch'):\n",
        "      m = X.shape[0]\n",
        "      \n",
        "      if algo == 'SGD':\n",
        "        for i in range(m):\n",
        "            rand_idx = np.random.randint(m)\n",
        "            x_i = X[rand_idx:rand_idx+1]\n",
        "            y_i = Y[rand_idx:rand_idx+1]\n",
        "            self.backprop(x_i, y_i)\n",
        "            for j in range(1, self.hidden_layers+1):\n",
        "              w_key, b_key, dw_key, db_key = [f\"{key}{j}\" for key in ['W', 'B', 'dW', 'dB']]\n",
        "              self.weights[w_key] -=self.learning_rate * self.grad_derivatice[dw_key]\n",
        "              self.weights[b_key] -=self.learning_rate * self.grad_derivatice[db_key]\n",
        "        self.wandlog(num_epoch, X, Y, X_val, Y_val)\n",
        "\n",
        "      elif algo == 'Momentum':\n",
        "        num_examples = X.shape[0]\n",
        "        num_batches = num_examples //self.batch_size\n",
        "        for batch in range(num_batches + 1):\n",
        "            start_index = batch *self.batch_size\n",
        "            end_index = min((batch+1)*self.batch_size, num_examples)\n",
        "            X_batch, Y_batch = X[start_index:end_index], Y[start_index:end_index]\n",
        "\n",
        "            self.backprop(X_batch, Y_batch)\n",
        "\n",
        "            for i in range(1, self.hidden_layers+1):\n",
        "              w_key, b_key, vw_key, vb_key,dw_key, db_key = [f\"{key}{i}\" for key in ['W', 'B', 'vw', 'vb', 'dW', 'dB']]\n",
        "              self.update_weights[vw_key] = gamma *self.update_weights[vw_key] + self.learning_rate * (self.grad_derivatice[dw_key])\n",
        "              self.update_weights[vb_key] = gamma *self.update_weights[vb_key] + self.learning_rate * (self.grad_derivatice[db_key])\n",
        "              self.weights[w_key] -= self.update_weights[vw_key]\n",
        "              self.weights[b_key] -= self.update_weights[vb_key]\n",
        "        self.wandlog(num_epoch, X, Y,X_val, Y_val)\n",
        "\n",
        "      elif algo == 'RMSProp':\n",
        "        num_examples = X.shape[0]\n",
        "        num_batches = num_examples //self.batch_size\n",
        "\n",
        "        for batch in range(num_batches + 1):\n",
        "            start_index = batch *self.batch_size\n",
        "            end_index = min((batch+1)*self.batch_size, num_examples)\n",
        "            X_batch, Y_batch = X[start_index:end_index], Y[start_index:end_index]\n",
        "\n",
        "            self.backprop(X_batch, Y_batch)\n",
        "\n",
        "            for i in range(1, self.hidden_layers+1):\n",
        "                w_key, b_key, vw_key, vb_key, dw_key, db_key = [f\"{key}{i}\" for key in ['W', 'B', 'vw', 'vb', 'dW', 'dB']]\n",
        "                self.update_weights[vw_key] = beta * self.update_weights[vw_key] + (1 - beta) * ((self.grad_derivatice[dw_key])**2)\n",
        "                self.update_weights[vb_key] = beta * self.update_weights[vb_key] + (1 - beta) * ((self.grad_derivatice[db_key])**2)\n",
        "                self.weights[w_key] -= (self.learning_rate / (np.sqrt(self.update_weights[vw_key] + eps))) * (self.grad_derivatice[dw_key])\n",
        "                self.weights[b_key] -= (self.learning_rate / (np.sqrt(self.update_weights[vb_key] + eps))) * (self.grad_derivatice[db_key])\n",
        "\n",
        "        self.wandlog(num_epoch, X, Y, X_val, Y_val)\n",
        "      \n",
        "      elif algo == 'Adam':\n",
        "\n",
        "        num_examples = X.shape[0]\n",
        "        num_batches = num_examples //self.batch_size\n",
        "\n",
        "        for batch in range(num_batches + 1):\n",
        "            start_index = batch *self.batch_size\n",
        "            end_index = min((batch+1)*self.batch_size, num_examples)\n",
        "            X_batch, Y_batch = X[start_index:end_index], Y[start_index:end_index]\n",
        "\n",
        "            self.backprop(X_batch, Y_batch)\n",
        "\n",
        "            for i in range(1, self.hidden_layers + 1):\n",
        "                w_key, b_key, vw_key, vb_key, mw_key, mb_key = [f\"{key}{i}\" for key in ['W', 'B', 'vw', 'vb', 'mw', 'mb']]\n",
        "                dw_key, db_key= [f\"{key}{i}\" for key in ['dW', 'dB']]\n",
        "\n",
        "                self.update_weights[mw_key] = beta1 * self.update_weights[mw_key] + (1 - beta1) * self.grad_derivatice[dw_key]\n",
        "                self.update_weights[vw_key] = beta2 * self.update_weights[vw_key] + (1 - beta2) * (self.grad_derivatice[dw_key] ** 2)\n",
        "                mw_hat = self.update_weights[mw_key] / (1 - np.power(beta1, batch + 1))\n",
        "                vw_hat = self.update_weights[vw_key] / (1 - np.power(beta2, batch + 1))\n",
        "                self.weights[w_key] -= (self.learning_rate / np.sqrt(vw_hat + eps)) * mw_hat\n",
        "\n",
        "                self.update_weights[mb_key] = beta1 * self.update_weights[mb_key] + (1 - beta1) * self.grad_derivatice[db_key]\n",
        "                self.update_weights[vb_key] = beta2 * self.update_weights[vb_key] + (1 - beta2) * (self.grad_derivatice[db_key] ** 2)\n",
        "                mb_hat = self.update_weights[mb_key] / (1 - np.power(beta1, batch + 1))\n",
        "                vb_hat = self.update_weights[vb_key] / (1 - np.power(beta2, batch + 1))\n",
        "                self.weights[b_key] -= (self.learning_rate / np.sqrt(vb_hat + eps)) * mb_hat\n",
        "\n",
        "        self.wandlog(num_epoch, X, Y,X_val, Y_val)\n",
        "          \n",
        "      elif algo == 'NAG':\n",
        "        num_examples = X.shape[0]\n",
        "        num_batches = num_examples //self.batch_size\n",
        "\n",
        "        temp_weights = {}\n",
        "        for i in range(1, self.hidden_layers+2):\n",
        "          w_key, b_key = [f\"{key}{i}\" for key in ['W', 'B']]\n",
        "          temp_weights[w_key] = np.zeros_like(self.weights[w_key])\n",
        "          temp_weights[b_key] = np.zeros_like(self.weights[b_key])\n",
        "        \n",
        "        for batch in range(num_batches + 1):\n",
        "            start_index = batch *self.batch_size\n",
        "            end_index = min((batch+1)*self.batch_size, num_examples)\n",
        "            X_batch, Y_batch = X[start_index:end_index], Y[start_index:end_index]\n",
        "\n",
        "            for i in range(1,self.hidden_layers+1):\n",
        "                w_key, b_key, vw_key, vb_key,dw_key, db_key = [f\"{key}{i}\" for key in ['W', 'B', 'vw', 'vb', 'dW', 'dB']]\n",
        "                self.update_weights[vw_key]=gamma*self.prev_update_weights[vw_key]\n",
        "                self.update_weights[vb_key]=gamma*self.prev_update_weights[vb_key]\n",
        "                temp_weights[w_key]=self.weights[w_key]-self.update_weights[vw_key]\n",
        "                temp_weights[b_key]=self.weights[b_key]-self.update_weights[vb_key]\n",
        "            self.backprop(X_batch,Y_batch,temp_weights)\n",
        "            for i in range(1,self.hidden_layers+1):\n",
        "                w_key, b_key, vw_key, vb_key,dw_key, db_key = [f\"{key}{i}\" for key in ['W', 'B', 'vw', 'vb', 'dW', 'dB']]\n",
        "                self.update_weights[vw_key] = gamma *self.update_weights[vw_key] + self.learning_rate * (self.grad_derivatice[dw_key])\n",
        "                self.update_weights[vb_key] = gamma *self.update_weights[vb_key] + self.learning_rate * (self.grad_derivatice[db_key])\n",
        "                self.weights[w_key] -= self.learning_rate * (self.update_weights[vw_key]/m)\n",
        "                self.weights[b_key] -= self.learning_rate * (self.update_weights[vb_key]/m) \n",
        "\n",
        "            self.prev_update_weights=self.update_weights\n",
        "\n",
        "        self.wandlog(num_epoch, X, Y,X_val, Y_val)\n",
        "\n",
        "      elif algo == 'Nadam':\n",
        "\n",
        "        num_examples = X.shape[0]\n",
        "        num_batches = num_examples //self.batch_size\n",
        "\n",
        "        num_updates = 0\n",
        "        for i in range(1, self.hidden_layers + 1):\n",
        "            w_key, b_key, vw_key, vb_key, mw_key, mb_key = [f\"{key}{i}\" for key in ['W', 'B', 'vw', 'vb', 'mw', 'mb']]\n",
        "            dw_key, db_key, mw_i_key, mb_i_key = [f\"{key}{i}\" for key in ['dW', 'dB', 'mw_inf', 'mb_inf']]\n",
        "\n",
        "            for batch in range(num_batches + 1):\n",
        "                start_index = batch *self.batch_size\n",
        "                end_index = min((batch+1)*self.batch_size, num_examples)\n",
        "                X_batch, Y_batch = X[start_index:end_index], Y[start_index:end_index]\n",
        "\n",
        "                self.backprop(X_batch, Y_batch)\n",
        "\n",
        "                num_updates += 1\n",
        "                self.update_weights.setdefault(mw_i_key, 0)\n",
        "                self.update_weights[mw_key] = beta1 * self.update_weights[mw_key] + (1 - beta1) * (self.grad_derivatice[dw_key] )\n",
        "                self.update_weights[vw_key] = beta2 * self.update_weights[vw_key] + (1 - beta2) * ((self.grad_derivatice[dw_key]) ** 2)\n",
        "                mw_hat = self.update_weights[mw_key] / (1 - np.power(beta1, num_updates))\n",
        "                vw_hat = self.update_weights[vw_key] / (1 - np.power(beta2, num_updates))\n",
        "                mw_inf = beta1 * self.update_weights[mw_i_key] + (1 - beta1) * np.abs(self.grad_derivatice[dw_key])\n",
        "                mw_inf_hat = mw_inf / (1 - np.power(beta1, num_updates))\n",
        "                self.weights[w_key] -= (self.learning_rate / np.sqrt(vw_hat + eps)) * ((beta1 * mw_hat) + ((1 - beta1) * self.grad_derivatice[dw_key])) / (1 - np.power(beta2, num_updates)) + self.learning_rate * eps * np.sqrt(1 - np.power(beta2, num_updates)) * mw_inf_hat\n",
        "\n",
        "                self.update_weights.setdefault(mb_i_key, 0)\n",
        "                self.update_weights[mb_key] = beta1 * self.update_weights[mb_key] + (1 - beta1) * (self.grad_derivatice[db_key])\n",
        "                self.update_weights[vb_key] = beta2 * self.update_weights[vb_key] + (1 - beta2) * ((self.grad_derivatice[db_key]) ** 2)\n",
        "                mb_hat = self.update_weights[mb_key] / (1 - np.power(beta1, num_updates))\n",
        "                vb_hat = self.update_weights[vb_key] / (1 - np.power(beta2, num_updates))\n",
        "                mb_inf = beta1 * self.update_weights[mb_i_key] + (1 - beta1) * np.abs(self.grad_derivatice[db_key])\n",
        "                mb_inf_hat = mb_inf / (1 - np.power(beta1, num_updates))\n",
        "                self.weights[b_key] -= (self.learning_rate / np.sqrt(vb_hat + eps)) * ((beta1 * mb_hat) + ((1 - beta1) * self.grad_derivatice[db_key])) / (1 - np.power(beta2, num_updates)) + self.learning_rate * eps * np.sqrt(1 - np.power(beta2, num_updates)) * mb_inf\n",
        "        self.wandlog(num_epoch, X, Y,X_val, Y_val)\n",
        "      \n",
        "      if show_loss:\n",
        "        loss, acc = self.performance(X_val, Y_val)\n",
        "        acc = acc * 100\n",
        "        los.append(loss)\n",
        "        accuracy.append(acc)\n",
        "\n",
        "\n",
        "\n",
        "    if show_loss:\n",
        "\n",
        "          max_acc_index = np.argmax(accuracy)\n",
        "\n",
        "          plt.plot(los, label='Loss')\n",
        "          plt.plot(accuracy, label='Accuracy')\n",
        "          plt.plot(max_acc_index, los[max_acc_index], marker='o', color='red')\n",
        "          plt.plot(max_acc_index, accuracy[max_acc_index], marker='o', color='green')\n",
        "\n",
        "          plt.text(max_acc_index, los[max_acc_index], f\"loss @ Max val acc: ({max_acc_index}, {los[max_acc_index]:.4f})\", ha='left', va='top')\n",
        "          plt.text(max_acc_index, accuracy[max_acc_index], f\"Max Val acc: ({max_acc_index}, {accuracy[max_acc_index]:.4f})\", ha='left', va='bottom')\n",
        "\n",
        "\n",
        "          plt.axhline(los[max_acc_index], color='red', linestyle='--')\n",
        "          plt.axhline(accuracy[max_acc_index], color='green', linestyle='--')\n",
        "          plt.yscale('log')\n",
        "\n",
        "\n",
        "          plt.ylim([min(los + accuracy) , max(los + accuracy) + 1000])  \n",
        "\n",
        "          plt.title('Val Loss and val Accuracy with {}'.format(self.loss_function))\n",
        "          plt.xlabel('Epochs')\n",
        "          plt.ylabel('Loss / Accuracy')\n",
        "          plt.legend()\n",
        "          plt.show()\n",
        "\n",
        "\n",
        "  def predict(self, X):\n",
        "    Y_pred = (self.forward_pass(X))\n",
        "    return np.array(Y_pred).squeeze()\n",
        "  \n",
        "  def accuracy_score(self, X, Y):\n",
        "    Y_true = np.argmax(Y, axis=1).reshape(-1, 1)\n",
        "    pred_labels = np.argmax(self.predict(X), axis=1).reshape(-1,1)\n",
        "    return np.sum(pred_labels == Y_true) / len(Y)\n",
        "\n",
        "  def Loss(self, X, Y):\n",
        "      Y_pred = self.predict(X)\n",
        "      if self.loss_function == 'CE':\n",
        "          loss = -np.mean(Y * np.log(Y_pred + 1e-8))\n",
        "          max_loss = -np.mean(Y * np.log(1e-8))\n",
        "      elif self.loss_function == 'MSE':\n",
        "          loss = np.mean((Y - Y_pred)**2)\n",
        "          max_loss = np.mean(Y**2)\n",
        "      reg_loss = 0\n",
        "      for layer in range(1, self.hidden_layers + 2):\n",
        "          w_key = f\"W{layer}\"\n",
        "          reg_loss += np.sum(np.square(self.weights[w_key]))\n",
        "      reg_loss = 0.5 * self.lambd * reg_loss\n",
        "      loss += reg_loss\n",
        "      max_reg_loss = 0.5 * self.lambd * np.sum(np.square(self.weights[w_key]))\n",
        "      max_loss += max_reg_loss\n",
        "      loss = loss / max_loss\n",
        "      return loss\n",
        "\n",
        "\n",
        "  def performance(self, X_test, Y_test):\n",
        "    loss = self.Loss(X_test, Y_test)\n",
        "    accuracy = self.accuracy_score(X_test, Y_test)\n",
        "    return loss, accuracy\n",
        "\n",
        "\n",
        "  def confusion_matrix(self, X, Y):\n",
        "\n",
        "      actual_labels = np.argmax(Y, axis=1)\n",
        "      predicted_labels = np.argmax(self.forward_pass(X), axis=1)\n",
        "\n",
        "\n",
        "      available_classes = np.unique(np.concatenate((actual_labels, predicted_labels)))\n",
        "\n",
        "      confusion_matrix_ = np.zeros((len(available_classes), len(available_classes)), dtype=int)\n",
        "      for i, actual in enumerate(available_classes):\n",
        "          for j, predicted in enumerate(available_classes):\n",
        "              confusion_matrix_[i,j] = np.where((actual_labels == actual) & (predicted_labels == predicted))[0].shape[0]\n",
        "      wandb.log({\"confusion_matrix\": wandb.plot.confusion_matrix(\n",
        "          probs=None,\n",
        "          y_true=actual_labels,\n",
        "          preds=predicted_labels,\n",
        "          class_names=list(available_classes),\n",
        "          title=\"Confusion Matrix\"\n",
        "      )})\n",
        "\n",
        "      return confusion_matrix_\n",
        "\n",
        "\n",
        "\n",
        "  def confusion_matrix_plot(self, confusion_matrix, title='Confusion matrix', cmap=plt.cm.gray_r):\n",
        "    confusion_matrix = confusion_matrix/10\n",
        "    plt.matshow(confusion_matrix, cmap=cmap)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(confusion_matrix))\n",
        "    plt.xticks(tick_marks)\n",
        "    plt.yticks(tick_marks)\n",
        "    plt.ylabel('actual')\n",
        "    plt.xlabel('predicted')\n",
        "\n",
        "\n",
        "  def wandlog(self, num_epoch, X, Y,X_val, Y_val):\n",
        "    accuracy = self.accuracy_score(X, Y)\n",
        "    loss_train = self.Loss(X, Y)\n",
        "    loss_valid = self.Loss(X_val, Y_val)\n",
        "    val_accuracy = self.accuracy_score(X_val, Y_val)\n",
        "    wandb.log({'epoch': num_epoch,           \n",
        "              'loss': loss_train,\n",
        "              'accuracy': accuracy,\n",
        "              'val_loss': loss_valid,\n",
        "              'val_accuracy': val_accuracy})\n",
        "    \n",
        "    if num_epoch % 5== 0:\n",
        "      accuracy = self.accuracy_score(X, Y)\n",
        "      loss_train = self.Loss(X, Y)\n",
        "      loss_valid = self.Loss(X_val, Y_val)\n",
        "      val_accuracy = self.accuracy_score(X_val, Y_val)\n",
        "      library = {'epoch': num_epoch,           \n",
        "              'loss': loss_train,\n",
        "              'accuracy': accuracy,\n",
        "              'val_loss': loss_valid,\n",
        "              'val_accuracy': val_accuracy}\n",
        "\n",
        "      print('Epoch: {}, Train Loss: {}, Train Accuracy: {}, Val Loss: {}, Val Accuracy: {}'.format(library['epoch'], library['loss'], library['accuracy'], library['val_loss'], library['val_accuracy']))\n",
        "      if num_epoch == self.epochs:\n",
        "        print('Model trained successfully !')\n",
        "\n",
        "model = FFNN(X_train, Y_train,\n",
        "                  epochs = 5, \n",
        "                  hidden_layer_count = 3,\n",
        "                  hidden_layers =  [64, 64, 64],\n",
        "                  learning_rate = 0.0001,\n",
        "                  batch_size = 32,\n",
        "                  activation='Relu',\n",
        "                  weight_init='random',\n",
        "                  loss = 'CE',\n",
        "                  weight_decay = 0.0005)\n",
        "model.fit(X_train, Y_train, X_val, Y_val,algo= 'Adam', a = 1, show_loss = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 553,
          "referenced_widgets": [
            "2da7c78f66cf4ecf8f49333aed87254c",
            "c1435e9f24d541839480dfd1ba10c6d7",
            "8bc894367c9a44a3a6bcbb501706be8b",
            "7c9c74642b0b4882acfac03a5e7f8cd5",
            "ca60f75b4077434e9d6310c1347a3c5c",
            "ec78f1fa449a4049bf28730b485cda67",
            "006f6610a3b94b818cfa91ff39d241e0",
            "5f86253aeed446e59d0e19f61b4a771b",
            "a98c76a2f18943568cd3032b2d10bbea",
            "72357e8d6b1c4d6eac202d35b7ae213d",
            "cc794ba6779947e09f823f398f5ec781"
          ]
        },
        "id": "rPIG4r_He0uF",
        "outputId": "03f562ee-5f68-447e-9c24-65d281c9a9d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:c8bu83tb) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">cerulean-water-42</strong> at: <a href='https://wandb.ai/ed22s009/Question_4_Best_Model/runs/c8bu83tb' target=\"_blank\">https://wandb.ai/ed22s009/Question_4_Best_Model/runs/c8bu83tb</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230308_152115-c8bu83tb/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:c8bu83tb). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230308_152118-pbsvunb2</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ed22s009/Question_4_Best_Model/runs/pbsvunb2' target=\"_blank\">dutiful-sun-43</a></strong> to <a href='https://wandb.ai/ed22s009/Question_4_Best_Model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/ed22s009/Question_4_Best_Model' target=\"_blank\">https://wandb.ai/ed22s009/Question_4_Best_Model</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/ed22s009/Question_4_Best_Model/runs/pbsvunb2' target=\"_blank\">https://wandb.ai/ed22s009/Question_4_Best_Model/runs/pbsvunb2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/5 [00:00<?, ?epoch/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2da7c78f66cf4ecf8f49333aed87254c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 5, Train Loss: 0.13555675265563655, Train Accuracy: 0.6874259259259259, Val Loss: 0.13605968411329328, Val Accuracy: 0.6765\n",
            "Model trained successfully !\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}