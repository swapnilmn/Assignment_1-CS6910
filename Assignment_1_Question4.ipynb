{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swapnilmn/Assignment_1_CS6910/blob/master/Assignment_1_Question4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0Q5opYMgYb-"
      },
      "source": [
        "# Question 4 (10 Marks) Use the sweep functionality provided by wandb to find the best values for the hyperparameters listed below. Use the standard train/test split of fashion_mnist (use (X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()). Keep 10% of the training data aside as validation data for this hyperparameter search. Here are some suggestions for different values to try for hyperparameters. As you can quickly see that this leads to an exponential number of combinations. You will have to think about strategies to do this hyperparameter search efficiently. Check out the options provided by wandb.sweep and write down what strategy you chose and why.\n",
        "\n",
        "        number of epochs: 5, 10\n",
        "        number of hidden layers: 3, 4, 5\n",
        "        size of every hidden layer: 32, 64, 128\n",
        "        weight decay (L2 regularisation): 0, 0.0005, 0.5\n",
        "        learning rate: 1e-3, 1 e-4\n",
        "        optimizer: sgd, momentum, nesterov, rmsprop, adam, nadam\n",
        "        batch size: 16, 32, 64\n",
        "        weight initialisation: random, Xavier\n",
        "        activation functions: sigmoid, tanh, ReLU\n",
        "wandb will automatically generate the following plots. Paste these plots below using the \"Add Panel to Report\" feature. Make sure you use meaningful names for each sweep (e.g. hl_3_bs_16_ac_tanh to indicate that there were 3 hidden layers, batch size was 16 and activation function was ReLU) instead of using the default names (whole-sweep, kind-sweep) given by wandb."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm -rf Assignment_1_CS6910\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "!git config --global credential.helper 'cache --timeout=3600'\n",
        "!git clone https://swapnilmn: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@github.com/swapnilmn/Assignment_1_CS6910.git\n",
        "\n",
        "!pip install import_ipynb\n",
        "import import_ipynb\n",
        "from Assignment_1_CS6910 import Question_3 as NN\n",
        "from Assignment_1_CS6910.Question_3 import FFNN\n",
        "\n",
        "!pip install wandb\n",
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "id": "gOOsP9ilgIUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Function"
      ],
      "metadata": {
        "id": "vRcqIRGif5Lr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(project = 'Question_4_Best_Model', entity = 'ed22s009')\n",
        "\n",
        "configuration = {\n",
        "    'learning_rate': 0.001,\n",
        "    'epochs': 2,\n",
        "    'hidden_layer_count': 3,\n",
        "    'size_hidden_layers': 128,\n",
        "    'optimizer': 'adam',\n",
        "    'batch_size': 128,\n",
        "    'activation': 'ReLU',\n",
        "    'weight_initializations': 'Xavier',\n",
        "    'weight_decay': 0,\n",
        "    'loss_function': 'cross_entropy',\n",
        "    'dataset': 'fashion_mnist'#, \"mnist\"\n",
        "}\n",
        "def train():\n",
        "  \n",
        "  wandb.init(project ='confusion_matrix',config=configuration)\n",
        "  config_dict = {\n",
        "    'batch_size': wandb.config.batch_size,\n",
        "    'learning_rate': wandb.config.learning_rate,\n",
        "    'epochs': wandb.config.epochs,\n",
        "    'optimizer': wandb.config.optimizer,\n",
        "    'hidden_layer_count': wandb.config.hidden_layer_count,\n",
        "    'size_hidden_layers': wandb.config.size_hidden_layers,\n",
        "    'activation': wandb.config.activation,\n",
        "    'weight_decay': wandb.config.weight_decay,\n",
        "    'weight_initializations': wandb.config.weight_initializations,\n",
        "    'loss_function': wandb.config.loss_function\n",
        "     }\n",
        "  wandb.run.name = '/'.join([f\"{k}/{v}\" for k, v in config_dict.items()])\n",
        "\n",
        "\n",
        "  X_train, X_val, X_test, Y_train, Y_val, Y_test = NN.dataset_type(dataset=wandb.config.dataset)\n",
        "\n",
        "  model = FFNN(X_train, Y_train,\n",
        "                  epochs=wandb.config.epochs,\n",
        "                  hidden_layer_count=wandb.config.hidden_layer_count,\n",
        "                  hidden_layers=[wandb.config.size_hidden_layers]*wandb.config.hidden_layer_count,\n",
        "                  learning_rate=wandb.config.learning_rate,\n",
        "                  batch_size=wandb.config.batch_size,\n",
        "                  activation=wandb.config.activation,\n",
        "                  weight_init=wandb.config.weight_initializations,\n",
        "                  loss=wandb.config.loss_function,\n",
        "                  weight_decay=wandb.config.weight_decay)\n",
        "\n",
        "\n",
        "  optimizers = {\n",
        "      'sgd': 'sgd',\n",
        "      'momentum': 'momentum',\n",
        "      'nag': 'nag',\n",
        "      'rmsprop': 'rmsprop',\n",
        "      'adam': 'adam',\n",
        "      'nadam': 'nadam'\n",
        "  }\n",
        "\n",
        "  optimizer = wandb.config.optimizer\n",
        "  if optimizer in optimizers:\n",
        "      weights = model.fit(X_train, Y_train, X_val, Y_val, algo=optimizers[optimizer])\n",
        "  else:\n",
        "      print('Invalid optimizer')\n",
        "\n",
        "  print(model.confusion_matrix(X_test, Y_test))\n",
        "  model.confo_matrixplot(model.confusion_matrix(X_test, Y_test))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  train()\n",
        "  wandb.finish()\n"
      ],
      "metadata": {
        "id": "9ul8IvvqUSss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sweeping the model"
      ],
      "metadata": {
        "id": "6jv4mfHTf8WE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0TOrrjwZyTR3"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import wandb\n",
        "    wandb_available = True\n",
        "except ImportError:\n",
        "    wandb_available = False\n",
        "\n",
        "if wandb_available:\n",
        "    sweep_config = {\n",
        "        'method': 'bayes',\n",
        "        'name': 'Assignement1',\n",
        "        'metric': {'goal': 'maximize', 'name': 'val_accuracy'},\n",
        "        'parameters': {\n",
        "          'epochs': {'values': [10, 15, 20]},#150\n",
        "          'hidden_layer_count':{'values': [3, 4, 5]},\n",
        "          'size_hidden_layers':{'values': [32,128, 256, 512]},\n",
        "          'learning_rate':{'values': [0.001, 0.0001]},\n",
        "          'optimizer':{'values': [\"sgd\", \"momentum\", \"nag\", \"rmsprop\", \"adam\", \"nadam\"]},\n",
        "          'batch_size':{'values': [32, 64, 128]},\n",
        "          'activation':{'values': ['sigmoid','tanh', 'ReLU']},\n",
        "          'weight_initializations':{'values': ['random','Xavier']},\n",
        "          'weight_decay':{'values': [0, 0.05,0.0005 ]}}\n",
        "        }\n",
        "     \n",
        "\n",
        "    # Create the sweep and run the training function\n",
        "    sweep_id = wandb.sweep(sweep_config, project='Question_4_Best_Model', entity = 'ed22s009')\n",
        "    wandb.agent(sweep_id, function=train)\n",
        "else:\n",
        "    print('wandb library not available')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rough Work"
      ],
      "metadata": {
        "id": "A9_QYxrqAqFq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# wandb.init(project = 'Question_4_Best_Model', entity = 'ed22s009')\n",
        "\n",
        "# algos = ['GD','SGD', 'MiniBatch', 'Momentum', 'NAG', 'AdaGrad', 'RMSProp', 'Adam','Nadam']\n",
        "# configuration = {\n",
        "#     'learning_rate': 0.001,\n",
        "#     'epochs': 2,\n",
        "#     'hidden_layer_count': 3,\n",
        "#     'size_hidden_layers': 128,\n",
        "#     'optimizer': 'adam',\n",
        "#     'batch_size': 128,\n",
        "#     'activation': 'ReLU',\n",
        "#     'weight_initializations': 'Xavier',\n",
        "#     'weight_decay': 0,\n",
        "#     'loss_function': 'cross_entropy',\n",
        "#     'dataset': 'fashion_mnist'#, \"mnist\"\n",
        "# }\n",
        "\n",
        "# def train():\n",
        "  \n",
        "#   wandb.init(project ='confusion_matrix',config=configuration)\n",
        "#   wandb.run.name = '/batch_size/'+str(wandb.config.batch_size)+'/learning_rate/'+ str(wandb.config.learning_rate)+'/epochs/'+str(wandb.config.epochs)+ '/optimizer/'+str(wandb.config.optimizer)+ '/hidden_layer_count/'+str(wandb.config.hidden_layer_count)+'/size_hidden_layers/'+str(wandb.config.size_hidden_layers)+ '/activation/'+str(wandb.config.activation)+'/weight_decay/'+str(wandb.config.weight_decay)+'/weight_initializations/'+str(wandb.config.weight_initializations)+'/loss_function/'+str(wandb.config.loss_function)\n",
        "\n",
        "  \n",
        "#   # [configuration['size_hidden_layers']] * configuration['hidden_layer_count']\n",
        "#   X_train, X_val, X_test, Y_train, Y_val, Y_test = NN.dataset_type(dataset = wandb.config.dataset)\n",
        "#   hidden_layer_count = wandb.config.hidden_layer_count \n",
        "#   size_hidden_layers = wandb.config.size_hidden_layers \n",
        "#   model = FFNN(X_train, Y_train,\n",
        "#                 epochs = wandb.config.epochs, \n",
        "#                 hidden_layer_count =  wandb.config.hidden_layer_count,\n",
        "#                 hidden_layers = [size_hidden_layers]*hidden_layer_count,\n",
        "#                 learning_rate = wandb.config.learning_rate,\n",
        "#                 batch_size = wandb.config.batch_size,\n",
        "#                 activation=wandb.config.activation,\n",
        "#                 weight_init=wandb.config.weight_initializations,\n",
        "#                 loss = wandb.config.loss_function,\n",
        "#                 weight_decay = wandb.config.weight_decay)\n",
        "\n",
        "#   algos = ['GD','SGD', 'MiniBatch', 'Momentum', 'NAG', 'AdaGrad', 'RMSProp', 'Adam','Nadam']\n",
        "#   [\"sgd\", \"momentum\", \"nag\", \"rmsprop\", \"adam\", \"nadam\"]\n",
        "#   ['momentum','sgd','rmsprop','nesterov','adam','nadam']\n",
        "#   optimizer = wandb.config.optimizer\n",
        "#   if optimizer == 'sgd':\n",
        "#     weights = model.fit(X_train, Y_train, X_val, Y_val, algo= 'sgd')\n",
        "#   elif optimizer == 'momentum':\n",
        "#     weights =model.fit(X_train, Y_train, X_val, Y_val, algo= 'momentum')\n",
        "#   elif optimizer == 'nag':\n",
        "#     weights =model.fit(X_train, Y_train, X_val, Y_val, algo= 'nag')\n",
        "#   elif optimizer == 'rmsprop':\n",
        "#     weights =model.fit(X_train, Y_train, X_val, Y_val, algo= 'rmsprop')\n",
        "#   elif optimizer == 'adam':\n",
        "#     weights =model.fit(X_train, Y_train, X_val, Y_val, algo='adam')\n",
        "#   elif optimizer =='nadam':\n",
        "#     weights =model.fit(X_train, Y_train, X_val, Y_val, algo= 'nadam')\n",
        "#   else:\n",
        "#     print('Invalid optimizer')\n",
        "\n",
        "\n",
        "\n",
        "#   confusion_matrix = model.confusion_matrix(X_test, Y_test)\n",
        "#   print(confusion_matrix)\n",
        "#   model.confo_matrixplot(confusion_matrix)\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#   train()\n",
        "#   wandb.finish()\n"
      ],
      "metadata": {
        "id": "waHc4HCyp6Av"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# wandb.init(project = 'Question_Best_Model', entity = 'ed22s009')\n",
        "# init_methods = ['random', 'Xavier']\n",
        "# activation_functions = ['sigmoid', 'tanh', 'ReLU']\n",
        "# algos = ['sgd', 'momentum', 'nag', 'rmsprop', 'adam','nadam']\n",
        "# losses = ['cross_entropy', 'mean_squared_error']\n",
        "# c = 0\n",
        "# d = 0\n",
        "# for init_method in init_methods:\n",
        "#     for activation_function in activation_functions:\n",
        "#         for algo in algos:\n",
        "#           for loss in losses:\n",
        "\n",
        "#             model = FFNN(NN.X_train, NN.Y_train,\n",
        "#                           epochs = 1, \n",
        "#                           hidden_layer_count = 1,\n",
        "#                           hidden_layers =  [10],\n",
        "#                           learning_rate = 0.0001,\n",
        "#                           batch_size = 32,\n",
        "#                           activation=activation_function,\n",
        "#                           weight_init=init_method,\n",
        "#                           loss = loss,\n",
        "#                           weight_decay = 0.0005)\n",
        "#             model.fit(NN.X_train, NN.Y_train, NN.X_val, NN.Y_val,algo= algo)\n",
        "#             c = c + 1\n",
        "\n",
        "#             print(c)"
      ],
      "metadata": {
        "id": "lXzNOxRqtARt"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "_dP0oEKrgmQc",
        "NiqBs52vjT6P",
        "j80rnJZjjYxh",
        "qvCdYtjPjesY",
        "Jf9807PEjrL_",
        "3QuoMWx3kp11"
      ],
      "provenance": [],
      "authorship_tag": "ABX9TyNaZ1i5m9GmJqm7Au4/fmWb",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}